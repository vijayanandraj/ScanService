# issue_grouping.py

import pandas as pd
import sqlite3
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import AgglomerativeClustering
from transformers import T5Tokenizer, T5ForConditionalGeneration, pipeline
import language_tool_python
from autocorrect import Speller
import os
import logging

# Configure logging
logging.basicConfig(
    filename='issue_grouping.log',
    level=logging.INFO,
    format='%(asctime)s:%(levelname)s:%(message)s'
)

# Initialize spell checkers
language_tool = language_tool_python.LanguageTool('en-US')
spell = Speller(lang='en')

def correct_spelling(text):
    """
    Correct spelling errors in the given text using LanguageTool and Autocorrect.
    
    Args:
        text (str): Text to be corrected.
    
    Returns:
        corrected_text (str): Corrected text.
    """
    try:
        # Correct using LanguageTool
        matches = language_tool.check(text)
        corrected_text = language_tool_python.utils.correct(text, matches)
        
        # Further correct using Autocorrect
        corrected_text = spell(corrected_text)
        
        return corrected_text
    except Exception as e:
        logging.error(f"Error in correct_spelling: {e}")
        return text  # Return original text if correction fails

def load_summarizer(model_name='t5-small', model_path=None):
    """
    Load the T5 model and tokenizer from Hugging Face or a local directory.
    
    Args:
        model_name (str): Name of the Hugging Face model.
        model_path (str): Path to the local directory containing the model.
    
    Returns:
        summarizer (pipeline): Hugging Face summarization pipeline.
    """
    try:
        if model_path and os.path.exists(model_path):
            tokenizer = T5Tokenizer.from_pretrained(model_path)
            model = T5ForConditionalGeneration.from_pretrained(model_path)
        else:
            tokenizer = T5Tokenizer.from_pretrained(model_name)
            model = T5ForConditionalGeneration.from_pretrained(model_name)
        
        summarizer = pipeline("summarization", model=model, tokenizer=tokenizer)
        return summarizer
    except Exception as e:
        logging.error(f"Error loading summarizer: {e}")
        raise e

def deduplicate_questions(questions):
    """
    Remove exact duplicate questions.
    
    Args:
        questions (list): List of question strings.
    
    Returns:
        unique_questions (list): List of unique question strings.
    """
    seen = set()
    unique_questions = []
    for q in questions:
        q_clean = q.lower().strip()
        if q_clean not in seen:
            unique_questions.append(q)
            seen.add(q_clean)
    return unique_questions

def format_questions_for_summarization(questions):
    """
    Format the list of questions into a single string with clear separation and instructions.
    
    Args:
        questions (list): List of question strings.
    
    Returns:
        formatted_text (str): Formatted string for summarization.
    """
    # Clear Instruction with Bullet Points
    formatted_questions = "\n".join([f"- {q}" for q in questions])
    formatted_text = f"Please provide a concise and descriptive summary for the following questions:\n{formatted_questions}"
    return formatted_text

def summarize_questions(summarizer, questions):
    """
    Generate a summary for a list of similar questions.
    
    Args:
        summarizer (pipeline): Hugging Face summarization pipeline.
        questions (list): List of question strings.
    
    Returns:
        summary (str): Generated and corrected summary string.
    """
    try:
        # Deduplicate questions
        unique_questions = deduplicate_questions(questions)
        
        # Handle cases where all questions are identical or nearly identical
        if len(unique_questions) == 1:
            return unique_questions[0]
        
        # Format questions
        input_text = format_questions_for_summarization(unique_questions)
        
        # Generate summary
        summary = summarizer(
            input_text,
            max_length=100,           # Adjusted for concise summaries
            min_length=30,            # Adjusted for concise summaries
            num_beams=4,              # Beam search for better quality
            early_stopping=True,
            no_repeat_ngram_size=3,   # Prevent repetition
            do_sample=False
        )
        
        # Extract summary text
        summary_text = summary[0]['summary_text']
        
        # Correct spelling
        corrected_summary = correct_spelling(summary_text)
        
        return corrected_summary
    except Exception as e:
        logging.error(f"Error in summarize_questions: {e}")
        return "Summary generation failed."

def cluster_issues(embeddings, distance_threshold=1.0):
    """
    Cluster issues based on embeddings using Agglomerative Clustering.
    
    Args:
        embeddings (ndarray): Array of embeddings.
        distance_threshold (float): The linkage distance threshold above which clusters will not be merged.
    
    Returns:
        cluster_labels (list): List of cluster labels for each issue.
    """
    try:
        clustering_model = AgglomerativeClustering(
            n_clusters=None,
            metric='cosine',           # Updated parameter from 'affinity' to 'metric'
            linkage='average',
            distance_threshold=distance_threshold
        )
        cluster_labels = clustering_model.fit_predict(embeddings)
        return cluster_labels
    except Exception as e:
        logging.error(f"Error in cluster_issues: {e}")
        raise e

def create_database(db_path='issues.db'):
    """
    Create SQLite database and necessary tables.
    
    Args:
        db_path (str): Path to the SQLite database file.
    
    Returns:
        conn (sqlite3.Connection): SQLite connection object.
    """
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # Create Issues table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS Issues (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                issue_short_description TEXT,
                issue_type TEXT,
                tool TEXT,
                cluster_id INTEGER,
                FOREIGN KEY (cluster_id) REFERENCES Clusters(cluster_id)
            )
        ''')
        
        # Create Clusters table with correct data types
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS Clusters (
                cluster_id INTEGER PRIMARY KEY,
                summary TEXT
            )
        ''')
        
        conn.commit()
        return conn
    except Exception as e:
        logging.error(f"Error creating database: {e}")
        raise e

def insert_data(conn, df, cluster_labels):
    """
    Insert issues into the Issues table.
    
    Args:
        conn (sqlite3.Connection): SQLite connection object.
        df (DataFrame): Pandas DataFrame containing the issues.
        cluster_labels (list): List of cluster labels corresponding to each issue.
    """
    try:
        cursor = conn.cursor()
        for idx, row in df.iterrows():
            cursor.execute('''
                INSERT INTO Issues (issue_short_description, issue_type, tool, cluster_id)
                VALUES (?, ?, ?, ?)
            ''', (
                row['Issue Short Description'],
                row['Issue Type'],
                row['Tool'],
                cluster_labels[idx]
            ))
        conn.commit()
    except Exception as e:
        logging.error(f"Error inserting data into Issues table: {e}")
        raise e

def insert_clusters(conn, cluster_summaries):
    """
    Insert cluster summaries into the Clusters table.
    
    Args:
        conn (sqlite3.Connection): SQLite connection object.
        cluster_summaries (dict): Dictionary mapping cluster_id to summary.
    """
    try:
        cursor = conn.cursor()
        for cluster_id, summary in cluster_summaries.items():
            # Ensure cluster_id is integer
            try:
                cluster_id_int = int(cluster_id)
            except ValueError:
                logging.error(f"Invalid cluster_id: {cluster_id}. Skipping.")
                continue
            
            # Debugging: Print types
            logging.info(f"Inserting Cluster ID: {cluster_id_int}, Summary: {summary}")
            
            cursor.execute('''
                INSERT OR REPLACE INTO Clusters (cluster_id, summary)
                VALUES (?, ?)
            ''', (
                cluster_id_int,
                summary
            ))
        conn.commit()
    except Exception as e:
        logging.error(f"Error inserting data into Clusters table: {e}")
        raise e

def generate_cluster_summaries(summarizer, df, cluster_labels):
    """
    Generate summaries for each cluster.
    
    Args:
        summarizer (pipeline): Hugging Face summarization pipeline.
        df (DataFrame): Pandas DataFrame containing the issues.
        cluster_labels (list): List of cluster labels corresponding to each issue.
    
    Returns:
        cluster_summaries (dict): Dictionary mapping cluster_id to summary.
    """
    try:
        cluster_summaries = {}
        clusters = {}
        
        # Organize issues by cluster
        for idx, label in enumerate(cluster_labels):
            if label not in clusters:
                clusters[label] = []
            clusters[label].append(df.iloc[idx]['Issue Short Description'])
        
        # Generate summaries for each cluster
        for cluster_id, issues in clusters.items():
            summary = summarize_questions(summarizer, issues)
            cluster_summaries[cluster_id] = summary
        
        return cluster_summaries
    except Exception as e:
        logging.error(f"Error generating cluster summaries: {e}")
        raise e

def query_top_issues(conn, tool=None, issue_type=None, top_n=10):
    """
    Query top N issues based on Tool and/or Issue Type.
    
    Args:
        conn (sqlite3.Connection): SQLite connection object.
        tool (str): Tool name to filter.
        issue_type (str): Issue Type to filter.
        top_n (int): Number of top issues to retrieve.
    
    Returns:
        results (list of tuples): Retrieved issues.
    """
    try:
        cursor = conn.cursor()
        query = '''
            SELECT Issues.issue_short_description, Issues.issue_type, Issues.tool, Clusters.summary, COUNT(*) as issue_count
            FROM Issues
            JOIN Clusters ON Issues.cluster_id = Clusters.cluster_id
        '''
        conditions = []
        params = []
        
        if tool:
            conditions.append("Issues.tool = ?")
            params.append(tool)
        if issue_type:
            conditions.append("Issues.issue_type = ?")
            params.append(issue_type)
        
        if conditions:
            query += " WHERE " + " AND ".join(conditions)
        
        query += " GROUP BY Issues.cluster_id ORDER BY issue_count DESC LIMIT ?"
        params.append(top_n)
        
        cursor.execute(query, params)
        results = cursor.fetchall()
        return results
    except Exception as e:
        logging.error(f"Error querying top issues: {e}")
        return []

def validate_dataframe(df):
    """
    Validate that the DataFrame contains the necessary columns and no missing values.
    
    Args:
        df (DataFrame): Pandas DataFrame to validate.
    
    Raises:
        ValueError: If validation fails.
    """
    required_columns = ['Issue Short Description', 'Issue Type', 'Tool']
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        raise ValueError(f"Missing columns: {missing_columns}")
    
    if df[required_columns].isnull().any().any():
        raise ValueError("There are missing values in the required columns.")

def main():
    try:
        # Path to your Excel file
        excel_path = 'issues.xlsx'  # Update this path as needed
        
        # Read Excel data
        print("Reading Excel data...")
        df = pd.read_excel(excel_path)
        
        # Validate DataFrame
        validate_dataframe(df)
        logging.info("DataFrame validation passed.")
        
        # Initialize SBERT model
        print("Loading SBERT model...")
        sbert_model = SentenceTransformer('all-MiniLM-L6-v2')  # You can choose a different SBERT model
        
        # Generate embeddings
        print("Generating embeddings...")
        embeddings = sbert_model.encode(df['Issue Short Description'].tolist(), show_progress_bar=True)
        logging.info("Embeddings generated.")
        
        # Cluster issues
        print("Clustering issues...")
        cluster_labels = cluster_issues(embeddings, distance_threshold=0.5)  # Adjust threshold as needed
        logging.info("Clustering completed.")
        
        # Initialize T5-small summarizer
        print("Loading summarizer...")
        summarizer = load_summarizer(model_name='t5-small')  # Use model_path if using local model
        logging.info("Summarizer loaded.")
        
        # Generate cluster summaries
        print("Generating cluster summaries...")
        cluster_summaries = generate_cluster_summaries(summarizer, df, cluster_labels)
        logging.info("Cluster summaries generated.")
        
        # Create SQLite database
        print("Creating database...")
        conn = create_database('issues.db')  # Update path as needed
        logging.info("Database created/opened.")
        
        # Insert issues into database
        print("Inserting issues into database...")
        insert_data(conn, df, cluster_labels)
        logging.info("Issues inserted into database.")
        
        # Insert cluster summaries into database
        print("Inserting cluster summaries into database...")
        insert_clusters(conn, cluster_summaries)
        logging.info("Cluster summaries inserted into database.")
        
        print("Data insertion complete.")
        
        # Example Queries
        print("\n=== Example Queries ===\n")
        
        # Top 10 issues for Bitbucket
        print("Top 10 Issues for Tool: Bitbucket")
        top_bitbucket = query_top_issues(conn, tool='Bitbucket', top_n=10)
        for row in top_bitbucket:
            print(f"Issue: {row[0]}\nIssue Type: {row[1]}\nTool: {row[2]}\nSummary: {row[3]}\nCount: {row[4]}\n")
        
        # Top 10 issues for Jira3 and Access Issue
        print("Top 10 Issues for Tool: Jira3 and Issue Type: Access Issue")
        top_jira3_access = query_top_issues(conn, tool='Jira3', issue_type='Access Issue', top_n=10)
        for row in top_jira3_access:
            print(f"Issue: {row[0]}\nIssue Type: {row[1]}\nTool: {row[2]}\nSummary: {row[3]}\nCount: {row[4]}\n")
        
        # Close the database connection
        conn.close()
        logging.info("Database connection closed.")
    
    except Exception as e:
        logging.error(f"An error occurred in main: {e}")
        print(f"An error occurred: {e}")

if __name__ == "__main__":
    main()

from transformers import T5Tokenizer, T5ForConditionalGeneration

# Load the T5-small model and tokenizer
model_path = 'path/to/your/t5-small'  # Update this with the path to your downloaded model
tokenizer = T5Tokenizer.from_pretrained(model_path)
model = T5ForConditionalGeneration.from_pretrained(model_path)

def summarize_questions(questions):
    # Combine all questions into a single string
    input_text = " ".join(questions)
    input_text = "summarize: " + input_text  # Prefix for T5

    # Encode the input text
    inputs = tokenizer.encode(input_text, return_tensors='pt', max_length=512, truncation=True)

    # Generate summary ids
    summary_ids = model.generate(inputs, max_length=50, min_length=25, length_penalty=2.0, num_beams=4, early_stopping=True)

    # Decode the summary ids
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

def main():
    # Load your unanswered questions
    df_unanswered = pd.read_csv('data/unanswered_questions.csv')  # Update path as needed

    # Example list of questions to summarize
    questions_to_summarize = [
        "What is the role of a project manager?",
        "How does a project manager ensure project success?",
        "What are the key responsibilities of project management?"
    ]

    # Summarize the questions
    summary = summarize_questions(questions_to_summarize)
    print(f"Summary: {summary}")

if __name__ == "__main__":
    main()



import os

def join_files(part_files_folder, output_file_path, base_filename="model.safetensors"):
    """
    Joins split files from a specified folder back into a single file.
    Args:
        part_files_folder (str): The path to the folder containing the part files.
        output_file_path (str): The path to save the joined file.
        base_filename (str): The base name of the part files (default is "model.safetensors").
    """
    part_num = 0
    with open(output_file_path, 'wb') as output_file:
        while True:
            part_file_name = os.path.join(part_files_folder, f"{base_filename}.part{part_num}")
            if not os.path.exists(part_file_name):
                break  # Stop when no more part files are found
            with open(part_file_name, 'rb') as part_file:
                output_file.write(part_file.read())
            print(f"Joined {part_file_name}")``
            part_num += 1

    print(f"All parts joined into {output_file_path}")

# Usage example:
part_files_folder = "path/to/your/part/files/folder"  # Folder where part files are located
output_file_path = "path/to/output/model_joined.safetensors"  # Path to save the joined file
join_files(part_files_folder, output_file_path, base_filename="model.safetensors")


from transformers import pipeline

def summarize_questions(questions):
    summarizer = pipeline("summarization")
    combined_text = " ".join(questions)
    summary = summarizer(combined_text, max_length=50, min_length=25, do_sample=False)
    return summary[0]['summary_text']


def find_similar_groups(similarity_matrix, threshold=0.8):
    similar_groups = []
    seen_questions = set()

    for i in range(similarity_matrix.shape[0]):
        if i not in seen_questions:
            group = [i]
            seen_questions.add(i)

            for j in range(i + 1, similarity_matrix.shape[1]):
                if similarity_matrix[i][j] > threshold:
                    group.append(j)
                    seen_questions.add(j)

            similar_groups.append(group)

    return similar_groups


from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer

def extract_keywords_lda(questions):
    vectorizer = CountVectorizer(stop_words='english')
    doc_term_matrix = vectorizer.fit_transform(questions)

    lda = LatentDirichletAllocation(n_components=1, random_state=42)
    lda.fit(doc_term_matrix)

    # Get the words in the topic
    feature_names = vectorizer.get_feature_names_out()
    keywords = [feature_names[i] for i in lda.components_[0].argsort()[-3:]]  # Top 3 keywords
    return " | ".join(keywords)


from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
import numpy as np
from collections import Counter
import re

def load_fine_tuned_model():
    # Load the fine-tuned SBERT model
    model = SentenceTransformer('models/fine-tuned-sbert')
    return model

def generate_embeddings(model, questions):
    embeddings = model.encode(questions, convert_to_tensor=True)
    return embeddings.cpu().numpy()

def find_similar_groups(similarity_matrix, threshold=0.8):
    similar_groups = []
    visited = set()

    for i in range(similarity_matrix.shape[0]):
        if i in visited:
            continue
        group = [i]
        visited.add(i)

        for j in range(i + 1, similarity_matrix.shape[0]):
            if similarity_matrix[i, j] > threshold:
                group.append(j)
                visited.add(j)

        similar_groups.append(group)

    return similar_groups

def extract_keywords(questions):
    # Combine all questions in the group into a single string
    combined_text = " ".join(questions)
    # Use regex to remove punctuation and split into words
    words = re.findall(r'\w+', combined_text.lower())
    # Count the frequency of each word
    word_counts = Counter(words)
    # Extract the most common words as keywords
    keywords = word_counts.most_common(3)  # Get top 3 keywords
    # Return a simple group name based on keywords
    return " | ".join(word for word, count in keywords)

def main():
    # Load the fine-tuned model
    model = load_fine_tuned_model()

    # Load your unanswered questions
    df_unanswered = pd.read_csv('data/unanswered_questions.csv')  # Update path as needed

    # Generate embeddings
    embeddings = generate_embeddings(model, df_unanswered['Question'].tolist())

    # Compute cosine similarity matrix
    similarity_matrix = cosine_similarity(embeddings)

    # Find similar groups based on threshold
    threshold = 0.8
    similar_groups = find_similar_groups(similarity_matrix, threshold)

    # Display similar question groups with names
    for group in similar_groups:
        questions_in_group = [df_unanswered.iloc[i]['Question'] for i in group]
        group_name = extract_keywords(questions_in_group)
        print(f"Group Name: {group_name}")
        for question in questions_in_group:
            print(f"- {question}")
        print()  # Blank line for better readability

if __name__ == "__main__":
    main()


from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
import numpy as np

def load_fine_tuned_model():
    # Load the fine-tuned SBERT model
    model = SentenceTransformer('models/fine-tuned-sbert')
    return model

def generate_embeddings(model, questions):
    embeddings = model.encode(questions, convert_to_tensor=True)
    return embeddings.cpu().numpy()

def find_similar_groups(similarity_matrix, threshold=0.8):
    similar_groups = []
    visited = set()
    
    for i in range(similarity_matrix.shape[0]):
        if i in visited:
            continue
        group = [i]
        visited.add(i)
        
        for j in range(i + 1, similarity_matrix.shape[0]):
            if similarity_matrix[i, j] > threshold:
                group.append(j)
                visited.add(j)
        
        similar_groups.append(group)
    
    return similar_groups

def main():
    # Load the fine-tuned model
    model = load_fine_tuned_model()

    # Load your unanswered questions
    df_unanswered = pd.read_csv('data/unanswered_questions.csv')  # Update path as needed

    # Generate embeddings
    embeddings = generate_embeddings(model, df_unanswered['Question'].tolist())

    # Compute cosine similarity matrix
    similarity_matrix = cosine_similarity(embeddings)

    # Find similar groups based on threshold
    threshold = 0.8
    similar_groups = find_similar_groups(similarity_matrix, threshold)

    # Display similar question groups
    for group in similar_groups:
        questions_in_group = [df_unanswered.iloc[i]['Question'] for i in group]
        print("Similar Questions Group:")
        for question in questions_in_group:
            print(f"- {question}")
        print()  # Blank line for better readability

if __name__ == "__main__":
    main()

sentence1,sentence2,label
"How do I install Jenkins on Ubuntu?", "What are the steps to set up Jenkins on an Ubuntu server?",1
"How can I create a new repository in Bitbucket?", "What's the process for initializing a repository on Bitbucket?",1
"How to configure Sonarqube for code quality analysis?", "What settings should I adjust in Sonarqube to evaluate code quality?",1
"How do I use Ansible to automate server deployments?", "What are the Ansible commands for automating the deployment of servers?",1
"How to integrate Confluence with Jira?", "What is the method to link Confluence pages to Jira tickets?",1
"How do I troubleshoot Jenkins build failures?", "What steps should I take to fix failed builds in Jenkins?",1
"How to set up Bitbucket pipelines for CI/CD?", "What is the procedure to configure CI/CD pipelines in Bitbucket?",1
"How can Sonarqube help in maintaining code standards?", "In what ways does Sonarqube assist with enforcing coding standards?",1
"How to manage playbooks in Ansible?", "What is the best practice for organizing Ansible playbooks?",1
"How do I create Jira dashboards for project tracking?", "What's the way to set up dashboards in Jira for monitoring projects?",1
"How to secure Jenkins with authentication?", "What methods can I use to add authentication to Jenkins?",1
"How can I merge branches in Bitbucket?", "What's the process for merging different branches in Bitbucket repositories?",1
"How to analyze code smells using Sonarqube?", "What steps do I follow to detect code smells with Sonarqube?",1
"How do I deploy applications using Ansible?", "What are the commands to deploy apps through Ansible?",1
"How to collaborate using Confluence and Jira together?", "What is the way to use Confluence and Jira for team collaboration?",1
"How to resolve Jenkins plugin compatibility issues?", "What should I do if Jenkins plugins are not compatible with each other?",1
"How to set up access controls in Bitbucket?", "What's the method to configure user permissions in Bitbucket?",1
"How can Sonarqube integrate with Jenkins?", "What is the process to connect Sonarqube with Jenkins for continuous integration?",1
"How to write Ansible roles for infrastructure management?", "What steps are involved in creating Ansible roles for managing infrastructure?",1
"How to track issues in Jira using Confluence?", "What is the way to monitor Jira issues through Confluence pages?",1
"How do I update Jenkins to the latest version?", "What's the procedure for upgrading Jenkins to its newest release?",1
"How can I clone a Bitbucket repository locally?", "What commands do I use to clone a repository from Bitbucket to my machine?",1
"How to interpret Sonarqube analysis reports?", "What should I understand from the reports generated by Sonarqube?",1
"How to configure Ansible inventory files?", "What is the method to set up inventory files in Ansible?",1
"How do I create a Confluence space for my team?", "What's the process to establish a new space in Confluence for team collaboration?",1
"How to assign tasks in Jira projects?", "What steps do I follow to allocate tasks within Jira project boards?",1
"How to back up Jenkins configurations?", "What's the way to create backups of Jenkins setup and configurations?",1
"How can I enable branch permissions in Bitbucket?", "What is the method to set branch access controls in Bitbucket?",1
"How to set quality gates in Sonarqube?", "What steps do I take to establish quality gates within Sonarqube?",1
"How do I use Ansible to manage Docker containers?", "What are the Ansible playbooks for handling Docker container deployments?",1
"How to link Jira issues to Confluence documents?", "What's the procedure to associate Jira tickets with Confluence pages?",1


from sentence_transformers import SentenceTransformer, InputExample, losses
from torch.utils.data import DataLoader
import pandas as pd
import torch
import logging

# Enable logging
logging.basicConfig(format='%(asctime)s - %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S',
                    level=logging.INFO)

def main():
    # Step 1: Load the dataset
    df = pd.read_csv('data/similar_questions.csv')

    # Step 2: Prepare the data
    train_examples = []
    for index, row in df.iterrows():
        train_examples.append(
            InputExample(texts=[row['sentence1'], row['sentence2']], label=row['label'])
        )

    # Step 3: Load the pre-trained SBERT model from local directory
    model_path = 'models/all-MiniLM-L6-v2'  # Update this path accordingly
    model = SentenceTransformer(model_path)

    # Step 4: Define the training objective
    # For regression (similarity scores)
    train_loss = losses.CosineSimilarityLoss(model)

    # For classification (binary labels), uncomment the following:
    # train_loss = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=2)

    # Step 5: Create a DataLoader
    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)

    # Step 6: Fine-tune the model
    num_epochs = 4
    warmup_steps = int(len(train_dataloader) * num_epochs * 0.1)  # 10% of train data

    model.fit(
        train_objectives=[(train_dataloader, train_loss)],
        epochs=num_epochs,
        warmup_steps=warmup_steps,
        device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    )

    # Step 7: Save the fine-tuned model
    model.save('models/fine-tuned-sbert')

if __name__ == "__main__":
    main()


from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
import numpy as np

def load_fine_tuned_model():
    # Load the fine-tuned SBERT model
    model = SentenceTransformer('models/fine-tuned-sbert')
    return model

def generate_embeddings(model, questions):
    embeddings = model.encode(questions, convert_to_tensor=True)
    return embeddings.cpu().numpy()

def find_similar_pairs(similarity_matrix, threshold=0.8):
    similar_pairs = np.argwhere(similarity_matrix > threshold)
    # Remove self-pairs and duplicate pairs
    similar_pairs = [(i, j) for i, j in similar_pairs if i < j]
    return similar_pairs

def main():
    # Load the fine-tuned model
    model = load_fine_tuned_model()

    # Load your unanswered questions
    df_unanswered = pd.read_csv('data/unanswered_questions.csv')  # Update path as needed

    # Generate embeddings
    embeddings = generate_embeddings(model, df_unanswered['Question'].tolist())

    # Compute cosine similarity matrix
    similarity_matrix = cosine_similarity(embeddings)

    # Find similar pairs based on threshold
    threshold = 0.8
    similar_pairs = find_similar_pairs(similarity_matrix, threshold)

    # Display similar question pairs
    for i, j in similar_pairs:
        print(f"Question {i+1} and Question {j+1} are similar:")
        print(f"Q{i+1}: {df_unanswered.iloc[i]['Question']}")
        print(f"Q{j+1}: {df_unanswered.iloc[j]['Question']}\n")

if __name__ == "__main__":
    main()

import pandas as pd
import random

# Load all questions
df = pd.read_csv('similar_questions.csv')

# Separate similar and non-similar questions
similar_df = df[df['label'] == 1]
all_questions = df['sentence1'].tolist() + df['sentence2'].tolist()

# Generate negative samples by pairing questions from different tools
negative_samples = []
tools = ['Jenkins', 'Bitbucket', 'Sonarqube', 'Ansible', 'Confluence', 'Jira']

for _ in range(100):  # Adjust the number as needed
    q1 = random.choice(all_questions)
    q2 = random.choice(all_questions)

    # Ensure questions are from different tools
    if any(tool in q1 for tool in tools) and any(tool in q2 for tool in tools):
        tool_q1 = next((tool for tool in tools if tool in q1), None)
        tool_q2 = next((tool for tool in tools if tool in q2), None)
        if tool_q1 != tool_q2:
            negative_samples.append({'sentence1': q1, 'sentence2': q2, 'label': 0})

# Create DataFrame for negative samples
negative_df = pd.DataFrame(negative_samples)

# Combine with similar questions
final_df = pd.concat([similar_df, negative_df], ignore_index=True)

# Shuffle the dataset
final_df = final_df.sample(frac=1).reset_index(drop=True)

# Save to CSV
final_df.to_csv('similar_questions.csv', index=False)


import os
import xml.etree.ElementTree as ET
import shutil
from typing import List, Tuple, Dict

# Mapping of connector namespaces to their corresponding Maven groupId and artifactId
CONNECTOR_DEPENDENCY_MAP = {
    'http://www.mulesoft.org/schema/mule/http': ('org.mule.connectors', 'mule-http-connector'),
    'http://www.mulesoft.org/schema/mule/db': ('org.mule.connectors', 'mule-db-connector'),
    'http://www.mulesoft.org/schema/mule/file': ('org.mule.connectors', 'mule-file-connector'),
    'http://www.mulesoft.org/schema/mule/ftp': ('org.mule.connectors', 'mule-ftp-connector'),
    'http://www.mulesoft.org/schema/mule/jms': ('org.mule.connectors', 'mule-jms-connector'),
    'http://www.mulesoft.org/schema/mule/vm': ('org.mule.connectors', 'mule-vm-connector'),
    # Add more mappings as needed
}

def parse_domain_resources(domain_path: str) -> Tuple[Dict[str, List[ET.Element]], set]:
    resources = {
        'connectors': [],
        'global_configs': [],
        'error_handlers': [],
        'security_configs': [],
        'schedulers': [],
        'transformers': [],
        'spring_beans': [],
        'other': []
    }
    used_namespaces = set()

    config_path = os.path.join(domain_path, "src", "main", "mule", "mule-domain-config.xml")
    tree = ET.parse(config_path)
    root = tree.getroot()

    # Collect used namespaces
    for key, value in root.attrib.items():
        if key.startswith('xmlns:'):
            used_namespaces.add(value)

    for child in root:
        if '}' in child.tag:
            namespace = child.tag.split('}')[0][1:]  # Extract namespace
            used_namespaces.add(namespace)
            tag = child.tag.split('}')[1]
            if any(x in tag for x in ['config', 'configuration']):
                resources['connectors'].append(child)
            elif tag == 'global-property':
                resources['global_configs'].append(child)
            elif tag == 'error-handler':
                resources['error_handlers'].append(child)
            elif 'security' in tag or 'oauth' in tag:
                resources['security_configs'].append(child)
            elif 'scheduler' in tag:
                resources['schedulers'].append(child)
            elif 'transformer' in tag:
                resources['transformers'].append(child)
            elif tag == 'beans':
                resources['spring_beans'].append(child)
            else:
                resources['other'].append(child)

    return resources, used_namespaces

def update_project_config(project_path: str, resources: Dict[str, List[ET.Element]], used_namespaces: set):
    config_path = os.path.join(project_path, "src", "main", "mule", "mule-config.xml")
    tree = ET.parse(config_path)
    root = tree.getroot()

    # Add all resources to the project config
    for resource_list in resources.values():
        for resource in resource_list:
            root.append(resource)

    # Update namespaces
    for namespace in used_namespaces:
        for prefix, uri in root.attrib.items():
            if uri == namespace:
                break
        else:
            # If namespace not found, add it with a generated prefix
            i = 1
            while f'xmlns:ns{i}' in root.attrib:
                i += 1
            root.set(f'xmlns:ns{i}', namespace)

    tree.write(config_path, encoding="UTF-8", xml_declaration=True)

def update_project_pom(project_path: str, domain_path: str, used_namespaces: set):
    domain_pom_path = os.path.join(domain_path, "pom.xml")
    project_pom_path = os.path.join(project_path, "pom.xml")

    domain_tree = ET.parse(domain_pom_path)
    domain_root = domain_tree.getroot()

    project_tree = ET.parse(project_pom_path)
    project_root = project_tree.getroot()

    # Transfer dependencies
    domain_deps = domain_root.find("./{*}dependencies")
    if domain_deps is not None:
        project_deps = project_root.find("./{*}dependencies")
        if project_deps is None:
            project_deps = ET.SubElement(project_root, "dependencies")

        for dep in domain_deps:
            if dep.find("./{*}classifier") is None or dep.find("./{*}classifier").text != "mule-domain":
                project_deps.append(dep)

    # Add connector dependencies based on used namespaces
    for namespace in used_namespaces:
        if namespace in CONNECTOR_DEPENDENCY_MAP:
            group_id, artifact_id = CONNECTOR_DEPENDENCY_MAP[namespace]
            new_dep = ET.SubElement(project_deps, "dependency")
            ET.SubElement(new_dep, "groupId").text = group_id
            ET.SubElement(new_dep, "artifactId").text = artifact_id
            # You might want to specify a version or use a property for versioning
            # ET.SubElement(new_dep, "version").text = "${mule.version}"

    # Transfer properties
    domain_props = domain_root.find("./{*}properties")
    if domain_props is not None:
        project_props = project_root.find("./{*}properties")
        if project_props is None:
            project_props = ET.SubElement(project_root, "properties")

        for prop in domain_props:
            if not project_props.find(f".//{prop.tag}"):
                project_props.append(prop)

    # Remove domain dependency
    for dep in project_root.findall(".//{*}dependency"):
        artifact_id = dep.find("./{*}artifactId")
        classifier = dep.find("./{*}classifier")
        if (artifact_id is not None and artifact_id.text == "my-domain-project" and
            classifier is not None and classifier.text == "mule-domain"):
            project_deps.remove(dep)

    project_tree.write(project_pom_path, encoding="UTF-8", xml_declaration=True)

def migrate_project(domain_path: str, project_path: str):
    print(f"Migrating project: {os.path.basename(project_path)}")

    # Parse domain resources and collect used namespaces
    resources, used_namespaces = parse_domain_resources(domain_path)

    # Update project configuration
    update_project_config(project_path, resources, used_namespaces)

    # Update project pom
    update_project_pom(project_path, domain_path, used_namespaces)

    # Copy any additional resources
    domain_resources_path = os.path.join(domain_path, "src", "main", "resources")
    project_resources_path = os.path.join(project_path, "src", "main", "resources")
    if os.path.exists(domain_resources_path):
        for item in os.listdir(domain_resources_path):
            s = os.path.join(domain_resources_path, item)
            d = os.path.join(project_resources_path, item)
            if os.path.isfile(s):
                shutil.copy2(s, d)

    print(f"Migration completed for project: {os.path.basename(project_path)}")

def main():
    domain_path = "/path/to/domain/project"
    projects = [
        "/path/to/project1",
        "/path/to/project2",
        # Add more project paths as needed
    ]

    for project_path in projects:
        migrate_project(domain_path, project_path)

if __name__ == "__main__":
    main()


