 Here are the common scenarios where symlink creation can fail and their solutions:

1. Target Path Doesn't Exist
```bash
ln -s /nonexistent/path/file.txt link.txt
# Creates broken symlink
```
Fix: Verify the target path exists first
```bash
test -e /path/to/target && ln -s /path/to/target link.txt
```

2. Link Already Exists
```bash
ln -s target.txt link.txt
ln: failed to create symbolic link: File exists
```
Fix: Use -f to force override or remove existing link first
```bash
ln -sf target.txt link.txt
# OR
rm link.txt && ln -s target.txt link.txt
```

3. Wrong Working Directory
```bash
# From wrong directory:
ln -s ../target.txt link.txt
# Creates broken link because relative path is wrong
```
Fix: Use absolute paths or ensure you're in correct directory
```bash
ln -s $(realpath ../target.txt) link.txt
```

4. Permissions Issues
```bash
ln -s /etc/config.txt ~/link.txt
ln: failed to create symbolic link: Permission denied
```
Fix: Check permissions and ownership
```bash
# If creating in protected directory
sudo ln -s target.txt /protected/dir/link.txt

# Or fix directory permissions
chmod +w directory
```

5. Wrong Path Order
```bash
# Wrong order:
ln -s link.txt target.txt
```
Fix: Remember target comes first, then link name
```bash
ln -s target.txt link.txt
```

6. Linking to Another Symlink
```bash
# Can create chains of symlinks - may not be intended
ln -s link1.txt link2.txt
```
Fix: Use readlink to link to actual target
```bash
ln -s $(readlink -f link1.txt) link2.txt
```

7. Cross-Device Links
```bash
# Trying to link across different filesystems
ln: failed to create symbolic link: Invalid cross-device link
```
Fix: Use absolute paths or verify both locations are on same filesystem
```bash
ln -s /absolute/path/to/target.txt /absolute/path/to/link.txt
```

To debug symlink issues:
```bash
# List symlinks with targets
ls -la

# Show real path
readlink -f linkname

# Check if symlink is valid
test -L linkname && test -e linkname
```

These are the most common scenarios you might encounter. Generally, using absolute paths and checking permissions beforehand can help avoid many of these issues.
1. Capturing the Existing Inventory

1.1. Identify Developer Workstations

Collect details of the operating systems (e.g., Windows 10/11), hardware specs, and installed software.

Assess the virtualization readiness (e.g., Hyper-V enabled) of developer machines.

Identify dependencies on tools like WSL, Docker, or Podman.


1.2. Map Technology Ecosystems

Categorize developers based on their tech stack:

Java Ecosystem: Spring Boot, Maven, Gradle.

.NET Ecosystem: .NET Core, MSBuild.

Python Ecosystem: Flask, Django, package managers.

Others (e.g., Node.js, Go).



1.3. Audit Existing Repositories

Identify active code repositories for each ecosystem.

Check for dependencies and existing configurations related to containerization.

Document DevContainer readiness (e.g., Dockerfile or devcontainer.json presence).


1.4. Assess Developer Tools

Inventory IDEs in use (e.g., VS Code, IntelliJ, Visual Studio).

Document existing extensions or plugins relevant to DevContainers.


1.5. Analyze Network and Registry Access

Verify access to Artifactory or other internal container registries.

Check for network constraints in air-gapped environments.




---

2. Running a Pilot

2.1. Define Pilot Objectives

Validate the feasibility of DevContainers across identified ecosystems.

Gather developer feedback on usability and performance.

Identify potential bottlenecks or blockers.


2.2. Select Pilot Groups

Choose a representative sample of developers across different ecosystems.

Include diverse roles (e.g., backend, frontend, full-stack developers).


2.3. Identify Pilot Applications

Select applications that represent typical use cases:

A simple Spring Boot app with database access.

A .NET Core app using MSBuild.

A Python Flask app with dependencies on specific libraries.



2.4. Prepare Pilot Environment

Set up DevContainers for pilot applications with:

Pre-configured devcontainer.json and Dockerfile.

Base images (e.g., RHEL 9 Micro, JDK 17).

Relevant build tools (e.g., Maven, MSBuild).


Provide access to Artifactory for base image pulling.

2.5. Conduct Pilot Testing

Guide pilot participants through:

Setting up DevContainers in their IDEs.

Running their pilot applications within containers.

Observing build and runtime performance.


Collect feedback via surveys or interviews.


2.6. Document Findings

Capture issues encountered during the pilot (e.g., compatibility, performance).

Summarize developer feedback on workflows and setup time.

List recommendations for improving the rollout process.




---

3. Identifying Ecosystems and AIT Applications

3.1. Prioritize Ecosystems

Rank ecosystems based on:

Developer population.

Strategic importance.

Complexity of integration with DevContainers

3.2. Select AIT Applications

Identify applications critical for cross-system integration testing.

Ensure AIT applications cover a range of dependencies and use cases:

Applications that interact with databases (e.g., PostgreSQL, MySQL).

Applications with complex build processes (e.g., multi-module projects).



3.3. Validate Dependencies

Test container compatibility with dependencies required by AIT applications.

Document environment setup needs for each AIT application.


3.4. Prepare Reusable Templates

Create devcontainer.json and Dockerfile templates tailored to AIT applications.

Include common configurations (e.g., database connectivity, port mappings).


4. Final Rollout

4.1. Refine Based on Pilot Feedback

Address issues identified during the pilot phase (e.g., IDE plugin compatibility, image performance).

Incorporate best practices and developer suggestions into templates and documentation.


4.2. Communicate Rollout Plan

Share a detailed rollout schedule with teams.

Provide updates on what to expect, timelines, and support availability.


4.3. Enable Tooling

Ensure Artifactory and container registries are ready to serve production-scale usage.

Finalize installer bundles (e.g., MSI/MSIX for WSL2 and Podman Desktop).


4.4. Rollout in Phases

Phase 1: Early adopters and high-priority teams.

Phase 2: Broader rollout to all developer groups.

Phase 3: Specialized use cases and exceptions.


4.5. Provide Training and Support

Conduct workshops and hands-on sessions for developers.

Share detailed documentation (setup guides, FAQs, troubleshooting steps).

Set up dedicated support channels for issue resolution.


4.6. Monitor Adoption

Track key metrics:

Number of developers onboarded.

Setup time reductions.

Feedback on developer productivity.


Address any ongoing issues or roadblocks.


4.7. Gather Feedback and Iterate

Collect feedback post-rollout through surveys or retrospectives.

Continuously refine templates, configurations, and processes.


1. Capturing Existing Inventory

Identify Developer Workstations: Collect details on OS, hardware, and software.

Map Ecosystems: Categorize developers by tech stack (Java, .NET, Python, etc.).

Audit Repositories: Identify DevContainer readiness in active codebases.

Assess Developer Tools: Check IDEs, extensions, and configuration status.

Analyze Network Access: Verify Artifactory or container registry access.


2. Running a Pilot

Define Objectives: Validate DevContainers' feasibility and gather feedback.

Select Pilot Groups: Choose diverse developers from different ecosystems.

Identify Pilot Applications: Select representative apps for testing (e.g., Spring Boot, .NET Core, Python Flask).

Prepare Pilot Environment: Set up DevContainer configurations and provide tools.

Conduct Testing: Test the environment with pilot apps, gather feedback, and address issues.

Document Findings: Record issues, feedback, and improvements.



3. Identifying Pilot Applications or AIT (Application Integration Testing)

Prioritize Ecosystems: Focus on critical tech stacks and ecosystems.

Select AIT Applications: Choose apps that require cross-system integration testing.

Validate Dependencies: Test compatibility with databases, tools, and libraries.

Create Reusable Templates: Prepare DevContainer templates for pilot apps.


4. Final Rollout

Refine Based on Pilot Feedback: Implement improvements from the pilot phase.

Communicate Rollout Plan: Share schedule and details with teams.

Enable Tooling: Finalize container registries, image access, and deployment tools.

Rollout in Phases: Begin with early adopters and gradually expand.

Provide Training and Support: Deliver training sessions, documentation, and support channels.

Monitor Adoption: Track usage, gather feedback, and resolve issues.

Iterate: Continuously improve based on post-rollout feedback


Thought for 4 seconds
Certainly! Here's a consolidated list of the major topics and subtopics for your DevContainers rollout project, including the user interface (UI) part. This comprehensive outline is designed to help you visualize and plan the key areas you need to address.

1. Centralized Base Image Creation and Management
1.1. Base Image Selection

Choose RHEL 9 Micro as the initial base OS image.
Decide on Windows 11 base images for future expansion.
1.2. Baked-in SDKs

Include standard SDKs in base images (e.g., JDK 17, .NET Core 6.0, Python 3.9).
Ensure consistency across development environments.
1.3. Image Customization

Configure images with organizational settings.
Apply necessary configurations and optimizations.
1.4. Version Control and Updates

Establish versioning strategy for base images.
Plan regular updates (e.g., quarterly).
Automate image build, test, and deployment processes.
1.5. Internal Container Registry Management

Use Artifactory for hosting and distributing images.
Implement access controls and permissions.
Synchronize images across multiple sites if necessary.
2. Developer Environment Enablement
2.1. Packaging WSL2, Podman Desktop, and Hyper-V Enablement

2.1.1. Bundle Creation

Package RHEL Micro OS Image, WSL2, Podman Desktop, and scripts into a single installer (MSI/MSIX).
Include scripts to enable Hyper-V and virtualization features.
2.1.2. Automated Deployment via SCCM

Use Microsoft SCCM for mass deployment.
Configure silent installations and handle reboots seamlessly.
Monitor deployment status and troubleshoot failures.
2.1.3. Compatibility Considerations

Ensure compatibility with Windows 10/11 versions.
Test on various hardware configurations used by developers.
2.1.4. BIOS Settings Management

Address scenarios where BIOS settings need changes (e.g., enable VT-x).
Provide guidance or tools for BIOS configuration if possible.
3. DevContainer Configuration Enablement
3.1. User Interface for Configuration Generation

3.1.1. UI Design and Development

Create a web-based tool for generating devcontainer.json and Dockerfile.
Design the UI to be intuitive and user-friendly.
3.1.2. Support for Multiple Languages and Frameworks

Include support for Java, .NET, Python, Node.js, Go, etc.
Allow selection of pre-baked SDKs from base images.
3.1.3. Mandatory and Tech-Specific Fields

Capture mandatory fields common across all tech stacks.
Provide tech-specific fields that dynamically appear based on language selection.
3.1.4. Configuration Customization

Allow users to specify build tools (e.g., Maven, Gradle, MSBuild).
Enable adding additional tools and dependencies as needed.
3.2. Template Management

Provide templates for common project types.
Allow users to customize templates according to project requirements.
3.3. Integration with Repositories

Facilitate easy addition of configurations to new or existing repositories.
Provide instructions or tools for integrating with version control systems.
3.4. Documentation and Guidance

Offer step-by-step guides on using the UI tool.
Include best practices for creating and maintaining DevContainer configurations.
4. IDE Compatibility and Configuration
4.1. Ensuring IDE Support for DevContainers

4.1.1. Visual Studio Code (VS Code)

Distribute the Remote - Containers extension as a .vsix file.
Provide instructions for offline installation.
Ensure developers have compatible versions of VS Code.
4.1.2. JetBrains IDEs (IntelliJ IDEA, PyCharm)

Identify required plugins (e.g., Docker plugin).
Host plugins internally and provide installation guidance.
Verify DevContainer support and provide workarounds if necessary.
4.1.3. Visual Studio

Explore Docker support within Visual Studio.
Provide guidance on using Docker features in lieu of DevContainers.
4.2. Extension and Plugin Distribution

Host required extensions/plugins on Artifactory or internal repositories.
Automate installation processes where possible.
4.3. IDE Version Management

Determine minimum required versions for IDEs.
Distribute updates or installers for IDEs as needed.
4.4. Configuration Automation

Provide scripts or tools to automate IDE configuration.
Distribute pre-configured settings or profiles.
4.5. Training and Support

Develop guides for setting up IDEs with DevContainers.
Establish support channels for assistance.
5. Tooling and Integration
5.1. CI/CD Pipeline Integration

Adapt CI/CD pipelines to work with containerized development environments.
Use containerized build agents for consistency.
5.2. Version Control Systems

Integrate DevContainer configurations with Git repositories.
Standardize workflows for adding and updating configurations.
5.3. Package Repository Management

Mirror external package repositories in Artifactory.
Configure build tools to use internal repositories by default.
5.4. Integration with Development Tools

Ensure seamless interaction between DevContainers and development tools.
Provide necessary configurations for tools like Git, debugging utilities, etc.
6. Update and Maintenance Strategy
6.1. Regular Updates Schedule

Define update cycles for base images and tools (e.g., quarterly).
Communicate schedules to developers to minimize disruptions.
6.2. Automated Update Deployment

Use SCCM to push updates to WSL2, Podman Desktop, and base images.
Automate testing of updates before wide deployment.
6.3. Configuration Management

Employ tools like Ansible or Puppet for maintaining configurations.
Keep infrastructure as code for reproducibility.
6.4. Rollback Procedures

Establish rollback plans in case updates cause issues.
Maintain previous versions for emergency use.
7. Developer Training and Support
7.1. Training Programs

Develop training materials and workshops to onboard developers.
Cover using WSL2, Podman Desktop, and DevContainers.
7.2. Documentation

Create comprehensive documentation, FAQs, and troubleshooting guides.
Maintain an internal knowledge base or wiki.
7.3. Support Channels

Establish support teams or help desks.
Provide channels for feedback and issue reporting.
7.4. Communication Strategy

Communicate changes and updates proactively.
Use newsletters, meetings, or internal portals to keep developers informed.
8. Performance and Scalability
8.1. Resource Optimization

Optimize containers for performance.
Configure resource limits to prevent host system overload.
8.2. Scalability Planning

Plan infrastructure to handle scaling to 30,000+ developers.
Consider network bandwidth and storage implications.
8.3. Monitoring and Analytics

Implement monitoring for resource usage and performance metrics.
Use data to optimize configurations and identify bottlenecks.
9. Feedback and Continuous Improvement
9.1. Feedback Mechanisms

Provide channels for developers to submit feedback.
Conduct surveys or feedback sessions.
9.2. Iterative Improvements

Use feedback to enhance tools and processes.
Adopt an agile approach to adapt to changing needs.
9.3. Community of Practice

Encourage knowledge sharing among developers.
Establish forums or groups for collaboration.
10. Additional Considerations
10.1. Air-Gapped Environment Challenges

Address limitations due to lack of internet access.
Provide offline access to essential documentation and resources.
10.2. Hardware and BIOS Settings

Develop strategies for enabling virtualization in BIOS settings.
Coordinate with IT teams for hardware configurations.
10.3. Pilot Testing

Start with a pilot group to test the rollout.
Use feedback to refine the deployment process.
10.4. Future Expansion

Plan for supporting additional IDEs (e.g., Eclipse) if demand arises.
Consider expanding base images to include Windows 11.
Summary
This list encompasses all the major topics and subtopics you need to consider for your DevContainers rollout project, including the development of a user interface for generating DevContainer configurations. By focusing on these areas, you can create a comprehensive plan that addresses the key aspects of:

Centralized management of base images with baked-in SDKs.
Deployment of WSL2, Podman Desktop, and enabling virtualization features across a large developer base.
Development of a UI tool to assist developers in generating devcontainer.json and Dockerfile configurations.
Ensuring IDE compatibility and providing necessary extensions and plugins within an air-gapped environment.
Integration with existing tooling, version control systems, and CI/CD pipelines.
Establishing a robust update and maintenance strategy.
Providing training, documentation, and support to developers.
Planning for performance optimization and scalability.
Creating feedback mechanisms for continuous improvement.
Next Steps:

Review and Prioritize Topics

Assess which areas require immediate attention.
Prioritize tasks based on impact and dependencies.
Assign Responsibilities

Delegate tasks to appropriate teams or individuals.
Establish clear ownership for each area.
Develop a Timeline

Create a project plan with milestones and deadlines.
Include pilot testing phases and feedback incorporation.
Communicate the Plan

Share the roadmap with stakeholders and development teams.
Set expectations and provide channels for queries.


FROM mcr.microsoft.com/devcontainers/java:17

ARG INSTALL_MAVEN="true"
ARG MAVEN_VERSION=""

ARG INSTALL_GRADLE="false"
ARG GRADLE_VERSION=""

RUN if [ "${INSTALL_MAVEN}" = "true" ]; then su vscode -c "umask 0002 && . /usr/local/sdkman/bin/sdkman-init.sh && sdk install maven \"${MAVEN_VERSION}\""; fi \
    && if [ "${INSTALL_GRADLE}" = "true" ]; then su vscode -c "umask 0002 && . /usr/local/sdkman/bin/sdkman-init.sh && sdk install gradle \"${GRADLE_VERSION}\""; fi

# [Optional] Install additional OS packages.
RUN apt-get update && export DEBIAN_FRONTEND=noninteractive \
    && apt-get -y install --no-install-recommends git curl

# Create target directory and set permissions
RUN mkdir -p /workspaces/spring-boot-devcontainer/target \
    && chown -R vscode:vscode /workspaces/spring-boot-devcontainer

{
    "name": "Spring Boot DevContainer",
    "dockerFile": "Dockerfile",
    "customizations": {
        "vscode": {
            "extensions": [
                "vscjava.vscode-java-pack",
                "vmware.vscode-spring-boot",
                "pivotal.vscode-spring-boot-dashboard",
                "vscjava.vscode-spring-initializr",
                "redhat.vscode-xml",
                "vscjava.vscode-maven"
            ],
            "settings": {
                "java.configuration.updateBuildConfiguration": "automatic",
                "java.server.launchMode": "Standard"
            }
        }
    },
    "forwardPorts": [8080],
    "postCreateCommand": "sudo chown -R vscode:vscode /workspaces/spring-boot-devcontainer",
    "remoteUser": "vscode",
    "mounts": [
        "source=${localWorkspaceFolder}/target,target=/workspaces/spring-boot-devcontainer/target,type=bind,consistency=cached"
    ]
}

def is_undelivered_notification(email):
    """Check if email is an undelivery notification based on content analysis"""
    content = email.text_body.lower() if hasattr(email, 'text_body') else ''

    undelivery_patterns = [
        "delivery has failed to these recipients or groups",
        "recipient not found by exchange legacy encapsulated email address lookup",
        "resolver.adr.exrecipnotfound",
        "the email address you entered couldn't be found",
        "diagnostic information for administrators:",
        "remote server returned '550"
    ]

    return any(pattern in content.lower() for pattern in undelivery_patterns)


Key Questions to Ask During KT
Configuration and Structure

What is the overall structure of our devcontainer configuration?
Which base images are we using, and why were they selected?
Dependencies and Tools

How are dependencies managed and installed within the devcontainer?
What VS Code extensions and other tools are pre-installed, and are they essential for all team members?
Environment Management

How do we handle environment variables and sensitive information within the devcontainer?
Are there specific scripts or tooling that automate parts of the setup?
Workflow Integration

How does the devcontainer integrate with our existing CI/CD pipelines?
What is the process for onboarding new team members using the devcontainer?
Maintenance and Updates

How are updates to the devcontainer (e.g., base image updates, dependency upgrades) managed and communicated?
Is there a schedule or trigger for reviewing and updating the devcontainer setup?
Best Practices and Standards

What best practices are we following to ensure consistency and security in our devcontainer configurations?
Are there any coding or configuration standards specific to our devcontainer setup?
Troubleshooting and Support

What are the most common issues encountered with our devcontainer, and how are they resolved?
Where can I find documentation or resources for troubleshooting devcontainer-related problems?
Customization and Extensibility

How can I customize the devcontainer for specific project needs without disrupting the standard setup?
Are there guidelines for adding new tools or dependencies to the devcontainer?
Performance and Optimization

What measures are in place to ensure the devcontainer performs efficiently?
Are there any known performance bottlenecks or areas for improvement?
Collaboration and Sharing

How do we share devcontainer configurations across different projects or teams?
Is there a centralized repository or standard template that we use for devcontainers?

def convert_plain_to_html(plain_text):
    """Convert plain text to HTML format using pre tag."""
    if not plain_text:
        return ""

    # Only need to escape HTML special characters
    escaped_text = html.escape(plain_text)

    return f"""
    <html>
        <body>
            <pre style="font-family: Arial, sans-serif; white-space: pre-wrap; margin: 0;">{escaped_text}</pre>
        </body>
    </html>
    """

from email.parser import BytesParser
from email.policy import default as default_policy
import io
import logging
import base64
import html

def convert_plain_to_html(plain_text):
    """Convert plain text to simple HTML format."""
    if not plain_text:
        return ""

    # Escape HTML special characters
    escaped_text = html.escape(plain_text)
    # Convert newlines to <br> tags and preserve spacing
    formatted_text = escaped_text.replace('\n', '<br>').replace('  ', '&nbsp;&nbsp;')

    return f"""
    <html>
        <body style="font-family: Arial, sans-serif; white-space: pre-wrap;">
            {formatted_text}
        </body>
    </html>
    """

def create_email_to_html_with_images(email):
    """Convert email to HTML with embedded images, handling both HTML and plain text emails."""
    try:
        html_parts = []
        plain_parts = []  # Store plain text parts as fallback
        image_cid_map = {}

        msg = BytesParser(policy=default_policy).parsebytes(email.mime_content)

        # First pass: Extract images
        for part in msg.walk():
            content_type = part.get_content_type()
            if "image" in content_type:
                cid = part.get("Content-ID")
                if cid:
                    cid = cid.strip('<>')
                    image_data = part.get_payload(decode=True)
                    if image_data:
                        encoded_data = base64.b64encode(image_data).decode('utf-8')
                        image_cid_map[cid] = f"data:{content_type};base64,{encoded_data}"

        # Second pass: Build HTML content
        for part in msg.walk():
            content_type = part.get_content_type()

            if content_type == "text/html":
                charset = part.get_content_charset() or 'utf-8'
                html_content = part.get_payload(decode=True).decode(charset)

                # Replace image references
                for cid, data_uri in image_cid_map.items():
                    html_content = html_content.replace(f'cid:{cid}', data_uri)

                html_parts.append(html_content)

            elif content_type == "text/plain":
                charset = part.get_content_charset() or 'utf-8'
                plain_content = part.get_payload(decode=True).decode(charset)
                plain_parts.append(plain_content)

        # If no HTML parts were found, convert plain text to HTML
        if not html_parts and plain_parts:
            logging.info("No HTML parts found, converting plain text to HTML")
            plain_text = '\n'.join(plain_parts)
            html_content = convert_plain_to_html(plain_text)
        else:
            html_content = ''.join(html_parts)

        # Validate we have content
        if not html_content.strip():
            logging.error("No content found in email (neither HTML nor plain text)")
            # You might want to raise an exception here or return a default HTML message
            html_content = "<html><body><p>No content found in email.</p></body></html>"

        html_file = io.BytesIO(html_content.encode('utf-8'))
        return html_file

    except Exception as e:
        logging.error(f"Error processing email: {str(e)}")
        raise

PROD_JSON = {
    "PROJECT_KEY": "PROD_KEY",
    "JIRA_EPIC_KEY": "PROD_EPIC",
    "TEAMHQ_TEAM": "123809",
    "IMPACTED_AITS": ["91673"],
    "ASSIGNEE": "zk9écvk",
    "PEOPLE_TESTING_DEPENDENCY": ["VY -- GLOBAL TECHNOLOGY AND OPERATIONS"],
    "TEST_CASE_TOOLS": ["Not Applicable"]
}

DEV_JSON = {
    "PROJECT_KEY": "DEV_KEY",
    "JIRA_EPIC_KEY": "DEV_EPIC",
    "TEAMHQ_TEAM": "123810",
    "IMPACTED_AITS": ["91674"]
}

# config.py
import os
import json
from typing import Dict, Any
from pathlib import Path

def load_env_specific_values(env: str = None) -> Dict[str, Any]:
    """
    Load environment-specific values from JSON file.
    If env is None, defaults are used (for local development)
    """
    if env is None:
        env = os.getenv('JIRA_ENV', 'dev')

    config_path = Path(__file__).parent / "config" / f"{env}.json"

    # Default values (can be overridden by environment variables)
    defaults = {
        'PROJECT_KEY': 'DEFAULT_PROJECT',
        'JIRA_EPIC_KEY': 'DEFAULT_EPIC',
        'TEAMHQ_TEAM': '123810',  # dev default
        'IMPACTED_AITS': ['91674'],  # dev default
        'ASSIGNEE': None,
        'PEOPLE_TESTING_DEPENDENCY': None,
        'TEST_CASE_TOOLS': None
    }

    # Load from JSON if exists
    if config_path.exists():
        with open(config_path) as f:
            env_config = json.load(f)
            defaults.update(env_config)

    # Environment variables override JSON
    env_vars = {
        'PROJECT_KEY': os.getenv('JIRA_PROJECT_KEY'),
        'JIRA_EPIC_KEY': os.getenv('JIRA_EPIC_KEY'),
        'TEAMHQ_TEAM': os.getenv('JIRA_TEAMHQ_TEAM'),
        'IMPACTED_AITS': os.getenv('JIRA_IMPACTED_AITS', '').split(',') if os.getenv('JIRA_IMPACTED_AITS') else None,
        'ASSIGNEE': os.getenv('JIRA_ASSIGNEE'),
        'PEOPLE_TESTING_DEPENDENCY': os.getenv('JIRA_PEOPLE_TESTING_DEP', '').split(',') if os.getenv('JIRA_PEOPLE_TESTING_DEP') else None,
        'TEST_CASE_TOOLS': os.getenv('JIRA_TEST_CASE_TOOLS', '').split(',') if os.getenv('JIRA_TEST_CASE_TOOLS') else None
    }

    # Update only if environment variable is set
    for key, value in env_vars.items():
        if value is not None:
            defaults[key] = value

    return defaults

def get_field_config() -> Dict[str, Dict]:
    """
    Get the field configuration with environment-specific values
    """
    env_values = load_env_specific_values()

    static_fields = {
        'project': {'key': env_values['PROJECT_KEY']},
        'Issue Type': {'name': 'Story'},
        'Acceptance Criteria': 'Given a Bank technologist requires assistance\n'
                             'When the user provides the issue details\n'
                             'Then the EDT developer helps in assisting the user in remediating their issue',
        'Priority': {'name': 'Trivial'},
        'Story Type': {'value': 'Enabler'},
        'Data Testing Dependency': [{'value': 'Not Applicable'}],
        'Activity Categories': {
            'value': 'L3',
            'child': {
                'value': 'Research and Respond to Application, Enterprise, External Regulatory or Audit Inquiries'
            }
        },
        'Story Points': 1,
        'TeamHQ Team': env_values['TEAMHQ_TEAM'],
        'Epic Link': env_values['JIRA_EPIC_KEY'],
        'Impacted AIT\'s': env_values['IMPACTED_AITS'],
        'Test Scope': {'value': 'No Testing'},
        'No Testing Justification': {'value': 'Other'},
        'No Testing Justification Other': 'No Testing is required'
    }

    # Add environment-specific fields only if they exist
    if env_values['ASSIGNEE']:
        static_fields['Assignee'] = {'name': env_values['ASSIGNEE']}

    if env_values['PEOPLE_TESTING_DEPENDENCY']:
        static_fields['People Testing Dependency'] = [
            {'value': dep} for dep in env_values['PEOPLE_TESTING_DEPENDENCY']
        ]

    if env_values['TEST_CASE_TOOLS']:
        static_fields['Test Case Management Tools'] = [
            {'value': tool} for tool in env_values['TEST_CASE_TOOLS']
        ]

    return {
        'static_fields': static_fields,
        'dynamic_fields': {
            'Summary': 'Summary',
            'Description': 'Description',
            'Labels': 'Labels'
        }
    }

# The main configuration object
field_config = get_field_config()

# Example environment-specific JSON files
PROD_JSON = {
    "PROJECT_KEY": "PROD_KEY",
    "JIRA_EPIC_KEY": "PROD_EPIC",
    "TEAMHQ_TEAM": "123809",
    "IMPACTED_AITS": ["91673"],
    "ASSIGNEE": "zk9écvk",
    "PEOPLE_TESTING_DEPENDENCY": ["VY -- GLOBAL TECHNOLOGY AND OPERATIONS"],
    "TEST_CASE_TOOLS": ["Not Applicable"]
}

DEV_JSON = {
    "PROJECT_KEY": "DEV_KEY",
    "JIRA_EPIC_KEY": "DEV_EPIC",
    "TEAMHQ_TEAM": "123810",
    "IMPACTED_AITS": ["91674"]
}

# Create example config files if running directly
if __name__ == "__main__":
    config_dir = Path(__file__).parent / "config"
    config_dir.mkdir(exist_ok=True)

    with open(config_dir / "prod.json", "w") as f:
        json.dump(PROD_JSON, f, indent=2)

    with open(config_dir / "dev.json", "w") as f:
        json.dump(DEV_JSON, f, indent=2)

import os
import platform
import subprocess
import sys

def set_env_variable_windows(var, value):
    try:
        # Use setx to set the environment variable for the current user
        subprocess.run(['setx', var, value], check=True)
        print(f"Set {var}={value} permanently for the user on Windows.")
    except subprocess.CalledProcessError as e:
        print(f"Failed to set {var} on Windows: {e}")

def set_env_variable_linux(var, value):
    bashrc_path = os.path.expanduser("~/.bashrc")
    export_line = f'\nexport {var}="{value}"\n'
    
    # Check if the variable already exists in .bashrc
    if os.path.exists(bashrc_path):
        with open(bashrc_path, 'r') as file:
            lines = file.readlines()
        
        for i, line in enumerate(lines):
            if line.startswith(f'export {var}='):
                lines[i] = export_line
                break
        else:
            lines.append(export_line)
    else:
        lines = [export_line]
    
    # Append or modify the .bashrc file
    with open(bashrc_path, 'w') as file:
        file.writelines(lines)
    
    print(f"Set {var}={value} permanently in {bashrc_path}.")
    print("Run `source ~/.bashrc` or restart your terminal to apply changes.")

def main():
    if len(sys.argv) < 2:
        print("Usage: python set_env_vars.py VAR1=VALUE1 VAR2=VALUE2 ...")
        sys.exit(1)
    
    # Parse key=value pairs from command-line arguments
    env_vars = {}
    for arg in sys.argv[1:]:
        if '=' in arg:
            key, value = arg.split('=', 1)
            env_vars[key] = value
        else:
            print(f"Invalid argument '{arg}'. Expected format VAR=VALUE.")
            sys.exit(1)
    
    current_os = platform.system()
    
    if current_os == "Windows":
        for var, value in env_vars.items():
            set_env_variable_windows(var, value)
    elif current_os in ["Linux", "Darwin"]:  # Darwin for macOS
        for var, value in env_vars.items():
            set_env_variable_linux(var, value)
    else:
        print(f"Unsupported OS: {current_os}")
        sys.exit(1)

if __name__ == "__main__":
    main()

from jira import JIRA
import pandas as pd
import os
from datetime import datetime
import concurrent.futures
from typing import Dict, List, Any
import time
from tqdm import tqdm

def connect_to_jira(jira_server: str, username: str, api_token: str) -> JIRA:
    """
    Connect to JIRA instance using basic authentication
    """
    try:
        jira = JIRA(server=jira_server, basic_auth=(username, api_token))
        return jira
    except Exception as e:
        print(f"Failed to connect to JIRA: {str(e)}")
        return None

def format_comments(comments: List[Any]) -> str:
    """
    Format comments with delimiter and user information
    Comments are sorted chronologically with oldest first
    """
    if not comments:
        return ""

    formatted_comments = []
    delimiter = "================\n"

    # Sort comments by created date
    sorted_comments = sorted(comments, key=lambda x: x.created)

    for comment in sorted_comments:
        comment_text = f"Author: {comment.author.displayName}\n"
        comment_text += f"Date: {comment.created}\n"
        comment_text += f"Comment:\n{comment.body}\n"
        formatted_comments.append(comment_text)

    return delimiter.join(formatted_comments)

def process_single_ticket(args: tuple) -> Dict[str, str]:
    """
    Process a single JIRA ticket
    Returns a dictionary with ticket data
    """
    ticket_id, jira_server, username, api_token = args

    try:
        # Create a new JIRA connection for each thread
        jira = connect_to_jira(jira_server, username, api_token)
        if not jira:
            return {
                'ticket_id': ticket_id,
                'summary': 'ERROR - Connection Failed',
                'description': 'ERROR - Connection Failed',
                'comments': 'ERROR - Connection Failed'
            }

        issue = jira.issue(ticket_id)

        # Get comments
        comments = jira.comments(issue)
        formatted_comments = format_comments(comments)

        return {
            'ticket_id': ticket_id,
            'summary': issue.fields.summary,
            'description': issue.fields.description or "",
            'comments': formatted_comments
        }

    except Exception as e:
        return {
            'ticket_id': ticket_id,
            'summary': f'ERROR: {str(e)}',
            'description': f'ERROR: {str(e)}',
            'comments': f'ERROR: {str(e)}'
        }

def process_jira_tickets(jira_server: str, username: str, api_token: str,
                        input_csv: str, output_excel: str, max_workers: int = 10):
    """
    Main function to process JIRA tickets in parallel and create Excel output
    """
    try:
        # Read ticket IDs from CSV
        df = pd.read_csv(input_csv)
        ticket_column = df.columns[0]  # Assume first column contains ticket IDs
        ticket_ids = df[ticket_column].tolist()

        # Prepare arguments for parallel processing
        args_list = [(ticket_id, jira_server, username, api_token)
                    for ticket_id in ticket_ids]

        results = []

        # Process tickets in parallel with progress bar
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(process_single_ticket, args)
                      for args in args_list]

            # Show progress bar
            for future in tqdm(concurrent.futures.as_completed(futures),
                             total=len(ticket_ids),
                             desc="Processing tickets"):
                results.append(future.result())

        # Sort results by original ticket order
        ticket_id_to_index = {tid: idx for idx, tid in enumerate(ticket_ids)}
        results.sort(key=lambda x: ticket_id_to_index[x['ticket_id']])

        # Create output DataFrame
        output_df = pd.DataFrame({
            'Ticket ID': [r['ticket_id'] for r in results],
            'Summary': [r['summary'] for r in results],
            'Description': [r['description'] for r in results],
            'Comments': [r['comments'] for r in results]
        })

        # Write to Excel
        output_df.to_excel(output_excel, index=False, engine='openpyxl')
        print(f"\nSuccessfully created Excel file: {output_excel}")

    except Exception as e:
        print(f"An error occurred: {str(e)}")

if __name__ == "__main__":
    # Configuration
    JIRA_SERVER = "https://your-jira-instance.com"
    USERNAME = "your-username"
    API_TOKEN = "your-api-token"

    # File paths
    INPUT_CSV = "jira_tickets.csv"
    OUTPUT_EXCEL = f"jira_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"

    # Number of parallel workers
    MAX_WORKERS = 10  # Adjust based on your system capabilities and JIRA rate limits

    # Run the process
    start_time = time.time()
    process_jira_tickets(
        JIRA_SERVER,
        USERNAME,
        API_TOKEN,
        INPUT_CSV,
        OUTPUT_EXCEL,
        MAX_WORKERS
    )
    end_time = time.time()

    print(f"Total processing time: {end_time - start_time:.2f} seconds")


GIVEN

The current source code contains hard-coded secrets (such as API keys, passwords) and a hard-coded nbkid identifier.
These hard-coded values are embedded directly within the application's codebase, posing security risks and hindering maintainability.
WHEN

A developer refactors the source code to remove all hard-coded secrets and the nbkid.
The secrets and nbkid are moved to secure external configuration sources (e.g., environment variables, configuration files, secret management services).
THEN

The application retrieves secrets and the nbkid from the external configuration sources at runtime.
Security is enhanced by preventing sensitive information from being exposed in the codebase.
Maintainability is improved, allowing for easier updates and management of secrets and identifiers without modifying the source code.
Compliance with security best practices and organizational policies is achieved.

def filter_email_addresses(to_list=None, from_addr=None, cc_list=None, exclude_patterns=None, exact_emails=None):
    if exclude_patterns is None:
        exclude_patterns = [r'\.deleted@']  # Default pattern changed to match BOFA .deleted@ format

    if exact_emails is None:
        exact_emails = []

    def should_exclude(email):
        if not email:
            return False
        if email.lower() in (e.lower() for e in exact_emails):
            return True
        for pattern in exclude_patterns:
            try:
                if re.search(pattern, email, re.IGNORECASE):
                    return True
            except re.error:
                if pattern.lower() in email.lower():
                    return True
        return False

    filtered_to = []
    if to_list:
        filtered_to = [email for email in to_list if not should_exclude(email)]

    filtered_from = from_addr
    if from_addr and should_exclude(from_addr):
        filtered_from = None

    filtered_cc = []
    if cc_list:
        filtered_cc = [email for email in cc_list if not should_exclude(email)]

    return filtered_to, filtered_from, filtered_cc


from jira import JIRA
import pandas as pd
import os
from datetime import datetime

def connect_to_jira(jira_server, username, api_token):
    """
    Connect to JIRA instance using basic authentication
    """
    try:
        jira = JIRA(server=jira_server, basic_auth=(username, api_token))
        return jira
    except Exception as e:
        print(f"Failed to connect to JIRA: {str(e)}")
        return None

def format_comments(comments):
    """
    Format comments with delimiter and user information
    Comments are sorted chronologically with oldest first
    """
    if not comments:
        return ""

    formatted_comments = []
    delimiter = "================\n"

    # Sort comments by created date
    sorted_comments = sorted(comments, key=lambda x: x.created)

    for comment in sorted_comments:
        comment_text = f"Author: {comment.author.displayName}\n"
        comment_text += f"Date: {comment.created}\n"
        comment_text += f"Comment:\n{comment.body}\n"
        formatted_comments.append(comment_text)

    return delimiter.join(formatted_comments)

def process_jira_tickets(jira_server, username, api_token, input_csv, output_excel):
    """
    Main function to process JIRA tickets and create Excel output
    """
    # Connect to JIRA
    jira = connect_to_jira(jira_server, username, api_token)
    if not jira:
        return

    try:
        # Read ticket IDs from CSV
        df = pd.read_csv(input_csv)
        ticket_column = df.columns[0]  # Assume first column contains ticket IDs

        # Initialize lists to store data
        summaries = []
        descriptions = []
        all_comments = []

        # Process each ticket
        for ticket_id in df[ticket_column]:
            try:
                issue = jira.issue(ticket_id)

                # Get summary
                summaries.append(issue.fields.summary)

                # Get description
                descriptions.append(issue.fields.description or "")

                # Get comments
                comments = jira.comments(issue)
                formatted_comments = format_comments(comments)
                all_comments.append(formatted_comments)

                print(f"Successfully processed ticket: {ticket_id}")

            except Exception as e:
                print(f"Error processing ticket {ticket_id}: {str(e)}")
                summaries.append("ERROR")
                descriptions.append("ERROR")
                all_comments.append("ERROR")

        # Create output DataFrame
        output_df = pd.DataFrame({
            'Summary': summaries,
            'Description': descriptions,
            'Comments': all_comments
        })

        # Write to Excel
        output_df.to_excel(output_excel, index=False, engine='openpyxl')
        print(f"Successfully created Excel file: {output_excel}")

    except Exception as e:
        print(f"An error occurred: {str(e)}")

if __name__ == "__main__":
    # Configuration
    JIRA_SERVER = "https://your-jira-instance.com"
    USERNAME = "your-username"
    API_TOKEN = "your-api-token"

    # File paths
    INPUT_CSV = "jira_tickets.csv"
    OUTPUT_EXCEL = f"jira_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"

    # Run the process
    process_jira_tickets(JIRA_SERVER, USERNAME, API_TOKEN, INPUT_CSV, OUTPUT_EXCEL)

# location_inspector.py
import site
import sys
from pathlib import Path
import os

def print_package_locations():
    """Print all relevant Python package installation locations"""

    print("\n=== Python Environment Information ===")
    print(f"Python Version: {sys.version}")
    print(f"Python Executable: {sys.executable}")

    print("\n=== Site Packages Locations ===")
    # Get all site-packages directories
    for path in site.getsitepackages():
        print(f"Site packages: {path}")
        print(f"edt_service would be installed at: {Path(path) / 'edt_service'}")

    print("\n=== Virtual Environment Status ===")
    is_venv = hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix)
    print(f"Running in virtual environment: {is_venv}")
    if is_venv:
        print(f"Virtual environment path: {sys.prefix}")
        venv_site_packages = Path(sys.prefix) / 'lib' / f'python{sys.version_info.major}.{sys.version_info.minor}' / 'site-packages'
        print(f"Venv site-packages: {venv_site_packages}")
        print(f"edt_service would be installed at: {venv_site_packages / 'edt_service'}")

if __name__ == "__main__":
    print_package_locations()

Ah, great question! Let me explain the architecture of VS Code with Dev Containers:
VS Code's architecture with Dev Containers works in a client-server model:

Client Side (Host Machine):


VS Code UI/Editor
Extension Host Process for UI-related extensions
Your source code files


Container Side (Dev Container):


Development tools (Java, Maven, etc.)
Language servers
Development-specific extensions
Build/run environment

When you specify extensions in devcontainer.json, they are installed in the container because:

Language Services:


Extensions like vscjava.vscode-java-pack need to run where the Java compiler and tools are
The Java Language Server runs inside the container to access the Java runtime
This ensures code completion, diagnostics, and debugging work with the container's Java version


Tool Integration:


Maven extension needs to interact with Maven installed in the container
Spring Boot extensions need to access Spring tools in the container
These extensions communicate with VS Code UI via VS Code's remote protocol

[Your Machine]                        [Dev Container]
VS Code UI  <--Remote Protocol-->     Java Language Server
Editor      <-------------------->    Maven
Extensions UI                         Spring Boot Tools
                                     Development Extensions

An example to illustrate:

When you type Java code, the keystroke happens in VS Code on your machine
The code is sent to the Java Language Server running in the container
The server analyzes the code using the container's Java installation
Results (completions, errors) are sent back to VS Code UI

If we didn't install these extensions in the container:

Code completion wouldn't work
Build tools wouldn't be accessible
Debugging wouldn't function
Language features would be missing


DevContainers (Development Containers) are standardized, containerized environments that encapsulate all necessary tools, dependencies, and configurations required for software development. They ensure that every developer works within an identical setup, regardless of their local machine’s operating system or hardware.

How DevContainers Work:

Configuration Specifications:

devcontainer.json: Defines environment settings, extensions, and tools needed for the project.
Dockerfile: Specifies the step-by-step instructions to build the container image, including installing dependencies and configuring the environment.
Containerization Platform:

Docker or Podman: Creates and manages the isolated containers based on the provided configurations, ensuring consistency and isolation from the host system.
IDE Integration:

Visual Studio Code (VSCode): Automatically builds and launches the DevContainer when a project is opened, providing a seamless development experience within the standardized environment.
DevContainer Specifications:

Standardization: Follows a defined specification to ensure compatibility across different tools and platforms.
Version Control: Configuration files are stored alongside the project’s source code, enabling versioning and easy reproducibility for all team members.

Introduction
In our rapidly evolving technological landscape, maintaining a high level of developer productivity and ensuring consistency across diverse projects is paramount. Currently, our organization supports a vast community of over 30,000 developers working across multiple ecosystems, including Java, .NET, Python, NodeJS, Scala, Go, and Rust. However, the traditional approach to setting up development environments is proving to be a significant bottleneck, impeding our ability to deliver high-quality software efficiently.

Current Challenges
Prolonged Onboarding Time

Issue: Developers spend an average of 5+ days configuring their local environments before making their first meaningful code commit.
Impact: Delays in project initiation, reduced productivity, and increased time-to-market for new features and products.
Inconsistent Development Environments

Issue: Variability in individual setups leads to the notorious "it works on my machine" problem.
Impact: Increased bug rates, integration issues, and extended debugging cycles, ultimately affecting software quality and reliability.
High Maintenance Overhead

Issue: Manual installation and updates of dependencies (e.g., app servers, language runtimes, build tools) across diverse ecosystems are time-consuming and error-prone.
Impact: Significant allocation of developer time to environment maintenance rather than productive coding and innovation.
Inefficient Collaboration

Issue: Disparate environments hinder seamless teamwork and knowledge sharing.
Impact: Fragmented development efforts, duplicated work, and slower project progression.

 Proposed Solution

1. Choice of Development Environments:

Cloud-Based IDEs:
Options: GitHub Codespaces, RedHat Devspaces
Advantages: Seamless cloud integration, scalability, and support for modern development workflows.
Native IDEs with DevContainers:
Tools: VSCode with DevContainers extension, PodMan Desktop, Artifactory for image repositories
Advantages: Familiar native environments, flexibility, and support for a wide range of IDEs.
2. Phased Implementation Approach:

Phase 1: Adopt DevContainers for Native IDE Users

Actions:
Standardize on DevContainers to provide consistent, containerized environments for native IDEs (e.g., VSCode, JetBrains).
Deploy PodMan Desktop and Artifactory to manage and distribute container images.
Address Eclipse users by developing specialized DevContainer configurations and providing enhanced support.
Goals: Achieve immediate reduction in onboarding time, ensure environment consistency, and enhance developer productivity.
Phase 2: Integrate Cloud-Based IDEs

Actions:
Pilot RedHat Devspaces to offer cloud-based development environments that support the DevContainers specification.
Enable GitHub Codespaces for developers comfortable with cloud IDEs, ensuring compatibility with existing DevContainer setups.
Evaluate and expand cloud IDE usage based on Phase 1 outcomes and developer feedback.
Goals: Provide flexibility for Gen-Z developers and those preferring cloud solutions, while maintaining support for native IDE workflows.
3. Addressing Eclipse Ecosystem Challenges:

Strategies:
Develop tailored DevContainer configurations specific to Eclipse.
Provide comprehensive training and support to Eclipse users to facilitate the transition.
Collaborate with Eclipse plugin developers to enhance DevContainer compatibility.

Infrastructure Components:
PodMan Desktop: Manage containerized environments locally with enhanced security.
Artifactory: Central repository for storing and versioning container images.
VSCode Extensions: Facilitate seamless integration with DevContainers for native IDE users.
WSL + Hyper-V: Ensure compatibility and performance for Windows-based developers.

2. Strategic Benefits

Accelerated Time-to-Productivity:
Drastically reduce environment setup time, enabling developers to contribute code within hours.
Consistency and Quality:
Ensure uniform development environments, minimizing bugs and enhancing software reliability.
Scalability and Flexibility:
Support a large and diverse developer community with varied preferences and toolsets.
Cost Efficiency:
Lower maintenance overhead and reduce delays, translating to significant cost savings and better resource allocation.
Enhanced Developer Satisfaction:
Provide modern, flexible development workflows catering to both cloud-native and traditional developers, increasing overall satisfaction and retention.


Key Benefits
Accelerated Onboarding and Setup

Reduction in Setup Time: Developers can initialize their environments in minutes instead of days, enabling immediate contributions.
Consistency Across Teams: Uniform environments eliminate setup discrepancies, ensuring that all developers work within the same configurations.
Enhanced Productivity and Efficiency

Focus on Development: Minimizes time spent on environment configuration, allowing developers to concentrate on writing and optimizing code.
Streamlined Updates: Centralized management of dependencies ensures that updates and patches are propagated uniformly, reducing individual maintenance efforts.
Improved Collaboration and Code Quality

Standardized Environments: Facilitates smoother collaboration by ensuring that all team members are working within the same setup.
Reduced Integration Issues: Consistent environments lower the incidence of environment-specific bugs, enhancing overall software quality.
Scalability and Flexibility

Support for Multiple Ecosystems: Easily accommodates various technology stacks (Java, .NET, Python, NodeJS, Scala, Go, Rust) through tailored DevContainer configurations.
Adaptable Infrastructure: Utilizes robust tools like PodMan and Artifactory to manage and scale containerized environments efficiently.
Cost Efficiency

Reduced Downtime: Faster setup times translate to higher utilization of developer resources and faster project turnaround.
Lower Maintenance Costs: Centralized environment management reduces the overhead associated with individual setups and troubleshooting.
Enhanced Security and Compliance

Controlled Environments: Containers encapsulate dependencies, minimizing the risk of security vulnerabilities and ensuring compliance with organizational standards.
Centralized Image Management: Artifactory serves as a secure repository for container images, enforcing access controls and versioning.



def extract_ticket_and_guid(email_body: str):
    """Extract Jira ticket ID and Thread ID from email body"""
    jira_pattern = r'https://jira3\.horizon\.bankofamerica\.com/browse/([A-Z]+-\d+)'
    guid_pattern = r'Thread ID: ([\w-]+)'

    jira_match = re.search(jira_pattern, email_body)
    guid_match = re.search(guid_pattern, email_body)

    jira_key = jira_match.group(1) if jira_match else None
    guid = guid_match.group(1) if guid_match else None

    return jira_key, guid

def find_email_thread_attachment(issue, guid):
    """Find the email thread attachment for given GUID"""
    for attachment in issue.fields.attachment:
        # Look for attachment with matching GUID in filename
        if f"email_thread_{guid}" in attachment.filename:
            return attachment.id
    return None

def update_email_thread_attachment(jira, issue_key, html_content, guid):
    """Update email thread attachment with new content"""
    issue = jira.issue(issue_key)

    # Find and delete old attachment
    old_attachment_id = find_email_thread_attachment(issue, guid)
    if old_attachment_id:
        jira.delete_attachment(old_attachment_id)

    # Add new attachment
    attachment_name = f"email_thread_{guid}.html"
    issue.add_attachment(
        attachment=html_content.encode('utf-8'),
        filename=attachment_name
    )

def create_response_email(mail, jira_key):
    """Create formatted email response with GUID"""
    guid = str(uuid.uuid4())

    response_body = (
        "Thank you for reaching out to Enterprise Developer Tooling.\n"  # First line
        "\n"  # Empty line
        "A ticket has been created for your reference:\n"  # Second line
        f"Jira Ticket: https://jira3.horizon.bankofamerica.com/browse/{jira_key}\n"  # Third line
        f"Thread ID: {guid}\n"  # Fourth line
        "\n"  # Empty line
        "Please feel free to comment on the ticket if you would like to leave feedback.\n"  # Fifth line
        "\n"  # Empty line
        "Best regards,\n"  # Sixth line
        "Enterprise Developer Tooling Team"  # Seventh line
    )

    # Create the email reply
    mail.create_reply_all(
        subject="Re: " + mail.subject,
        body=response_body
    )

    return guid

import uuid
import re

def generate_guid():
    """Generate a simple GUID"""
    return str(uuid.uuid4())

def add_guid_to_body(body, guid=None):
    """Add GUID to email body"""
    if guid is None:
        guid = generate_guid()
    guid_line = f"\n\nReference ID: {guid}\n"
    return body + guid_line

def extract_guid_from_body(body):
    """Extract GUID from email body if present"""
    pattern = r'Reference ID: ([\w-]+)'
    match = re.search(pattern, body)
    return match.group(1) if match else None

# Example usage:
original_body = """
Thank you for contacting IT Support.
Your ticket has been created and we will get back to you shortly.

Best regards,
IT Support Team
"""

# Adding GUID to email response
response_body = add_guid_to_body(original_body)
print("Email body with GUID:")
print(response_body)

# Later, when receiving a reply, extract the GUID
extracted_guid = extract_guid_from_body(response_body)
print(f"\nExtracted GUID: {extracted_guid}")

def setup_logging():
    log_folder = os.path.join(os.getcwd(), 'logs')

    if not os.path.exists(log_folder):
        os.makedirs(log_folder)

    log_file = os.path.join(log_folder, 'app.log')
    handler = RotatingFileHandler(log_file, maxBytes=10*1024*1024, backupCount=5)
    handler.setLevel(logging.INFO)
    formatter = logging.Formatter('%(asctime)s %(levelname)s: %(message)s [in %(pathname)s:%(lineno)d]')
    handler.setFormatter(formatter)

    # Set up root logger
    logging.getLogger().setLevel(logging.INFO)
    logging.getLogger().addHandler(handler)

# File: setup.py
import json
from setuptools import setup, find_packages

# Read the contents of your README file
with open('README.md', 'r', encoding='utf-8') as f:
    long_description = f.read()

# Read the contents of your setup.json file
with open('setup.json', 'r') as f:
    config = json.load(f)

# Read the contents of your requirements.txt file
with open('requirements.txt', 'r') as f:
    requirements = f.read().splitlines()

setup(
    name=config['name'],
    version=config['version'],
    author=config.get('author', ''),
    author_email=config.get('author_email', ''),
    description=config.get('description', ''),
    long_description=long_description,
    long_description_content_type='text/markdown',
    url=config.get('url', ''),
    package_dir={"": "src"},
    packages=find_packages(where="src"),
    include_package_data=True,
    install_requires=requirements,
    classifiers=[
        "Programming Language :: Python :: 3",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
    ],
    python_requires='>=3.6',
    entry_points={
        'console_scripts': [
            'edt-service=edt_service.app:main',
        ],
    },
)



# File: MANIFEST.in
include README.md
include requirements.txt
include setup.json
recursive-include src/edt_service/templates *
recursive-include src/edt_service/static *

import logging
from queue import Queue
from threading import Thread
from jira import JIRA
from tqdm import tqdm
import time

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    filename='jira_cleanup.log',
    filemode='w'
)

# Add console handler if you want to see logs in console as well
console = logging.StreamHandler()
console.setLevel(logging.INFO)
logging.getLogger('').addHandler(console)

# Replace these with your actual JIRA instance details
JIRA_SERVER = "https://your-domain.atlassian.net"
JIRA_EMAIL = "your-email@example.com"
JIRA_API_TOKEN = "your-api-token"

# JQL query to find the tickets
JQL_QUERY = '''
    project = DLAB
    AND issuekey >= DLAB-1651
    AND issuekey <= DLAB-12500
    AND summary ~ "Your specific text here"
'''

CANCELLED_RESOLUTION = "Cancelled"
CUSTOM_COMMENT = "This issue has been automatically closed as part of a cleanup process."

def connect_to_jira():
    try:
        return JIRA(server=JIRA_SERVER, basic_auth=(JIRA_EMAIL, JIRA_API_TOKEN))
    except Exception as e:
        logging.error(f"Failed to connect to JIRA: {str(e)}")
        raise

def get_tickets(jira):
    try:
        return jira.search_issues(JQL_QUERY, maxResults=False)
    except Exception as e:
        logging.error(f"Failed to fetch tickets: {str(e)}")
        raise

def process_ticket(jira, issue):
    start_time = time.time()
    try:
        # Your existing process_ticket logic here
        # For example:
        jira.transition_issue(issue, 'Open')
        time.sleep(1)
        jira.transition_issue(issue, 'Close', fields={'resolution': {'name': CANCELLED_RESOLUTION}})
        jira.add_comment(issue, CUSTOM_COMMENT)
        # Detach from epic logic here

        end_time = time.time()
        processing_time = end_time - start_time
        logging.info(f"Processed ticket {issue.key} in {processing_time:.2f} seconds")
    except Exception as e:
        end_time = time.time()
        processing_time = end_time - start_time
        logging.error(f"Error processing ticket {issue.key} after {processing_time:.2f} seconds: {str(e)}")

def worker(jira, queue, progress_bar):
    while True:
        issue = queue.get()
        if issue is None:
            break
        try:
            process_ticket(jira, issue)
        except Exception as e:
            logging.error(f"Error in worker thread for ticket {issue.key}: {str(e)}")
        finally:
            queue.task_done()
            progress_bar.update(1)

def main():
    overall_start_time = time.time()

    jira = connect_to_jira()
    issues = get_tickets(jira)

    logging.info(f"Found {len(issues)} tickets to process")

    queue = Queue()
    for issue in issues:
        queue.put(issue)

    num_threads = 10
    progress_bar = tqdm(total=len(issues), desc="Processing tickets")

    threads = []
    for _ in range(num_threads):
        thread = Thread(target=worker, args=(jira, queue, progress_bar))
        thread.start()
        threads.append(thread)

    queue.join()

    for _ in range(num_threads):
        queue.put(None)
    for thread in threads:
        thread.join()

    progress_bar.close()

    overall_end_time = time.time()
    total_time = overall_end_time - overall_start_time
    logging.info(f"Total processing time: {total_time:.2f} seconds")
    logging.info(f"Average time per ticket: {total_time / len(issues):.2f} seconds")

if __name__ == "__main__":
    main()

def add_comment(jira, issue):
    try:
        jira.add_comment(issue, CUSTOM_COMMENT)
        logging.info(f"Added comment to ticket {issue.key}")
        return True
    except JIRAError as e:
        logging.error(f"Failed to add comment to ticket {issue.key}: {str(e)}")
        return False


from jira import JIRA
from jira.exceptions import JIRAError
from tqdm import tqdm
import logging
import time

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Replace these with your actual JIRA instance details
JIRA_SERVER = "https://your-domain.atlassian.net"
JIRA_EMAIL = "your-email@example.com"
JIRA_API_TOKEN = "your-api-token"

# JQL query to find the tickets in the specified range
JQL_QUERY = 'project = DLAB AND issuekey >= DLAB-1651 AND issuekey <= DLAB-12500'

# Define the resolution
CANCELLED_RESOLUTION = "Cancelled"

def connect_to_jira():
    try:
        return JIRA(server=JIRA_SERVER, basic_auth=(JIRA_EMAIL, JIRA_API_TOKEN))
    except JIRAError as e:
        logging.error(f"Failed to connect to JIRA: {str(e)}")
        raise

def get_tickets(jira):
    try:
        return jira.search_issues(JQL_QUERY, maxResults=False)
    except JIRAError as e:
        logging.error(f"Failed to fetch tickets: {str(e)}")
        raise

def transition_to_open(jira, issue):
    try:
        jira.transition_issue(issue, 'Open')
        logging.info(f"Transitioned ticket {issue.key} to 'Open'")
        return True
    except JIRAError as e:
        logging.error(f"Failed to transition ticket {issue.key} to 'Open': {str(e)}")
        return False

def close_with_cancelled_resolution(jira, issue):
    try:
        jira.transition_issue(issue, 'Close', fields={'resolution': {'name': CANCELLED_RESOLUTION}})
        logging.info(f"Closed ticket {issue.key} with 'Cancelled' resolution")
        return True
    except JIRAError as e:
        logging.error(f"Failed to close ticket {issue.key} with 'Cancelled' resolution: {str(e)}")
        return False

def detach_from_epic(jira, issue):
    try:
        epic_link_field = next((f for f in jira.fields() if f['name'] == 'Epic Link'), None)
        if epic_link_field:
            issue.update(fields={epic_link_field['id']: None})
            logging.info(f"Detached ticket {issue.key} from EPIC")
            return True
        else:
            logging.warning(f"No 'Epic Link' field found for {issue.key}")
            return False
    except JIRAError as e:
        logging.error(f"Failed to detach ticket {issue.key} from EPIC: {str(e)}")
        return False

def process_ticket(jira, issue):
    # Step 1: Transition to "Open"
    if transition_to_open(jira, issue):
        time.sleep(1)  # Wait a short time to allow JIRA to process the transition

        # Step 2: Close with "Cancelled" resolution
        if close_with_cancelled_resolution(jira, issue):
            # Step 3: Detach from EPIC
            detach_from_epic(jira, issue)
        else:
            logging.warning(f"Could not close ticket {issue.key} with 'Cancelled' resolution")
    else:
        logging.warning(f"Could not open ticket {issue.key}. It may already be in the desired state.")

def main():
    jira = connect_to_jira()
    issues = get_tickets(jira)

    logging.info(f"Found {len(issues)} tickets to process")

    for issue in tqdm(issues, desc="Processing tickets"):
        process_ticket(jira, issue)

if __name__ == "__main__":
    main()

from jira import JIRA
from jira.exceptions import JIRAError
from tqdm import tqdm
import logging
import time

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Replace these with your actual JIRA instance details
JIRA_SERVER = "https://your-domain.atlassian.net"
JIRA_EMAIL = "your-email@example.com"
JIRA_API_TOKEN = "your-api-token"

# JQL query to find the tickets in the specified range
JQL_QUERY = 'project = DLAB AND issuekey >= DLAB-1651 AND issuekey <= DLAB-12500'

def connect_to_jira():
    try:
        return JIRA(server=JIRA_SERVER, basic_auth=(JIRA_EMAIL, JIRA_API_TOKEN))
    except JIRAError as e:
        logging.error(f"Failed to connect to JIRA: {str(e)}")
        raise

def get_tickets(jira):
    try:
        return jira.search_issues(JQL_QUERY, maxResults=False)
    except JIRAError as e:
        logging.error(f"Failed to fetch tickets: {str(e)}")
        raise

def transition_issue(jira, issue, transition_name):
    try:
        transitions = jira.transitions(issue)
        transition_id = next((t['id'] for t in transitions if t['name'].lower() == transition_name.lower()), None)

        if transition_id:
            jira.transition_issue(issue, transition_id)
            logging.info(f"Transitioned ticket {issue.key} to '{transition_name}'")
            return True
        else:
            logging.warning(f"No '{transition_name}' transition found for {issue.key}")
            return False
    except JIRAError as e:
        logging.error(f"Failed to transition ticket {issue.key} to '{transition_name}': {str(e)}")
        return False

def detach_from_epic(jira, issue):
    try:
        epic_link_field = next((f for f in jira.fields() if f['name'] == 'Epic Link'), None)
        if epic_link_field:
            issue.update(fields={epic_link_field['id']: None})
            logging.info(f"Detached ticket {issue.key} from EPIC")
            return True
        else:
            logging.warning(f"No 'Epic Link' field found for {issue.key}")
            return False
    except JIRAError as e:
        logging.error(f"Failed to detach ticket {issue.key} from EPIC: {str(e)}")
        return False

def process_ticket(jira, issue):
    # Step 1: Transition to "Open"
    if transition_issue(jira, issue, "Open"):
        # Wait a short time to allow JIRA to process the first transition
        time.sleep(1)

        # Step 2: Transition to "Close"
        if transition_issue(jira, issue, "Close"):
            # Step 3: Detach from EPIC
            detach_from_epic(jira, issue)
        else:
            logging.warning(f"Could not close ticket {issue.key}")
    else:
        logging.warning(f"Could not open ticket {issue.key}. It may already be in the desired state.")

def main():
    jira = connect_to_jira()
    issues = get_tickets(jira)

    logging.info(f"Found {len(issues)} tickets to process")

    for issue in tqdm(issues, desc="Processing tickets"):
        process_ticket(jira, issue)

if __name__ == "__main__":
    main()

from jira import JIRA
from jira.exceptions import JIRAError
from tqdm import tqdm
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Replace these with your actual JIRA instance details
JIRA_SERVER = "https://your-domain.atlassian.net"
JIRA_EMAIL = "your-email@example.com"
JIRA_API_TOKEN = "your-api-token"

# JQL query to find the tickets in the specified range
JQL_QUERY = 'project = DLAB AND issuekey >= DLAB-1651 AND issuekey <= DLAB-12500'

def connect_to_jira():
    try:
        return JIRA(server=JIRA_SERVER, basic_auth=(JIRA_EMAIL, JIRA_API_TOKEN))
    except JIRAError as e:
        logging.error(f"Failed to connect to JIRA: {str(e)}")
        raise

def get_tickets(jira):
    try:
        return jira.search_issues(JQL_QUERY, maxResults=False)
    except JIRAError as e:
        logging.error(f"Failed to fetch tickets: {str(e)}")
        raise

def cancel_and_detach_from_epic(jira, issue):
    try:
        # Find the 'Cancel' transition ID
        transitions = jira.transitions(issue)
        cancel_transition_id = next((t['id'] for t in transitions if t['name'].lower() == 'cancel'), None)

        if cancel_transition_id:
            jira.transition_issue(issue, cancel_transition_id)
            logging.info(f"Cancelled ticket {issue.key}")
        else:
            logging.warning(f"No 'Cancel' transition found for {issue.key}")

        # Detach from EPIC
        epic_link_field = next((f for f in jira.fields() if f['name'] == 'Epic Link'), None)
        if epic_link_field:
            issue.update(fields={epic_link_field['id']: None})
            logging.info(f"Detached ticket {issue.key} from EPIC")
        else:
            logging.warning(f"No 'Epic Link' field found for {issue.key}")

        return True
    except JIRAError as e:
        logging.error(f"Failed to cancel and detach ticket {issue.key}: {str(e)}")
        return False

def main():
    jira = connect_to_jira()
    issues = get_tickets(jira)

    logging.info(f"Found {len(issues)} tickets to process")

    for issue in tqdm(issues, desc="Processing tickets"):
        cancel_and_detach_from_epic(jira, issue)

if __name__ == "__main__":
    main()

import json
from setuptools import setup, find_packages

# Load setup configuration from setup.json
with open("setup.json", "r") as f:
    setup_config = json.load(f)

# Load dependencies from requirements.txt
with open("requirements.txt", "r") as f:
    install_requires = f.read().splitlines()

setup(
    name=setup_config["name"],
    version=setup_config["version"],
    description=setup_config["description"],
    author=setup_config["author"],
    author_email=setup_config["author_email"],
    url=setup_config["url"],
    packages=find_packages(include=["*"]),
    install_requires=install_requires,
    include_package_data=True,
    package_data={
        '': ['*.html'],  # Include HTML templates
    },
    entry_points={
        'console_scripts': [
            'run-flask-app=your_module.your_entry_file:main',  # Define the entry point
        ],
    },
    classifiers=[
        "Programming Language :: Python :: 3",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
    ],
    python_requires='>=3.6',
)


[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "your-package-name"
version = "0.1.0"
description = "A description of your Python package"
authors = [
    {name = "Your Name", email = "your.email@example.com"},
]
dependencies = [
    "requests",
    "numpy",
]
readme = "README.md"
requires-python = ">=3.6"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]

[tool.setuptools.packages.find]
where = ["."]
include = ["*"]

[tool.setuptools.package-data]
"your_package_name" = ["*.html"]  # Include HTML templates, adjust this to your needs

[project.scripts]
run-flask-app = "your_package_name.your_entry_file:main"


from transformers import T5Tokenizer, T5ForConditionalGeneration

# Load the T5-small model and tokenizer
model_path = 'path/to/your/t5-small'  # Update this with the path to your downloaded model
tokenizer = T5Tokenizer.from_pretrained(model_path)
model = T5ForConditionalGeneration.from_pretrained(model_path)

def summarize_questions(questions):
    # Combine all questions into a single string
    input_text = " ".join(questions)
    input_text = "summarize: " + input_text  # Prefix for T5

    # Encode the input text
    inputs = tokenizer.encode(input_text, return_tensors='pt', max_length=512, truncation=True)

    # Generate summary ids
    summary_ids = model.generate(inputs, max_length=50, min_length=25, length_penalty=2.0, num_beams=4, early_stopping=True)

    # Decode the summary ids
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

def main():
    # Load your unanswered questions
    df_unanswered = pd.read_csv('data/unanswered_questions.csv')  # Update path as needed

    # Example list of questions to summarize
    questions_to_summarize = [
        "What is the role of a project manager?",
        "How does a project manager ensure project success?",
        "What are the key responsibilities of project management?"
    ]

    # Summarize the questions
    summary = summarize_questions(questions_to_summarize)
    print(f"Summary: {summary}")

if __name__ == "__main__":
    main()



import os

def join_files(part_files_folder, output_file_path, base_filename="model.safetensors"):
    """
    Joins split files from a specified folder back into a single file.
    Args:
        part_files_folder (str): The path to the folder containing the part files.
        output_file_path (str): The path to save the joined file.
        base_filename (str): The base name of the part files (default is "model.safetensors").
    """
    part_num = 0
    with open(output_file_path, 'wb') as output_file:
        while True:
            part_file_name = os.path.join(part_files_folder, f"{base_filename}.part{part_num}")
            if not os.path.exists(part_file_name):
                break  # Stop when no more part files are found
            with open(part_file_name, 'rb') as part_file:
                output_file.write(part_file.read())
            print(f"Joined {part_file_name}")``
            part_num += 1

    print(f"All parts joined into {output_file_path}")

# Usage example:
part_files_folder = "path/to/your/part/files/folder"  # Folder where part files are located
output_file_path = "path/to/output/model_joined.safetensors"  # Path to save the joined file
join_files(part_files_folder, output_file_path, base_filename="model.safetensors")


from transformers import pipeline

def summarize_questions(questions):
    summarizer = pipeline("summarization")
    combined_text = " ".join(questions)
    summary = summarizer(combined_text, max_length=50, min_length=25, do_sample=False)
    return summary[0]['summary_text']


def find_similar_groups(similarity_matrix, threshold=0.8):
    similar_groups = []
    seen_questions = set()

    for i in range(similarity_matrix.shape[0]):
        if i not in seen_questions:
            group = [i]
            seen_questions.add(i)

            for j in range(i + 1, similarity_matrix.shape[1]):
                if similarity_matrix[i][j] > threshold:
                    group.append(j)
                    seen_questions.add(j)

            similar_groups.append(group)

    return similar_groups


from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer

def extract_keywords_lda(questions):
    vectorizer = CountVectorizer(stop_words='english')
    doc_term_matrix = vectorizer.fit_transform(questions)

    lda = LatentDirichletAllocation(n_components=1, random_state=42)
    lda.fit(doc_term_matrix)

    # Get the words in the topic
    feature_names = vectorizer.get_feature_names_out()
    keywords = [feature_names[i] for i in lda.components_[0].argsort()[-3:]]  # Top 3 keywords
    return " | ".join(keywords)


from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
import numpy as np
from collections import Counter
import re

def load_fine_tuned_model():
    # Load the fine-tuned SBERT model
    model = SentenceTransformer('models/fine-tuned-sbert')
    return model

def generate_embeddings(model, questions):
    embeddings = model.encode(questions, convert_to_tensor=True)
    return embeddings.cpu().numpy()

def find_similar_groups(similarity_matrix, threshold=0.8):
    similar_groups = []
    visited = set()

    for i in range(similarity_matrix.shape[0]):
        if i in visited:
            continue
        group = [i]
        visited.add(i)

        for j in range(i + 1, similarity_matrix.shape[0]):
            if similarity_matrix[i, j] > threshold:
                group.append(j)
                visited.add(j)

        similar_groups.append(group)

    return similar_groups

def extract_keywords(questions):
    # Combine all questions in the group into a single string
    combined_text = " ".join(questions)
    # Use regex to remove punctuation and split into words
    words = re.findall(r'\w+', combined_text.lower())
    # Count the frequency of each word
    word_counts = Counter(words)
    # Extract the most common words as keywords
    keywords = word_counts.most_common(3)  # Get top 3 keywords
    # Return a simple group name based on keywords
    return " | ".join(word for word, count in keywords)

def main():
    # Load the fine-tuned model
    model = load_fine_tuned_model()

    # Load your unanswered questions
    df_unanswered = pd.read_csv('data/unanswered_questions.csv')  # Update path as needed

    # Generate embeddings
    embeddings = generate_embeddings(model, df_unanswered['Question'].tolist())

    # Compute cosine similarity matrix
    similarity_matrix = cosine_similarity(embeddings)

    # Find similar groups based on threshold
    threshold = 0.8
    similar_groups = find_similar_groups(similarity_matrix, threshold)

    # Display similar question groups with names
    for group in similar_groups:
        questions_in_group = [df_unanswered.iloc[i]['Question'] for i in group]
        group_name = extract_keywords(questions_in_group)
        print(f"Group Name: {group_name}")
        for question in questions_in_group:
            print(f"- {question}")
        print()  # Blank line for better readability

if __name__ == "__main__":
    main()


from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
import numpy as np

def load_fine_tuned_model():
    # Load the fine-tuned SBERT model
    model = SentenceTransformer('models/fine-tuned-sbert')
    return model

def generate_embeddings(model, questions):
    embeddings = model.encode(questions, convert_to_tensor=True)
    return embeddings.cpu().numpy()

def find_similar_groups(similarity_matrix, threshold=0.8):
    similar_groups = []
    visited = set()
    
    for i in range(similarity_matrix.shape[0]):
        if i in visited:
            continue
        group = [i]
        visited.add(i)
        
        for j in range(i + 1, similarity_matrix.shape[0]):
            if similarity_matrix[i, j] > threshold:
                group.append(j)
                visited.add(j)
        
        similar_groups.append(group)
    
    return similar_groups

def main():
    # Load the fine-tuned model
    model = load_fine_tuned_model()

    # Load your unanswered questions
    df_unanswered = pd.read_csv('data/unanswered_questions.csv')  # Update path as needed

    # Generate embeddings
    embeddings = generate_embeddings(model, df_unanswered['Question'].tolist())

    # Compute cosine similarity matrix
    similarity_matrix = cosine_similarity(embeddings)

    # Find similar groups based on threshold
    threshold = 0.8
    similar_groups = find_similar_groups(similarity_matrix, threshold)

    # Display similar question groups
    for group in similar_groups:
        questions_in_group = [df_unanswered.iloc[i]['Question'] for i in group]
        print("Similar Questions Group:")
        for question in questions_in_group:
            print(f"- {question}")
        print()  # Blank line for better readability

if __name__ == "__main__":
    main()

sentence1,sentence2,label
"How do I install Jenkins on Ubuntu?", "What are the steps to set up Jenkins on an Ubuntu server?",1
"How can I create a new repository in Bitbucket?", "What's the process for initializing a repository on Bitbucket?",1
"How to configure Sonarqube for code quality analysis?", "What settings should I adjust in Sonarqube to evaluate code quality?",1
"How do I use Ansible to automate server deployments?", "What are the Ansible commands for automating the deployment of servers?",1
"How to integrate Confluence with Jira?", "What is the method to link Confluence pages to Jira tickets?",1
"How do I troubleshoot Jenkins build failures?", "What steps should I take to fix failed builds in Jenkins?",1
"How to set up Bitbucket pipelines for CI/CD?", "What is the procedure to configure CI/CD pipelines in Bitbucket?",1
"How can Sonarqube help in maintaining code standards?", "In what ways does Sonarqube assist with enforcing coding standards?",1
"How to manage playbooks in Ansible?", "What is the best practice for organizing Ansible playbooks?",1
"How do I create Jira dashboards for project tracking?", "What's the way to set up dashboards in Jira for monitoring projects?",1
"How to secure Jenkins with authentication?", "What methods can I use to add authentication to Jenkins?",1
"How can I merge branches in Bitbucket?", "What's the process for merging different branches in Bitbucket repositories?",1
"How to analyze code smells using Sonarqube?", "What steps do I follow to detect code smells with Sonarqube?",1
"How do I deploy applications using Ansible?", "What are the commands to deploy apps through Ansible?",1
"How to collaborate using Confluence and Jira together?", "What is the way to use Confluence and Jira for team collaboration?",1
"How to resolve Jenkins plugin compatibility issues?", "What should I do if Jenkins plugins are not compatible with each other?",1
"How to set up access controls in Bitbucket?", "What's the method to configure user permissions in Bitbucket?",1
"How can Sonarqube integrate with Jenkins?", "What is the process to connect Sonarqube with Jenkins for continuous integration?",1
"How to write Ansible roles for infrastructure management?", "What steps are involved in creating Ansible roles for managing infrastructure?",1
"How to track issues in Jira using Confluence?", "What is the way to monitor Jira issues through Confluence pages?",1
"How do I update Jenkins to the latest version?", "What's the procedure for upgrading Jenkins to its newest release?",1
"How can I clone a Bitbucket repository locally?", "What commands do I use to clone a repository from Bitbucket to my machine?",1
"How to interpret Sonarqube analysis reports?", "What should I understand from the reports generated by Sonarqube?",1
"How to configure Ansible inventory files?", "What is the method to set up inventory files in Ansible?",1
"How do I create a Confluence space for my team?", "What's the process to establish a new space in Confluence for team collaboration?",1
"How to assign tasks in Jira projects?", "What steps do I follow to allocate tasks within Jira project boards?",1
"How to back up Jenkins configurations?", "What's the way to create backups of Jenkins setup and configurations?",1
"How can I enable branch permissions in Bitbucket?", "What is the method to set branch access controls in Bitbucket?",1
"How to set quality gates in Sonarqube?", "What steps do I take to establish quality gates within Sonarqube?",1
"How do I use Ansible to manage Docker containers?", "What are the Ansible playbooks for handling Docker container deployments?",1
"How to link Jira issues to Confluence documents?", "What's the procedure to associate Jira tickets with Confluence pages?",1


from sentence_transformers import SentenceTransformer, InputExample, losses
from torch.utils.data import DataLoader
import pandas as pd
import torch
import logging

# Enable logging
logging.basicConfig(format='%(asctime)s - %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S',
                    level=logging.INFO)

def main():
    # Step 1: Load the dataset
    df = pd.read_csv('data/similar_questions.csv')

    # Step 2: Prepare the data
    train_examples = []
    for index, row in df.iterrows():
        train_examples.append(
            InputExample(texts=[row['sentence1'], row['sentence2']], label=row['label'])
        )

    # Step 3: Load the pre-trained SBERT model from local directory
    model_path = 'models/all-MiniLM-L6-v2'  # Update this path accordingly
    model = SentenceTransformer(model_path)

    # Step 4: Define the training objective
    # For regression (similarity scores)
    train_loss = losses.CosineSimilarityLoss(model)

    # For classification (binary labels), uncomment the following:
    # train_loss = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=2)

    # Step 5: Create a DataLoader
    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)

    # Step 6: Fine-tune the model
    num_epochs = 4
    warmup_steps = int(len(train_dataloader) * num_epochs * 0.1)  # 10% of train data

    model.fit(
        train_objectives=[(train_dataloader, train_loss)],
        epochs=num_epochs,
        warmup_steps=warmup_steps,
        device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    )

    # Step 7: Save the fine-tuned model
    model.save('models/fine-tuned-sbert')

if __name__ == "__main__":
    main()


from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
import numpy as np

def load_fine_tuned_model():
    # Load the fine-tuned SBERT model
    model = SentenceTransformer('models/fine-tuned-sbert')
    return model

def generate_embeddings(model, questions):
    embeddings = model.encode(questions, convert_to_tensor=True)
    return embeddings.cpu().numpy()

def find_similar_pairs(similarity_matrix, threshold=0.8):
    similar_pairs = np.argwhere(similarity_matrix > threshold)
    # Remove self-pairs and duplicate pairs
    similar_pairs = [(i, j) for i, j in similar_pairs if i < j]
    return similar_pairs

def main():
    # Load the fine-tuned model
    model = load_fine_tuned_model()

    # Load your unanswered questions
    df_unanswered = pd.read_csv('data/unanswered_questions.csv')  # Update path as needed

    # Generate embeddings
    embeddings = generate_embeddings(model, df_unanswered['Question'].tolist())

    # Compute cosine similarity matrix
    similarity_matrix = cosine_similarity(embeddings)

    # Find similar pairs based on threshold
    threshold = 0.8
    similar_pairs = find_similar_pairs(similarity_matrix, threshold)

    # Display similar question pairs
    for i, j in similar_pairs:
        print(f"Question {i+1} and Question {j+1} are similar:")
        print(f"Q{i+1}: {df_unanswered.iloc[i]['Question']}")
        print(f"Q{j+1}: {df_unanswered.iloc[j]['Question']}\n")

if __name__ == "__main__":
    main()

import pandas as pd
import random

# Load all questions
df = pd.read_csv('similar_questions.csv')

# Separate similar and non-similar questions
similar_df = df[df['label'] == 1]
all_questions = df['sentence1'].tolist() + df['sentence2'].tolist()

# Generate negative samples by pairing questions from different tools
negative_samples = []
tools = ['Jenkins', 'Bitbucket', 'Sonarqube', 'Ansible', 'Confluence', 'Jira']

for _ in range(100):  # Adjust the number as needed
    q1 = random.choice(all_questions)
    q2 = random.choice(all_questions)

    # Ensure questions are from different tools
    if any(tool in q1 for tool in tools) and any(tool in q2 for tool in tools):
        tool_q1 = next((tool for tool in tools if tool in q1), None)
        tool_q2 = next((tool for tool in tools if tool in q2), None)
        if tool_q1 != tool_q2:
            negative_samples.append({'sentence1': q1, 'sentence2': q2, 'label': 0})

# Create DataFrame for negative samples
negative_df = pd.DataFrame(negative_samples)

# Combine with similar questions
final_df = pd.concat([similar_df, negative_df], ignore_index=True)

# Shuffle the dataset
final_df = final_df.sample(frac=1).reset_index(drop=True)

# Save to CSV
final_df.to_csv('similar_questions.csv', index=False)


import os
import xml.etree.ElementTree as ET
import shutil
from typing import List, Tuple, Dict

# Mapping of connector namespaces to their corresponding Maven groupId and artifactId
CONNECTOR_DEPENDENCY_MAP = {
    'http://www.mulesoft.org/schema/mule/http': ('org.mule.connectors', 'mule-http-connector'),
    'http://www.mulesoft.org/schema/mule/db': ('org.mule.connectors', 'mule-db-connector'),
    'http://www.mulesoft.org/schema/mule/file': ('org.mule.connectors', 'mule-file-connector'),
    'http://www.mulesoft.org/schema/mule/ftp': ('org.mule.connectors', 'mule-ftp-connector'),
    'http://www.mulesoft.org/schema/mule/jms': ('org.mule.connectors', 'mule-jms-connector'),
    'http://www.mulesoft.org/schema/mule/vm': ('org.mule.connectors', 'mule-vm-connector'),
    # Add more mappings as needed
}

def parse_domain_resources(domain_path: str) -> Tuple[Dict[str, List[ET.Element]], set]:
    resources = {
        'connectors': [],
        'global_configs': [],
        'error_handlers': [],
        'security_configs': [],
        'schedulers': [],
        'transformers': [],
        'spring_beans': [],
        'other': []
    }
    used_namespaces = set()

    config_path = os.path.join(domain_path, "src", "main", "mule", "mule-domain-config.xml")
    tree = ET.parse(config_path)
    root = tree.getroot()

    # Collect used namespaces
    for key, value in root.attrib.items():
        if key.startswith('xmlns:'):
            used_namespaces.add(value)

    for child in root:
        if '}' in child.tag:
            namespace = child.tag.split('}')[0][1:]  # Extract namespace
            used_namespaces.add(namespace)
            tag = child.tag.split('}')[1]
            if any(x in tag for x in ['config', 'configuration']):
                resources['connectors'].append(child)
            elif tag == 'global-property':
                resources['global_configs'].append(child)
            elif tag == 'error-handler':
                resources['error_handlers'].append(child)
            elif 'security' in tag or 'oauth' in tag:
                resources['security_configs'].append(child)
            elif 'scheduler' in tag:
                resources['schedulers'].append(child)
            elif 'transformer' in tag:
                resources['transformers'].append(child)
            elif tag == 'beans':
                resources['spring_beans'].append(child)
            else:
                resources['other'].append(child)

    return resources, used_namespaces

def update_project_config(project_path: str, resources: Dict[str, List[ET.Element]], used_namespaces: set):
    config_path = os.path.join(project_path, "src", "main", "mule", "mule-config.xml")
    tree = ET.parse(config_path)
    root = tree.getroot()

    # Add all resources to the project config
    for resource_list in resources.values():
        for resource in resource_list:
            root.append(resource)

    # Update namespaces
    for namespace in used_namespaces:
        for prefix, uri in root.attrib.items():
            if uri == namespace:
                break
        else:
            # If namespace not found, add it with a generated prefix
            i = 1
            while f'xmlns:ns{i}' in root.attrib:
                i += 1
            root.set(f'xmlns:ns{i}', namespace)

    tree.write(config_path, encoding="UTF-8", xml_declaration=True)

def update_project_pom(project_path: str, domain_path: str, used_namespaces: set):
    domain_pom_path = os.path.join(domain_path, "pom.xml")
    project_pom_path = os.path.join(project_path, "pom.xml")

    domain_tree = ET.parse(domain_pom_path)
    domain_root = domain_tree.getroot()

    project_tree = ET.parse(project_pom_path)
    project_root = project_tree.getroot()

    # Transfer dependencies
    domain_deps = domain_root.find("./{*}dependencies")
    if domain_deps is not None:
        project_deps = project_root.find("./{*}dependencies")
        if project_deps is None:
            project_deps = ET.SubElement(project_root, "dependencies")

        for dep in domain_deps:
            if dep.find("./{*}classifier") is None or dep.find("./{*}classifier").text != "mule-domain":
                project_deps.append(dep)

    # Add connector dependencies based on used namespaces
    for namespace in used_namespaces:
        if namespace in CONNECTOR_DEPENDENCY_MAP:
            group_id, artifact_id = CONNECTOR_DEPENDENCY_MAP[namespace]
            new_dep = ET.SubElement(project_deps, "dependency")
            ET.SubElement(new_dep, "groupId").text = group_id
            ET.SubElement(new_dep, "artifactId").text = artifact_id
            # You might want to specify a version or use a property for versioning
            # ET.SubElement(new_dep, "version").text = "${mule.version}"

    # Transfer properties
    domain_props = domain_root.find("./{*}properties")
    if domain_props is not None:
        project_props = project_root.find("./{*}properties")
        if project_props is None:
            project_props = ET.SubElement(project_root, "properties")

        for prop in domain_props:
            if not project_props.find(f".//{prop.tag}"):
                project_props.append(prop)

    # Remove domain dependency
    for dep in project_root.findall(".//{*}dependency"):
        artifact_id = dep.find("./{*}artifactId")
        classifier = dep.find("./{*}classifier")
        if (artifact_id is not None and artifact_id.text == "my-domain-project" and
            classifier is not None and classifier.text == "mule-domain"):
            project_deps.remove(dep)

    project_tree.write(project_pom_path, encoding="UTF-8", xml_declaration=True)

def migrate_project(domain_path: str, project_path: str):
    print(f"Migrating project: {os.path.basename(project_path)}")

    # Parse domain resources and collect used namespaces
    resources, used_namespaces = parse_domain_resources(domain_path)

    # Update project configuration
    update_project_config(project_path, resources, used_namespaces)

    # Update project pom
    update_project_pom(project_path, domain_path, used_namespaces)

    # Copy any additional resources
    domain_resources_path = os.path.join(domain_path, "src", "main", "resources")
    project_resources_path = os.path.join(project_path, "src", "main", "resources")
    if os.path.exists(domain_resources_path):
        for item in os.listdir(domain_resources_path):
            s = os.path.join(domain_resources_path, item)
            d = os.path.join(project_resources_path, item)
            if os.path.isfile(s):
                shutil.copy2(s, d)

    print(f"Migration completed for project: {os.path.basename(project_path)}")

def main():
    domain_path = "/path/to/domain/project"
    projects = [
        "/path/to/project1",
        "/path/to/project2",
        # Add more project paths as needed
    ]

    for project_path in projects:
        migrate_project(domain_path, project_path)

if __name__ == "__main__":
    main()


