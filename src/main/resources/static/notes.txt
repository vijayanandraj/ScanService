Architecture Leadership in MAT Project
Spearheaded the architecture, design, and development of the MAT tool, serving as the primary architect for the product team. This tool stands as a cornerstone in our modernization initiatives.
Pioneered the transition of the Workload Module in MAT 2.0 from Python to Java, laying a robust foundation and enabling dynamic surveys for seamless question updates without code modifications.
Took a mentoring role in critical tasks such as the development of the Dashboard and PDF export features, while also designing and implementing a versatile scoring engine compatible with multiple scan engines like CSA and RedHat MTA.

Websphere Support Enhancements
Authored comprehensive documentation for 54 unique rule groups identified within Websphere, enhancing our team's understanding and efficiency.
Guided the team in the creation of seven major recipes for Websphere, drawing on insights from 45 application scans, and played a pivotal role in integrating MTA, facilitating valuable comparisons and collaborations with the Middleware team.

Advancements in Dotnet Support
Innovated the Scoring Logic for CSA, demonstrating versatility and technical expertise.
Developed a Python script to parse CSA results and Dotnet Metadata, streamlining the data extraction, transformation, and loading process into the MAT System, in collaboration with Dotnet experts to map components to workloads effectively.

Scan as a Service Initiative
Led the architecture, design, and execution of the Scan as a Service module, a critical component in automating numerous manual processes and advancing MAT towards a self-service portal.
Despite a stringent timeline, the module was successfully built and tested within 2-3 weeks, establishing a solid foundation for real-time scan services and facilitating its adaptation by the CRR team for their specific needs.
Mentored the team on this new codebase, ensuring they can independently enhance and extend the service.

Reference Data Service and Innovation
Led the architecture, design, mentorship and implementation of the reference data service for TechADS and MDH.
Introduced the JobRunr Framework/Library as a proof of concept for batch job execution, exploring viable alternatives to Autosys for better compatibility with containerized environments.

TCO Implementation and Analysis
Undertook the challenging and critical task of analyzing projected costs for modernization efforts, navigating the complexities of limited data and undefined design models.

Product Evangelization and Roadmap Development for MAT
Presented MAT in over 15 forums, including Innovation Councils and team meetings, showcasing its capabilities and value to a wide range of stakeholders.
Developed extensive content focused on modernization strategies and the specifics of MAT, demonstrating deep expertise and thought leadership in this domain.
Crafted a comprehensive 'Big Picture' document, outlining a clear and strategic roadmap for the future development and implementation of MAT.
Created an impactful Value Proposition Deck for MAT, tailored for leadership consumption.
This presentation made a significant impression during the visits of Craig and Gully, highlighting MAT's potential and aligning with organizational goals.


Contributions to CDE and GBS Teams
Pioneering two patents based on MAT, with plans to actively pursue them starting next year. This initiative represents a significant leap in our technological and intellectual property capabilities.
Recognized as the go-to expert for architecture and technical guidance within the CDE team. Consistently provided mentoring, consulting, and code support, enhancing team performance and knowledge.
Played a key role in multiple OPEX initiatives, contributing to the collection and analysis of crucial data, thereby driving efficiency and innovation in our operations.
Actively involved in the solutioning for the EET proposal, focusing on aligning layout with the guiding principles of Modernization and Vulnerability Management.
Currently developing an Automated Vulnerability Solution, designed to streamline vulnerability management processes. Successfully conducted a proof of concept, laying the groundwork for an end-to-end solution that promises to significantly reduce associated challenges.

Played a pivotal role in the exceptional growth of our team, which expanded from 5 to 50 members within a year. My primary focus was on ensuring a smooth assimilation of new members, accelerating their journey to becoming productive and integrated team players.
Leveraging my people skills and approachable nature, I established an open and supportive environment where team members felt comfortable seeking guidance and asking questions, no matter how small.
Actively participated in and often spearheaded team events, lunches, and parties, which were instrumental in strengthening team bonds and camaraderie. These efforts helped in cultivating a team spirit that resonates with enthusiasm and collaboration.
I take immense pride in being part of this team. It's truly one of the best I've had the pleasure of working with in recent times, marked by a supportive, inclusive, and highly productive atmosphere.

* Areas of Improvement
One of the challenges I've encountered stems from my long tenure of 12 years at the bank, including the last 5 years in the Cloud team.
My practical experience in running modernization initiatives is somewhat limited.
This has occasionally made it challenging to develop and justify modernization strategies and TCO assessments, as these often rely on external literature rather than on in-house data and experiences.
Recognizing this gap, I see an opportunity for improvement by potentially bringing in external/intra-bank expertise. This addition could offer fresh perspectives, practical experience in cloud adoption and modernization, and assist in making more data-driven decisions. Such support could not only enhance our strategies but also strengthen our ability to defend our decisions with solid evidence and real-world examples.

AutoVulnHealer: The Comprehensive Container Security Solution

AutoVulnHealer is an innovative, automated tool designed to enhance the security and efficiency of containerized environments. Its core functionality revolves around the identification, management, and remediation of vulnerabilities in container images, making it an essential asset for modern DevOps and IT security teams.

Key Features:

Vulnerability Assessment and Management: AutoVulnHealer scans container images for known vulnerabilities, leveraging a comprehensive database of CVEs (Common Vulnerabilities and Exposures). It ensures that all potential security risks are identified and categorized for action.

Unused Image Detection and Cleanup: The tool intelligently identifies container images that are not actively used in pods. It integrates with JFrog Artifactory API to safely remove these unused images, thereby reducing clutter and potential attack vectors.

Active Image Analysis and Reporting: For images in use, AutoVulnHealer generates detailed reports. These reports include information on the pod name, OpenShift namespace, image name, and associated unique vulnerabilities. This data is compiled into an accessible Excel file for easy review and auditing.

Automated Remediation Planning: Perhaps its most innovative feature, AutoVulnHealer can automatically generate an action plan or 'recipe' to address identified vulnerabilities. This proactive approach streamlines the process of securing container environments.

Future Integration Capabilities: Looking ahead, AutoVulnHealer is poised for integration with Horizon Insights/Artifactory. This will enable checks on whether the images are the latest release versions and provide critical support for rollback operations.

Use Cases:

DevOps Teams: Streamlining the process of identifying and fixing vulnerabilities in container environments.
Security Professionals: Offering a robust tool for proactive security management in containerized applications.
IT Administrators: Providing a comprehensive overview of container image usage and security status.
Benefits:

Enhanced Security: By keeping container environments free from known vulnerabilities and unused images.
Operational Efficiency: Automating routine tasks saves time and reduces the likelihood of human error.
Compliance and Reporting: Facilitating better compliance with security standards and simplifying audit processes.
In summary, AutoVulnHealer is not just a tool; it's a comprehensive solution for ensuring the security and efficiency of containerized applications in a fast-paced, ever-evolving tech landscape.



def get_pods_in_namespace(oc_client, namespace):
    v1_pods = oc_client.resources.get(api_version='v1', kind='Pod')
    pods = v1_pods.get(namespace=namespace)
    return pods

def is_image_in_use(oc_client, namespace, image):
    v1_pods = oc_client.resources.get(api_version='v1', kind='Pod')
    pods = v1_pods.get(namespace=namespace)

    for pod in pods.items:
        containers = pod.spec.containers
        for container in containers:
            if container.image == image:
                print(f"Image {image} found in pod {pod.metadata.name} in namespace {namespace}")
                return True
    return False

import requests
import json

OPENSHIFT_API_URL = "https://your-openshift-api-server-url"
USERNAME = "your-username"
PASSWORD = "your-password"

def get_oauth_token():
    data = {
        'grant_type': 'password',
        'client_id': 'openshift-challenging-client',
        'username': USERNAME,
        'password': PASSWORD
    }
    headers = {
        'Content-Type': 'application/x-www-form-urlencoded',
        'Accept': 'application/json'
    }
    response = requests.post(f"{OPENSHIFT_API_URL}/oauth/authorize", data=data, headers=headers, verify=False)
    if response.status_code == 200:
        return json.loads(response.text)['access_token']
    else:
        print("Failed to obtain token")
        return None

def get_running_pods(token):
    # Example function to get running pods
    headers = {
        'Authorization': f'Bearer {token}',
    }
    # ... rest of your function ...

# Main execution
def main():
    token = get_oauth_token()
    if token:
        pods = get_running_pods(token)
        # ... rest of your script ...
    else:
        print("Authentication failed")

if __name__ == "__main__":
    main()



```python
import requests
import json

# Replace with your OpenShift API URL and the specific image you're looking for
OPENSHIFT_API_URL = "https://your-openshift-api-server-url"
SPECIFIC_IMAGE = "your-artifactory-image"

# Function to get all running pods
def get_running_pods():
    url = f"{OPENSHIFT_API_URL}/api/v1/pods"
    headers = {
        'Authorization': 'Bearer YOUR_ACCESS_TOKEN',
    }
    response = requests.get(url, headers=headers, verify=False) # Set verify to True in a production environment
    if response.status_code == 200:
        return json.loads(response.text)
    else:
        print("Failed to fetch pods")
        return None

# Function to check if the specific image is in use
def is_image_in_use(pods, image):
    for pod in pods['items']:
        containers = pod['spec']['containers']
        for container in containers:
            if container['image'] == image:
                return True
    return False

# Main execution
def main():
    pods = get_running_pods()
    if pods:
        image_in_use = is_image_in_use(pods, SPECIFIC_IMAGE)
        if image_in_use:
            print(f"The image {SPECIFIC_IMAGE} is in use.")
        else:
            print(f"The image {SPECIFIC_IMAGE} is not in use.")
    else:
        print("No pod information available.")

if __name__ == "__main__":
    main()
```




Commiting changes from ChatGPT text. This can be a game changer for work.

DECLARE @TableName NVARCHAR(256) = 'OriginalTable';
DECLARE @SQL NVARCHAR(MAX) = '';

SELECT @SQL += 'CREATE ' +
    CASE WHEN i.is_primary_key = 1 THEN 'PRIMARY KEY '
         WHEN i.is_unique = 1 THEN 'UNIQUE '
         ELSE ''
    END +
    i.type_desc + ' INDEX ' + i.name + ' ON ' + @TableName +
    ' (' +
    STRING_AGG(c.name, ', ') WITHIN GROUP (ORDER BY ic.key_ordinal) +
    ')' + CHAR(13) + CHAR(10)
FROM sys.indexes i
INNER JOIN sys.index_columns ic ON i.object_id = ic.object_id AND i.index_id = ic.index_id
INNER JOIN sys.columns c ON ic.object_id = c.object_id AND ic.column_id = c.column_id
WHERE i.object_id = OBJECT_ID(@TableName) AND i.is_primary_key = 0 AND i.is_unique_constraint = 0
GROUP BY i.object_id, i.index_id, i.name, i.is_primary_key, i.is_unique, i.type_desc;

PRINT @SQL;


@Override
@Job(name = "MDH_AIT_Infra_Job", retries = 0)
public void runETLTaskInBatch() {
    int offset = 0;
    Set<Integer> exclusionSet = getExcludedAitIds();

    List<Map<String, Object>> batch;
    do {
        batch = extractBatch(offset, BATCH_SIZE);

        // Collect unique AIT numbers for deletion
        Set<Integer> uniqueAitNumbers = new HashSet<>();
        List<Map<String, Object>> filteredBatch = new ArrayList<>();

        for (Map<String, Object> row : batch) {
            int aitNumber = ((Number) row.get("aitnumber")).intValue();
            uniqueAitNumbers.add(aitNumber);

            // Apply exclusion logic
            if (!exclusionSet.contains(aitNumber)) {
                filteredBatch.add(row);
            }
        }

        // Perform deletion for AIT numbers not previously deleted
        performDeletion(uniqueAitNumbers);

        // Transform and load filtered batch
        List<Map<String, Object>> transformedBatch = transformBatch(filteredBatch);
        loadBatch(transformedBatch);

        offset += batch.size();
    } while (batch.size() == BATCH_SIZE);
}

private void performDeletion(Set<Integer> uniqueAitNumbers) {
    // Filter unique AIT numbers to find which ones haven't been deleted yet
    Set<Integer> newAitNumbersForDeletion = uniqueAitNumbers.stream()
        .filter(aitNumber -> !deletedAitNumbers.contains(aitNumber))
        .collect(Collectors.toSet());

    // Perform deletion for new AIT numbers
    if (!newAitNumbersForDeletion.isEmpty()) {
        deleteAitNumbers(newAitNumbersForDeletion);
        deletedAitNumbers.addAll(newAitNumbersForDeletion);
    }
}

private void deleteAitNumbers(Set<Integer> aitNumbers) {
    // Build the SQL query with IN clause
    String sql = "DELETE FROM ait_infra WHERE ait_id IN (:aitNumbers)";
    Map<String, Object> params = Map.of("aitNumbers", aitNumbers);
    sqlServerJdbcTemplate.update(sql, params);
}



columnMapping.put("address", "address2");
columnMapping.put("city", "address3");
columnMapping.put("state", "address4");
columnMapping.put("zip_code", "address5");
columnMapping.put("cputype", "cpu_model");
columnMapping.put("cpuspeed", "cpuspeed");
columnMapping.put("datacenter", "dc_name");
columnMapping.put("regioncode", "dc_region");
columnMapping.put("environment", "environment");
columnMapping.put("fqdn", "fqdn");
columnMapping.put("hostname", "host_name");
columnMapping.put("is_dmz", "is_dmz");
columnMapping.put("is_virtual", "is_virtual");
columnMapping.put("site_name", "location description");
columnMapping.put("memory", "memory");
columnMapping.put("model", "model");
columnMapping.put("ait_aligning_cio_org", "org");
columnMapping.put("os_name", "os_name");
columnMapping.put("os_type", "os_type");
columnMapping.put("os_version", "os_version");
columnMapping.put("server_function", "server_function");
columnMapping.put("three_dot_hirerachy", "threedot_hirerachy");
columnMapping.put("corecount", "total_cores");
columnMapping.put("ait_number", "ait_id");



<dependency>
    <groupId>com.github.javafaker</groupId>
    <artifactId>javafaker</artifactId>
    <version>1.0.2</version>
    <scope>test</scope>
</dependency>


import com.github.javafaker.Faker;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import org.mockito.Mock;
import org.mockito.MockitoAnnotations;
import org.springframework.jdbc.core.namedparam.NamedParameterJdbcTemplate;

import java.util.List;
import java.util.Map;

import static org.mockito.ArgumentMatchers.any;
import static org.mockito.Mockito.*;

class EtlServiceTest {

    @Mock
    private NamedParameterJdbcTemplate sourceJdbcTemplate; // Mock for source DB

    @Mock
    private NamedParameterJdbcTemplate targetJdbcTemplate; // Mock for target DB

	private EtlService etlService;


	    @BeforeEach
    void setUp() {
        MockitoAnnotations.openMocks(this);
        etlService = new EtlService();

        // Inject the mocks into the private fields of the EtlService instance
        ReflectionTestUtils.setField(etlService, "sourceJdbcTemplate", sourceJdbcTemplate);
        ReflectionTestUtils.setField(etlService, "targetJdbcTemplate", targetJdbcTemplate);
    }

    // Method to generate mock source data
    private List<Map<String, Object>> createMockSourceData() {
		Faker faker = new Faker();
		List<Map<String, Object>> mockData = new ArrayList<>();

		for (int i = 0; i < 10; i++) { // Generate 10 rows of data
			Map<String, Object> row = new HashMap<>();
			// Using the source column names as per your mappings
			row.put("ADDRESS", faker.address().streetAddress()); // Source for target column "ADDRESS2"
			row.put("CITY", faker.address().city());             // Source for "ADDRESS3"
			row.put("STATE", faker.address().state());           // Source for "ADDRESS4"
			row.put("ZIP_CODE", faker.address().zipCode());      // Source for "ADDRESS5"
			row.put("CPUTYPE", faker.app().name());              // Source for "CPU_MODEL"
			row.put("CPUSPEED", faker.number().randomDigitNotZero() + " GHz"); // Source for "CPUSPEED"
			row.put("DATACENTER", "DataCenter" + faker.number().digit()); // Source for "DC_NAME"
			row.put("REGIONCODE", faker.country().countryCode2()); // Source for "DC_REGION"
			row.put("ENVIRONMENT", faker.options().option("Production", "Development", "Testing")); // Source for "ENVIRONMENT"
			row.put("FQDN", faker.internet().domainName());      // Source for "FQDN"
			row.put("HOSTNAME", faker.internet().domainWord());  // Source for "HOST_NAME"
			row.put("IS_DMZ", faker.options().option("yes", "no")); // Source for "IS_DMZ"
			row.put("IS_VIRTUAL", faker.bool().bool() ? "yes" : "no"); // Source for "IS_VIRTUAL"
			row.put("SITE_NAME", faker.company().industry());    // Source for "LOCATION DESCRIPTION"
			row.put("MEMORY", faker.number().numberBetween(4, 64) + " GB"); // Source for "MEMORY"
			row.put("MODEL", faker.commerce().productName());    // Source for "MODEL"
			row.put("AIT_ALIGNING_CIO_ORG", faker.company().name()); // Source for "ORG"
			row.put("OS_NAME", faker.operatingSystem().name());  // Source for "OS_NAME"
			row.put("OS_TYPE", faker.programmingLanguage().name()); // Source for "OS_TYPE"
			row.put("OS_VERSION", faker.app().version());        // Source for "OS_VERSION"
			row.put("SERVER_FUNCTION", faker.company().profession()); // Source for "SERVER_FUNCTION"
			row.put("THREE_DOT_HIRERACHY", faker.company().bs()); // Source for "THREEDOT_HIRERACHY"
			row.put("CORECOUNT", faker.number().numberBetween(1, 16)); // Source for "TOTAL_CORES"
			row.put("AIT_NUMBER", faker.number().randomDouble(2, 1, 1000));  // Source for "ait_id"

			mockData.add(row);
		}

		return mockData;

    }

    // Test the ETL process
    @Test
    void performEtlTest() {
        // Arrange: Create mock source data
        List<Map<String, Object>> mockSourceData = createMockSourceData();

        // Set up the mock behavior for the extractBatch method
        when(namedParameterJdbcTemplate.queryForList(any(String.class), any(Map.class)))
            .thenReturn(mockSourceData)
            .thenReturn(List.of()); // To simulate the end of data stream

        // Act: Perform the ETL process
        etlService.performEtl();

        // Assert: Verify the interactions with the mock
        // Verify extractBatch was called at least once
        verify(namedParameterJdbcTemplate, atLeastOnce()).queryForList(any(String.class), any(Map.class));

        // Verify loadBatch was called with the correct transformed data
        // You might need to customize this part based on how you implement the loadBatch method
        verify(namedParameterJdbcTemplate, times(1))
            .batchUpdate(any(String.class), any(Map[].class));
    }
}



import numpy as np

# Generate some synthetic CPU utilization data as an example
np.random.seed(42)
data_points = np.random.randint(0, 100, 43200)

# Sort the data
sorted_data = np.sort(data_points)

# Calculate the 95th percentile index
index_95th = int((95 / 100) * len(sorted_data))

# Get the 95th percentile value
value_95th = sorted_data[index_95th]




# Importing pandas and recalculating the target number of cores

import pandas as pd

data = {
    'Server name': ['Server1', 'Server2', 'Server3', 'Server4', 'Server5', 'Server6', 'Server7', 'Server8'],
    'Total Cores': [12, 12, 12, 12, 16, 16, 16, 16],
    'March Utilization': [3.98, 2.98, 1.98, 10.98, 3.71, 4.98, 3.71, 16.07],
    'April Utilization': [4.03, 5.03, 8.03, 45.50, 3.06, 7.50, 3.06, 24.21],
    'May Utilization': [5.67, 8.67, 4.67, 3.67, 2.73, 3.67, 2.73, 23.67],
    'June Utilization': [3.98, 12.98, 11.98, 4.98, 11.53, 4.98, 11.53, 4.98],
    'July Utilization': [10.91, 12.91, 10.91, 2.91, 18.77, 2.91, 18.77, 2.91],
    'August Utilization': [69.1, 0.74, 1.74, 3.28, 27.10, 3.28, 27.10, 33.28]
}

#df = pd.read_excel("/path/to/your/excel/file.xlsx", sheet_name="Sheet1")

df = pd.DataFrame(data)



# Calculate monthly peak core utilization for each server
for month in ['March Utilization', 'April Utilization', 'May Utilization', 'June Utilization', 'July Utilization', 'August Utilization']:
    df[f'{month} Cores'] = df[month] * df['Total Cores']

# Aggregate monthly peaks
monthly_peaks = df[[f'{month} Cores' for month in ['March Utilization', 'April Utilization', 'May Utilization', 'June Utilization', 'July Utilization', 'August Utilization']]].sum()

# Determine overall peak utilization
overall_peak = monthly_peaks.max()

# Consider 20% headroom
headroom = 0.20 * overall_peak

# Calculate total target cores
target_cores = overall_peak + headroom

target_cores

# Compute monthly peaks for each server
monthly_peaks_df = df[[f'{month} Cores' for month in ['March Utilization', 'April Utilization', 'May Utilization', 'June Utilization', 'July Utilization', 'August Utilization']]]

# Compute the median of peaks for each month
median_peaks = monthly_peaks_df.median()

# Anomaly detection using Interquartile Range (IQR)
Q1 = median_peaks.quantile(0.25)
Q3 = median_peaks.quantile(0.75)
IQR = Q3 - Q1

# Define bounds for outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Filter out outliers
adjusted_peaks = median_peaks[(median_peaks >= lower_bound) & (median_peaks <= upper_bound)]

# Compute the adjusted peak and add 20% headroom
adjusted_peak = adjusted_peaks.max()
headroom = 0.20 * adjusted_peak
target_cores_adjusted = adjusted_peak + headroom

target_cores_adjusted


@Service
public class EtlService {

    @Autowired
    private JdbcTemplate jdbcTemplate;

    public void startEtlProcess() {
        List<String> allAitIds = getAllAitIds(); // Assume this gets all unique AIT_IDs using native SQL

        List<CompletableFuture<Void>> futures = new ArrayList<>();
        for (String aitId : allAitIds) {
            CompletableFuture<Void> future = processAitId(aitId);
            futures.add(future);
        }

        CompletableFuture.allOf(futures.toArray(new CompletableFuture[0])).join();
    }

    @Async("taskExecutor")
    public CompletableFuture<Void> processAitId(String aitId) {
        // Fetch records for a specific AIT_ID
        String fetchQuery = "SELECT * FROM source_table WHERE AIT_ID = ?";
        List<Map<String, Object>> fetchedRecords = jdbcTemplate.queryForList(fetchQuery, new Object[]{aitId});

        // Transformation logic
        List<Map<String, Object>> transformedRecords = transform(fetchedRecords); // Implement your own transform method

        // Batch insert into the target table
        String insertQuery = "INSERT INTO target_table (column1, column2, ...) VALUES (?, ?, ...)";
        List<Object[]> batchArgsList = new ArrayList<>();
        for (Map<String, Object> record : transformedRecords) {
            Object[] args = {record.get("column1"), record.get("column2"), /*...*/};
            batchArgsList.add(args);
        }

        jdbcTemplate.batchUpdate(insertQuery, batchArgsList);
        return CompletableFuture.completedFuture(null);
    }

    private List<Map<String, Object>> transform(List<Map<String, Object>> records) {
        // Your transformation logic here
        return records; // Return the transformed records
    }
}
