def get_pods_in_namespace(oc_client, namespace):
    v1_pods = oc_client.resources.get(api_version='v1', kind='Pod')
    pods = v1_pods.get(namespace=namespace)
    return pods

def is_image_in_use(oc_client, namespace, image):
    v1_pods = oc_client.resources.get(api_version='v1', kind='Pod')
    pods = v1_pods.get(namespace=namespace)

    for pod in pods.items:
        containers = pod.spec.containers
        for container in containers:
            if container.image == image:
                print(f"Image {image} found in pod {pod.metadata.name} in namespace {namespace}")
                return True
    return False

import requests
import json

OPENSHIFT_API_URL = "https://your-openshift-api-server-url"
USERNAME = "your-username"
PASSWORD = "your-password"

def get_oauth_token():
    data = {
        'grant_type': 'password',
        'client_id': 'openshift-challenging-client',
        'username': USERNAME,
        'password': PASSWORD
    }
    headers = {
        'Content-Type': 'application/x-www-form-urlencoded',
        'Accept': 'application/json'
    }
    response = requests.post(f"{OPENSHIFT_API_URL}/oauth/authorize", data=data, headers=headers, verify=False)
    if response.status_code == 200:
        return json.loads(response.text)['access_token']
    else:
        print("Failed to obtain token")
        return None

def get_running_pods(token):
    # Example function to get running pods
    headers = {
        'Authorization': f'Bearer {token}',
    }
    # ... rest of your function ...

# Main execution
def main():
    token = get_oauth_token()
    if token:
        pods = get_running_pods(token)
        # ... rest of your script ...
    else:
        print("Authentication failed")

if __name__ == "__main__":
    main()



```python
import requests
import json

# Replace with your OpenShift API URL and the specific image you're looking for
OPENSHIFT_API_URL = "https://your-openshift-api-server-url"
SPECIFIC_IMAGE = "your-artifactory-image"

# Function to get all running pods
def get_running_pods():
    url = f"{OPENSHIFT_API_URL}/api/v1/pods"
    headers = {
        'Authorization': 'Bearer YOUR_ACCESS_TOKEN',
    }
    response = requests.get(url, headers=headers, verify=False) # Set verify to True in a production environment
    if response.status_code == 200:
        return json.loads(response.text)
    else:
        print("Failed to fetch pods")
        return None

# Function to check if the specific image is in use
def is_image_in_use(pods, image):
    for pod in pods['items']:
        containers = pod['spec']['containers']
        for container in containers:
            if container['image'] == image:
                return True
    return False

# Main execution
def main():
    pods = get_running_pods()
    if pods:
        image_in_use = is_image_in_use(pods, SPECIFIC_IMAGE)
        if image_in_use:
            print(f"The image {SPECIFIC_IMAGE} is in use.")
        else:
            print(f"The image {SPECIFIC_IMAGE} is not in use.")
    else:
        print("No pod information available.")

if __name__ == "__main__":
    main()
```




Commiting changes from ChatGPT text. This can be a game changer for work.

DECLARE @TableName NVARCHAR(256) = 'OriginalTable';
DECLARE @SQL NVARCHAR(MAX) = '';

SELECT @SQL += 'CREATE ' +
    CASE WHEN i.is_primary_key = 1 THEN 'PRIMARY KEY '
         WHEN i.is_unique = 1 THEN 'UNIQUE '
         ELSE ''
    END +
    i.type_desc + ' INDEX ' + i.name + ' ON ' + @TableName +
    ' (' +
    STRING_AGG(c.name, ', ') WITHIN GROUP (ORDER BY ic.key_ordinal) +
    ')' + CHAR(13) + CHAR(10)
FROM sys.indexes i
INNER JOIN sys.index_columns ic ON i.object_id = ic.object_id AND i.index_id = ic.index_id
INNER JOIN sys.columns c ON ic.object_id = c.object_id AND ic.column_id = c.column_id
WHERE i.object_id = OBJECT_ID(@TableName) AND i.is_primary_key = 0 AND i.is_unique_constraint = 0
GROUP BY i.object_id, i.index_id, i.name, i.is_primary_key, i.is_unique, i.type_desc;

PRINT @SQL;


@Override
@Job(name = "MDH_AIT_Infra_Job", retries = 0)
public void runETLTaskInBatch() {
    int offset = 0;
    Set<Integer> exclusionSet = getExcludedAitIds();

    List<Map<String, Object>> batch;
    do {
        batch = extractBatch(offset, BATCH_SIZE);

        // Collect unique AIT numbers for deletion
        Set<Integer> uniqueAitNumbers = new HashSet<>();
        List<Map<String, Object>> filteredBatch = new ArrayList<>();

        for (Map<String, Object> row : batch) {
            int aitNumber = ((Number) row.get("aitnumber")).intValue();
            uniqueAitNumbers.add(aitNumber);

            // Apply exclusion logic
            if (!exclusionSet.contains(aitNumber)) {
                filteredBatch.add(row);
            }
        }

        // Perform deletion for AIT numbers not previously deleted
        performDeletion(uniqueAitNumbers);

        // Transform and load filtered batch
        List<Map<String, Object>> transformedBatch = transformBatch(filteredBatch);
        loadBatch(transformedBatch);

        offset += batch.size();
    } while (batch.size() == BATCH_SIZE);
}

private void performDeletion(Set<Integer> uniqueAitNumbers) {
    // Filter unique AIT numbers to find which ones haven't been deleted yet
    Set<Integer> newAitNumbersForDeletion = uniqueAitNumbers.stream()
        .filter(aitNumber -> !deletedAitNumbers.contains(aitNumber))
        .collect(Collectors.toSet());

    // Perform deletion for new AIT numbers
    if (!newAitNumbersForDeletion.isEmpty()) {
        deleteAitNumbers(newAitNumbersForDeletion);
        deletedAitNumbers.addAll(newAitNumbersForDeletion);
    }
}

private void deleteAitNumbers(Set<Integer> aitNumbers) {
    // Build the SQL query with IN clause
    String sql = "DELETE FROM ait_infra WHERE ait_id IN (:aitNumbers)";
    Map<String, Object> params = Map.of("aitNumbers", aitNumbers);
    sqlServerJdbcTemplate.update(sql, params);
}



columnMapping.put("address", "address2");
columnMapping.put("city", "address3");
columnMapping.put("state", "address4");
columnMapping.put("zip_code", "address5");
columnMapping.put("cputype", "cpu_model");
columnMapping.put("cpuspeed", "cpuspeed");
columnMapping.put("datacenter", "dc_name");
columnMapping.put("regioncode", "dc_region");
columnMapping.put("environment", "environment");
columnMapping.put("fqdn", "fqdn");
columnMapping.put("hostname", "host_name");
columnMapping.put("is_dmz", "is_dmz");
columnMapping.put("is_virtual", "is_virtual");
columnMapping.put("site_name", "location description");
columnMapping.put("memory", "memory");
columnMapping.put("model", "model");
columnMapping.put("ait_aligning_cio_org", "org");
columnMapping.put("os_name", "os_name");
columnMapping.put("os_type", "os_type");
columnMapping.put("os_version", "os_version");
columnMapping.put("server_function", "server_function");
columnMapping.put("three_dot_hirerachy", "threedot_hirerachy");
columnMapping.put("corecount", "total_cores");
columnMapping.put("ait_number", "ait_id");



<dependency>
    <groupId>com.github.javafaker</groupId>
    <artifactId>javafaker</artifactId>
    <version>1.0.2</version>
    <scope>test</scope>
</dependency>


import com.github.javafaker.Faker;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import org.mockito.Mock;
import org.mockito.MockitoAnnotations;
import org.springframework.jdbc.core.namedparam.NamedParameterJdbcTemplate;

import java.util.List;
import java.util.Map;

import static org.mockito.ArgumentMatchers.any;
import static org.mockito.Mockito.*;

class EtlServiceTest {

    @Mock
    private NamedParameterJdbcTemplate sourceJdbcTemplate; // Mock for source DB

    @Mock
    private NamedParameterJdbcTemplate targetJdbcTemplate; // Mock for target DB

	private EtlService etlService;


	    @BeforeEach
    void setUp() {
        MockitoAnnotations.openMocks(this);
        etlService = new EtlService();

        // Inject the mocks into the private fields of the EtlService instance
        ReflectionTestUtils.setField(etlService, "sourceJdbcTemplate", sourceJdbcTemplate);
        ReflectionTestUtils.setField(etlService, "targetJdbcTemplate", targetJdbcTemplate);
    }

    // Method to generate mock source data
    private List<Map<String, Object>> createMockSourceData() {
		Faker faker = new Faker();
		List<Map<String, Object>> mockData = new ArrayList<>();

		for (int i = 0; i < 10; i++) { // Generate 10 rows of data
			Map<String, Object> row = new HashMap<>();
			// Using the source column names as per your mappings
			row.put("ADDRESS", faker.address().streetAddress()); // Source for target column "ADDRESS2"
			row.put("CITY", faker.address().city());             // Source for "ADDRESS3"
			row.put("STATE", faker.address().state());           // Source for "ADDRESS4"
			row.put("ZIP_CODE", faker.address().zipCode());      // Source for "ADDRESS5"
			row.put("CPUTYPE", faker.app().name());              // Source for "CPU_MODEL"
			row.put("CPUSPEED", faker.number().randomDigitNotZero() + " GHz"); // Source for "CPUSPEED"
			row.put("DATACENTER", "DataCenter" + faker.number().digit()); // Source for "DC_NAME"
			row.put("REGIONCODE", faker.country().countryCode2()); // Source for "DC_REGION"
			row.put("ENVIRONMENT", faker.options().option("Production", "Development", "Testing")); // Source for "ENVIRONMENT"
			row.put("FQDN", faker.internet().domainName());      // Source for "FQDN"
			row.put("HOSTNAME", faker.internet().domainWord());  // Source for "HOST_NAME"
			row.put("IS_DMZ", faker.options().option("yes", "no")); // Source for "IS_DMZ"
			row.put("IS_VIRTUAL", faker.bool().bool() ? "yes" : "no"); // Source for "IS_VIRTUAL"
			row.put("SITE_NAME", faker.company().industry());    // Source for "LOCATION DESCRIPTION"
			row.put("MEMORY", faker.number().numberBetween(4, 64) + " GB"); // Source for "MEMORY"
			row.put("MODEL", faker.commerce().productName());    // Source for "MODEL"
			row.put("AIT_ALIGNING_CIO_ORG", faker.company().name()); // Source for "ORG"
			row.put("OS_NAME", faker.operatingSystem().name());  // Source for "OS_NAME"
			row.put("OS_TYPE", faker.programmingLanguage().name()); // Source for "OS_TYPE"
			row.put("OS_VERSION", faker.app().version());        // Source for "OS_VERSION"
			row.put("SERVER_FUNCTION", faker.company().profession()); // Source for "SERVER_FUNCTION"
			row.put("THREE_DOT_HIRERACHY", faker.company().bs()); // Source for "THREEDOT_HIRERACHY"
			row.put("CORECOUNT", faker.number().numberBetween(1, 16)); // Source for "TOTAL_CORES"
			row.put("AIT_NUMBER", faker.number().randomDouble(2, 1, 1000));  // Source for "ait_id"

			mockData.add(row);
		}

		return mockData;

    }

    // Test the ETL process
    @Test
    void performEtlTest() {
        // Arrange: Create mock source data
        List<Map<String, Object>> mockSourceData = createMockSourceData();

        // Set up the mock behavior for the extractBatch method
        when(namedParameterJdbcTemplate.queryForList(any(String.class), any(Map.class)))
            .thenReturn(mockSourceData)
            .thenReturn(List.of()); // To simulate the end of data stream

        // Act: Perform the ETL process
        etlService.performEtl();

        // Assert: Verify the interactions with the mock
        // Verify extractBatch was called at least once
        verify(namedParameterJdbcTemplate, atLeastOnce()).queryForList(any(String.class), any(Map.class));

        // Verify loadBatch was called with the correct transformed data
        // You might need to customize this part based on how you implement the loadBatch method
        verify(namedParameterJdbcTemplate, times(1))
            .batchUpdate(any(String.class), any(Map[].class));
    }
}



import numpy as np

# Generate some synthetic CPU utilization data as an example
np.random.seed(42)
data_points = np.random.randint(0, 100, 43200)

# Sort the data
sorted_data = np.sort(data_points)

# Calculate the 95th percentile index
index_95th = int((95 / 100) * len(sorted_data))

# Get the 95th percentile value
value_95th = sorted_data[index_95th]




# Importing pandas and recalculating the target number of cores

import pandas as pd

data = {
    'Server name': ['Server1', 'Server2', 'Server3', 'Server4', 'Server5', 'Server6', 'Server7', 'Server8'],
    'Total Cores': [12, 12, 12, 12, 16, 16, 16, 16],
    'March Utilization': [3.98, 2.98, 1.98, 10.98, 3.71, 4.98, 3.71, 16.07],
    'April Utilization': [4.03, 5.03, 8.03, 45.50, 3.06, 7.50, 3.06, 24.21],
    'May Utilization': [5.67, 8.67, 4.67, 3.67, 2.73, 3.67, 2.73, 23.67],
    'June Utilization': [3.98, 12.98, 11.98, 4.98, 11.53, 4.98, 11.53, 4.98],
    'July Utilization': [10.91, 12.91, 10.91, 2.91, 18.77, 2.91, 18.77, 2.91],
    'August Utilization': [69.1, 0.74, 1.74, 3.28, 27.10, 3.28, 27.10, 33.28]
}

#df = pd.read_excel("/path/to/your/excel/file.xlsx", sheet_name="Sheet1")

df = pd.DataFrame(data)



# Calculate monthly peak core utilization for each server
for month in ['March Utilization', 'April Utilization', 'May Utilization', 'June Utilization', 'July Utilization', 'August Utilization']:
    df[f'{month} Cores'] = df[month] * df['Total Cores']

# Aggregate monthly peaks
monthly_peaks = df[[f'{month} Cores' for month in ['March Utilization', 'April Utilization', 'May Utilization', 'June Utilization', 'July Utilization', 'August Utilization']]].sum()

# Determine overall peak utilization
overall_peak = monthly_peaks.max()

# Consider 20% headroom
headroom = 0.20 * overall_peak

# Calculate total target cores
target_cores = overall_peak + headroom

target_cores

# Compute monthly peaks for each server
monthly_peaks_df = df[[f'{month} Cores' for month in ['March Utilization', 'April Utilization', 'May Utilization', 'June Utilization', 'July Utilization', 'August Utilization']]]

# Compute the median of peaks for each month
median_peaks = monthly_peaks_df.median()

# Anomaly detection using Interquartile Range (IQR)
Q1 = median_peaks.quantile(0.25)
Q3 = median_peaks.quantile(0.75)
IQR = Q3 - Q1

# Define bounds for outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Filter out outliers
adjusted_peaks = median_peaks[(median_peaks >= lower_bound) & (median_peaks <= upper_bound)]

# Compute the adjusted peak and add 20% headroom
adjusted_peak = adjusted_peaks.max()
headroom = 0.20 * adjusted_peak
target_cores_adjusted = adjusted_peak + headroom

target_cores_adjusted


@Service
public class EtlService {

    @Autowired
    private JdbcTemplate jdbcTemplate;

    public void startEtlProcess() {
        List<String> allAitIds = getAllAitIds(); // Assume this gets all unique AIT_IDs using native SQL

        List<CompletableFuture<Void>> futures = new ArrayList<>();
        for (String aitId : allAitIds) {
            CompletableFuture<Void> future = processAitId(aitId);
            futures.add(future);
        }

        CompletableFuture.allOf(futures.toArray(new CompletableFuture[0])).join();
    }

    @Async("taskExecutor")
    public CompletableFuture<Void> processAitId(String aitId) {
        // Fetch records for a specific AIT_ID
        String fetchQuery = "SELECT * FROM source_table WHERE AIT_ID = ?";
        List<Map<String, Object>> fetchedRecords = jdbcTemplate.queryForList(fetchQuery, new Object[]{aitId});

        // Transformation logic
        List<Map<String, Object>> transformedRecords = transform(fetchedRecords); // Implement your own transform method

        // Batch insert into the target table
        String insertQuery = "INSERT INTO target_table (column1, column2, ...) VALUES (?, ?, ...)";
        List<Object[]> batchArgsList = new ArrayList<>();
        for (Map<String, Object> record : transformedRecords) {
            Object[] args = {record.get("column1"), record.get("column2"), /*...*/};
            batchArgsList.add(args);
        }

        jdbcTemplate.batchUpdate(insertQuery, batchArgsList);
        return CompletableFuture.completedFuture(null);
    }

    private List<Map<String, Object>> transform(List<Map<String, Object>> records) {
        // Your transformation logic here
        return records; // Return the transformed records
    }
}
