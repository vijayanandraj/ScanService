import pandas as pd
import os
import sys
import re

def load_data(file_path):
    """
    Load data from CSV or XLSX file based on file extension.
    """
    try:
        if file_path.endswith('.csv'):
            df = pd.read_csv(file_path)
        elif file_path.endswith(('.xlsx', '.xls')):
            df = pd.read_excel(file_path, engine='openpyxl')  # Specify engine if necessary
        else:
            raise ValueError("Unsupported file format. Please provide a CSV or XLSX file.")
        return df
    except Exception as e:
        print(f"Error loading file: {e}")
        sys.exit(1)

def process_timestamps(df):
    """
    Convert timestamp columns from Unix epoch to datetime.
    Handle missing values for Edit_Time and Delete_Time.
    """
    # Define timestamp columns and their respective conversions
    timestamp_columns = {
        'Create_time': 'create_at',
        'Update_Time': 'update_at',
        'Edit_Time': 'edit_at',
        'Delete_Time': 'delete_at'
    }

    for original, new in timestamp_columns.items():
        if original in df.columns:
            # Convert timestamps; assuming they are in milliseconds
            df[new] = pd.to_datetime(df[original], unit='ms', errors='coerce')
        else:
            print(f"Warning: '{original}' column not found in the data.")
            df[new] = pd.NaT  # Not a Time for missing columns

    return df

def is_user_join_message(message):
    """
    Determine if a message indicates a user joining the channel.
    Adjust the regex pattern based on the exact format of join messages.
    """
    # Example pattern: "<<Nickname>> joins the channel"
    pattern = r'.*joins the channel.*'
    return bool(re.match(pattern, message))

def filter_join_messages(df):
    """
    Remove messages that indicate a user joining the channel.
    """
    if 'Message' not in df.columns:
        print("Error: 'Message' column not found in the data.")
        sys.exit(1)

    # Apply the filter
    join_messages = df['Message'].apply(is_user_join_message)
    filtered_df = df[~join_messages].copy()

    removed_count = join_messages.sum()
    print(f"Filtered out {removed_count} 'user joins the channel' messages.")

    return filtered_df

def process_conversations(df, unique_id_col='Message_ID'):
    """
    Identify root messages and replies based on thread information.
    """
    # Check if 'RootID' column exists
    if 'RootID' not in df.columns:
        print("Error: 'RootID' column not found. Cannot process threaded conversations.")
        sys.exit(1)

    # Identify root messages (original posts) and replies
    root_messages = df[df['RootID'].isnull()].copy()
    replies = df[df['RootID'].notnull()].copy()

    # Check if unique_id_col exists
    if unique_id_col not in root_messages.columns:
        print(f"Error: Unique identifier column '{unique_id_col}' not found in root messages.")
        sys.exit(1)

    # Merge replies with their root messages
    conversations = replies.merge(
        root_messages,
        left_on='RootID',
        right_on=unique_id_col,
        suffixes=('_reply', '_root'),
        how='left'
    )

    # Handle cases where root messages might not be found
    missing_roots = conversations['Message_root'].isnull().sum()
    if missing_roots > 0:
        print(f"Warning: {missing_roots} replies have no corresponding root messages. These will be skipped.")
        conversations = conversations[conversations['Message_root'].notnull()]

    return root_messages, conversations

def export_conversations(root_messages, conversations, unique_id_col='Message_ID', output_file='all_conversations.txt'):
    """
    Export all conversation threads into a single text file, separated by indicators.
    """
    # Open the output file
    with open(output_file, 'w', encoding='utf-8') as f:
        # Group by RootID and iterate through each conversation
        grouped = conversations.groupby('RootID')

        for root_id, group in grouped:
            try:
                # Retrieve root message details
                root_message = root_messages[root_messages[unique_id_col] == root_id].iloc[0]

                # Use 'Nickname' if available; otherwise, fallback to user ID
                root_nickname = root_message['Nickname'] if 'Nickname' in root_message and pd.notnull(root_message['Nickname']) else root_message.get('user_id', 'Unknown User')

                # Write conversation indicator
                f.write(f"--- Conversation Start ---\n\n")

                # Write Root Message
                f.write(f"Root Message:\n")
                f.write(f"Message: {root_message['Message']} \n")
                f.write(f"User: {root_nickname} \n")
                f.write(f"Created At: {root_message['create_at']} \n")

                # Include Edit and Delete Times if available
                if pd.notnull(root_message['edit_at']):
                    f.write(f"Edited At: {root_message['edit_at']} \n")
                if pd.notnull(root_message['delete_at']):
                    f.write(f"Deleted At: {root_message['delete_at']} \n")

                f.write("\nReplies:\n")

                # Sort replies by creation time
                group_sorted = group.sort_values('create_at_reply')

                for _, reply in group_sorted.iterrows():
                    # Use 'Nickname_reply' if available; otherwise, fallback to user ID
                    reply_nickname = reply['Nickname_reply'] if 'Nickname_reply' in reply and pd.notnull(reply['Nickname_reply']) else reply.get('user_id_reply', 'Unknown User')

                    f.write(f"-----------------------------\n")
                    f.write(f"Reply Message: {reply['Message_reply']} \n")
                    f.write(f"User: {reply_nickname} \n")
                    f.write(f"Created At: {reply['create_at_reply']} \n")

                    # Include Edit and Delete Times if available
                    if pd.notnull(reply['edit_at_reply']):
                        f.write(f"Edited At: {reply['edit_at_reply']} \n")
                    if pd.notnull(reply['delete_at_reply']):
                        f.write(f"Deleted At: {reply['delete_at_reply']} \n")

                # Write conversation end indicator
                f.write(f"\n=============================\n\n")

            except IndexError:
                print(f"No root message found for RootID: {root_id}")

    print(f"All conversations have been successfully exported to '{output_file}'.")

def main():
    # =====================
    # User Configuration
    # =====================

    # Specify your file path here
    file_path = 'mattermost_export.xlsx'  # Replace with your actual file path

    # Specify the unique identifier column name
    unique_id_col = 'Message_ID'  # Replace with your actual unique ID column name, e.g., 'id', 'Post_ID'

    # Specify the output file name
    output_file = 'all_conversations.txt'  # You can change this as desired

    # =====================
    # Script Execution
    # =====================

    # Load data
    df = load_data(file_path)

    # Process timestamps
    df = process_timestamps(df)

    # Filter out 'user joins the channel' messages
    df = filter_join_messages(df)

    # Process conversations
    root_messages, conversations = process_conversations(df, unique_id_col=unique_id_col)

    # Export conversations to a single file
    export_conversations(root_messages, conversations, unique_id_col=unique_id_col, output_file=output_file)

    print("Processing complete.")

if __name__ == "__main__":
    main()


import pandas as pd
import os
import sys

def load_data(file_path):
    """
    Load data from CSV or XLSX file based on file extension.
    """
    try:
        if file_path.endswith('.csv'):
            df = pd.read_csv(file_path)
        elif file_path.endswith(('.xlsx', '.xls')):
            df = pd.read_excel(file_path, engine='openpyxl')  # Specify engine if necessary
        else:
            raise ValueError("Unsupported file format. Please provide a CSV or XLSX file.")
        return df
    except Exception as e:
        print(f"Error loading file: {e}")
        sys.exit(1)

def process_timestamps(df):
    """
    Convert timestamp columns from Unix epoch to datetime.
    Handle missing values for Edit_Time and Delete_Time.
    """
    # Define timestamp columns and their respective conversions
    timestamp_columns = {
        'Update_Time': 'create_at',
        'Edit_Time': 'edit_at',
        'Delete_Time': 'delete_at'
    }

    for original, new in timestamp_columns.items():
        if original in df.columns:
            # Convert timestamps; assuming they are in milliseconds
            df[new] = pd.to_datetime(df[original], unit='ms', errors='coerce')
        else:
            print(f"Warning: '{original}' column not found in the data.")
            df[new] = pd.NaT  # Not a Time for missing columns

    return df

def process_conversations(df):
    """
    Identify root messages and replies based on thread information.
    """
    # Assuming 'root_id' and 'parent_id' columns exist. Adjust if your columns have different names.
    # If your thread information is stored differently, modify accordingly.

    # Check if 'root_id' column exists
    if 'root_id' not in df.columns:
        print("Error: 'root_id' column not found. Cannot process threaded conversations.")
        sys.exit(1)

    # Identify root messages (original posts) and replies
    root_messages = df[df['root_id'].isnull()].copy()
    replies = df[df['root_id'].notnull()].copy()

    # Merge replies with their root messages
    conversations = replies.merge(
        root_messages,
        left_on='root_id',
        right_on='id',
        suffixes=('_reply', '_root'),
        how='left'
    )

    # Handle cases where root messages might not be found
    missing_roots = conversations['message_root'].isnull().sum()
    if missing_roots > 0:
        print(f"Warning: {missing_roots} replies have no corresponding root messages.")
        conversations = conversations[conversations['message_root'].notnull()]

    return root_messages, conversations

def export_conversations(root_messages, conversations):
    """
    Export each conversation thread to a separate text file, including edit and delete information.
    """
    # Create directory for conversations
    os.makedirs('conversations', exist_ok=True)

    # Group by root_id and export
    grouped = conversations.groupby('root_id')

    for root_id, group in grouped:
        try:
            root_message = root_messages[root_messages['id'] == root_id].iloc[0]
            # Define filename, optionally include a portion of the root message for readability
            safe_root_id = str(root_id).replace('/', '_').replace('\\', '_')
            filename = f"conversations/{safe_root_id}.txt"
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(f"--- Conversation Thread ---\n\n")
                f.write(f"Root Message:\n")
                f.write(f"Message: {root_message['message']} \n")
                f.write(f"User: {root_message['user_id']} \n")
                f.write(f"Created At: {root_message['create_at']} \n")

                # If the root message was edited or deleted
                if pd.notnull(root_message['edit_at']):
                    f.write(f"Edited At: {root_message['edit_at']} \n")
                if pd.notnull(root_message['delete_at']):
                    f.write(f"Deleted At: {root_message['delete_at']} \n")

                f.write("\nReplies:\n")
                for _, reply in group.iterrows():
                    f.write(f"-----------------------------\n")
                    f.write(f"Reply Message: {reply['message_reply']} \n")
                    f.write(f"User: {reply['user_id_reply']} \n")
                    f.write(f"Created At: {reply['create_at_reply']} \n")

                    # Include edit and delete times if available
                    if pd.notnull(reply['edit_at_reply']):
                        f.write(f"Edited At: {reply['edit_at_reply']} \n")
                    if pd.notnull(reply['delete_at_reply']):
                        f.write(f"Deleted At: {reply['delete_at_reply']} \n")
                f.write("\n=============================\n\n")
        except IndexError:
            print(f"No root message found for root_id: {root_id}")

def main():
    # Specify your file path here
    file_path = 'mattermost_export.xlsx'  # Replace with your actual file path

    # Load data
    df = load_data(file_path)

    # Process timestamps
    df = process_timestamps(df)

    # Process conversations
    root_messages, conversations = process_conversations(df)

    # Export conversations
    export_conversations(root_messages, conversations)

    print("Conversations have been successfully exported to the 'conversations' directory.")

if __name__ == "__main__":
    main()


Title: As a user, I want the EDT Service to be an easily installable package so that I can quickly set up the service without manually installing dependencies.

Given:

The EDT Service has been refactored into a Flask web application with dashboard, labeling, and scheduling features.
The service includes multiple dependencies and requires a virtual environment setup for running the application.
When:

A user wants to install the EDT Service on their machine.
The user runs pip install edt-service or a similar installation command.
Then:

The service should install all necessary dependencies automatically as defined in setup.py.
The installation should create a command-line entry point (e.g., edt-service) for starting the Flask web application.
The user should be able to run the EDT service without manually setting up the virtual environment or installing dependencies.
Any required environment configurations should be handled during installation, and the user should be guided through setting up default values (if needed).

Title: As a user, I want the refactored EDT Service to run in the Apps space using a service ID so that it operates in a standardized and secure environment.

Given:

The current EDT Service is running in the user's home space.
The refactored EDT Service is now a Flask web application with additional features like a dashboard, labeling, and scheduling.
The target environment for the service is the Apps space, which requires the service to be run with a specific service ID.
When:

The user or administrator deploys the refactored EDT Service.
The service is executed with a valid service ID within the Apps space.
Then:

The service should automatically launch in the designated Apps space, ensuring it is not dependent on the user’s home space.
The service should use the provided service ID for all necessary operations, such as file permissions, resource allocation, and logging.
The user should not be required to modify environment configurations manually to migrate from the home space to the Apps space.
The service should adhere to security and operational policies tied to the Apps space and the service ID, ensuring secure execution and access control.


Title: As a user, I want the EDT Service to be managed by SystemD so that it can automatically restart and survive server reboots, ensuring continuous operation.

Given:

The current EDT Service is run using a shell script that automatically restarts if the process is killed.
The current setup does not survive server reboots, requiring manual intervention to restart the service.
The refactored EDT Service should run as a Flask web application with features like scheduling, labeling, and a dashboard.
When:

The system is rebooted, or the EDT Service is manually killed.
The EDT Service is configured to run as a SystemD service.
Then:

SystemD should automatically restart the EDT Service upon process failure or server reboot.
The EDT Service should be set to run as a background service, with systemctl start, stop, status, and restart commands available for manual management.
The service should have a SystemD unit file configured with options like Restart=always and After=network.target to ensure it starts after system boot and networking is available.
Logs for the EDT Service should be accessible through journalctl for easy monitoring and troubleshooting.
The service should remain resilient across server restarts without requiring manual intervention from the user.

from setuptools import setup, find_packages

setup(
    name='my_flask_app',
    version='1.0.0',
    description='My Flask App',
    long_description=open('README.md').read(),
    long_description_content_type='text/markdown',
    author='Your Name',
    author_email='youremail@example.com',
    packages=find_packages(),
    include_package_data=True,  # Includes data files specified in MANIFEST.in
    install_requires=[
        'Flask',
        'gunicorn',
        # other dependencies
    ],
    entry_points={
        'console_scripts': [
            'run-flask-app=my_flask_app.app:main',  # Adds the 'run-flask-app' command to the user's PATH
        ],
    },
)


include my_flask_app/static/*
include my_flask_app/templates/*
include README.md


sudo nano /etc/systemd/system/flask_app.service

[Unit]
Description=Flask App with Virtual Environment

[Service]
ExecStart=/path/to/your/venv/bin/python /path/to/your/app.py
WorkingDirectory=/path/to/your/app
Environment="PATH=/path/to/your/venv/bin"
Restart=always

[Install]
WantedBy=multi-user.target


sudo systemctl daemon-reload

sudo systemctl start flask_app

sudo systemctl enable flask_app


# model_downloader.py

import os
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

def set_proxy():
    # Set these values according to your proxy configuration
    proxy = "http://username:password@proxy_ip:proxy_port"
    os.environ['HTTP_PROXY'] = proxy
    os.environ['HTTPS_PROXY'] = proxy

def download_model(model_name="facebook/bart-large-cnn"):
    print("Downloading model...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    print("Model downloaded successfully!")

    # Save the model and tokenizer
    output_dir = "./bart_large_model"
    tokenizer.save_pretrained(output_dir)
    model.save_pretrained(output_dir)
    print(f"Model and tokenizer saved to {output_dir}")

if __name__ == "__main__":
    set_proxy()  # Comment this line if you're not using a proxy
    download_model()


# email_summarizer.py

import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from email import policy
from email.parser import BytesParser
import os

# Set up device (CPU/GPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

def load_model(model_dir="./bart_large_model"):
    print("Loading model...")
    tokenizer = AutoTokenizer.from_pretrained(model_dir)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_dir).to(device)
    print("Model loaded successfully!")
    return tokenizer, model

def extract_email_content(file_path):
    with open(file_path, 'rb') as fp:
        msg = BytesParser(policy=policy.default).parse(fp)
    return msg.get_body(preferencelist=('plain', 'html')).get_content()

def summarize_email(email_content, tokenizer, model, max_length=1024, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True):
    inputs = tokenizer("summarize: " + email_content, return_tensors="pt", max_length=max_length, truncation=True)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    summary_ids = model.generate(
        inputs["input_ids"],
        max_length=150,
        min_length=min_length,
        length_penalty=length_penalty,
        num_beams=num_beams,
        early_stopping=early_stopping
    )

    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

def main():
    tokenizer, model = load_model()

    # Example usage
    email_file_path = "path/to/your/email/file.eml"

    if not os.path.exists(email_file_path):
        print(f"Error: File not found at {email_file_path}")
        return

    email_content = extract_email_content(email_file_path)
    summary = summarize_email(email_content, tokenizer, model)

    print("Original Email Content:")
    print(email_content[:500] + "..." if len(email_content) > 500 else email_content)
    print("\nEmail Summary:")
    print(summary)

if __name__ == "__main__":
    main()

pip install torch transformers email-parser

python model_downloader.py


from flask import Flask
from exchangelib import Credentials, Account, DELEGATE, Message
from exchangelib.subscription import Subscription
import threading
import logging

# Initialize Flask app and logging
app = Flask(__name__)
logging.basicConfig(level=logging.INFO)

# Exchange credentials and account setup
EMAIL = 'youremail@domain.com'
PASSWORD = 'yourpassword'

credentials = Credentials(EMAIL, PASSWORD)
subscription_thread = None
subscription_active = False  # Flag to track subscription state

def process_notification(items):
    """Callback function to process notifications."""
    for item in items:
        if isinstance(item, Message):
            logging.info(f"New message from {item.sender.email_address}: {item.subject}")

def subscribe_to_streaming():
    """Sets up the streaming subscription."""
    global subscription_active
    try:
        account = Account(EMAIL, credentials=credentials, autodiscover=True, access_type=DELEGATE)
        folder = account.inbox

        with folder.subscribe_to_streaming(on_notification=process_notification) as subscription:
            subscription_active = True
            logging.info("Subscribed to streaming notifications.")
            subscription.connection.run_forever()

    except Exception as e:
        logging.error(f"Error in subscription: {e}")
        subscription_active = False  # Mark subscription as failed

def start_subscription_in_thread():
    """Starts the subscription in a separate thread."""
    global subscription_thread
    subscription_thread = threading.Thread(target=subscribe_to_streaming)
    subscription_thread.daemon = True  # Daemon thread to stop with Flask app
    subscription_thread.start()
    logging.info("Streaming subscription thread started.")

@app.route('/')
def index():
    return "Flask App with Streaming Subscription"

@app.before_first_request
def initialize_subscription():
    """Starts the streaming subscription when the Flask app is initialized."""
    logging.info("Initializing streaming subscription.")
    start_subscription_in_thread()

if __name__ == "__main__":
    app.run(debug=True)


import os
import xml.etree.ElementTree as ET
import shutil
from typing import List, Tuple, Dict

# Mapping of connector namespaces to their corresponding Maven groupId and artifactId
CONNECTOR_DEPENDENCY_MAP = {
    'http://www.mulesoft.org/schema/mule/http': ('org.mule.connectors', 'mule-http-connector'),
    'http://www.mulesoft.org/schema/mule/db': ('org.mule.connectors', 'mule-db-connector'),
    'http://www.mulesoft.org/schema/mule/file': ('org.mule.connectors', 'mule-file-connector'),
    'http://www.mulesoft.org/schema/mule/ftp': ('org.mule.connectors', 'mule-ftp-connector'),
    'http://www.mulesoft.org/schema/mule/jms': ('org.mule.connectors', 'mule-jms-connector'),
    'http://www.mulesoft.org/schema/mule/vm': ('org.mule.connectors', 'mule-vm-connector'),
    # Add more mappings as needed
}

def parse_domain_resources(domain_path: str) -> Tuple[Dict[str, List[ET.Element]], set]:
    resources = {
        'connectors': [],
        'global_configs': [],
        'error_handlers': [],
        'security_configs': [],
        'schedulers': [],
        'transformers': [],
        'spring_beans': [],
        'other': []
    }
    used_namespaces = set()

    config_path = os.path.join(domain_path, "src", "main", "mule", "mule-domain-config.xml")
    tree = ET.parse(config_path)
    root = tree.getroot()

    # Collect used namespaces
    for key, value in root.attrib.items():
        if key.startswith('xmlns:'):
            used_namespaces.add(value)

    for child in root:
        if '}' in child.tag:
            namespace = child.tag.split('}')[0][1:]  # Extract namespace
            used_namespaces.add(namespace)
            tag = child.tag.split('}')[1]
            if any(x in tag for x in ['config', 'configuration']):
                resources['connectors'].append(child)
            elif tag == 'global-property':
                resources['global_configs'].append(child)
            elif tag == 'error-handler':
                resources['error_handlers'].append(child)
            elif 'security' in tag or 'oauth' in tag:
                resources['security_configs'].append(child)
            elif 'scheduler' in tag:
                resources['schedulers'].append(child)
            elif 'transformer' in tag:
                resources['transformers'].append(child)
            elif tag == 'beans':
                resources['spring_beans'].append(child)
            else:
                resources['other'].append(child)

    return resources, used_namespaces

def update_project_config(project_path: str, resources: Dict[str, List[ET.Element]], used_namespaces: set):
    config_path = os.path.join(project_path, "src", "main", "mule", "mule-config.xml")
    tree = ET.parse(config_path)
    root = tree.getroot()

    # Add all resources to the project config
    for resource_list in resources.values():
        for resource in resource_list:
            root.append(resource)

    # Update namespaces
    for namespace in used_namespaces:
        for prefix, uri in root.attrib.items():
            if uri == namespace:
                break
        else:
            # If namespace not found, add it with a generated prefix
            i = 1
            while f'xmlns:ns{i}' in root.attrib:
                i += 1
            root.set(f'xmlns:ns{i}', namespace)

    tree.write(config_path, encoding="UTF-8", xml_declaration=True)

def update_project_pom(project_path: str, domain_path: str, used_namespaces: set):
    domain_pom_path = os.path.join(domain_path, "pom.xml")
    project_pom_path = os.path.join(project_path, "pom.xml")

    domain_tree = ET.parse(domain_pom_path)
    domain_root = domain_tree.getroot()

    project_tree = ET.parse(project_pom_path)
    project_root = project_tree.getroot()

    # Transfer dependencies
    domain_deps = domain_root.find("./{*}dependencies")
    if domain_deps is not None:
        project_deps = project_root.find("./{*}dependencies")
        if project_deps is None:
            project_deps = ET.SubElement(project_root, "dependencies")

        for dep in domain_deps:
            if dep.find("./{*}classifier") is None or dep.find("./{*}classifier").text != "mule-domain":
                project_deps.append(dep)

    # Add connector dependencies based on used namespaces
    for namespace in used_namespaces:
        if namespace in CONNECTOR_DEPENDENCY_MAP:
            group_id, artifact_id = CONNECTOR_DEPENDENCY_MAP[namespace]
            new_dep = ET.SubElement(project_deps, "dependency")
            ET.SubElement(new_dep, "groupId").text = group_id
            ET.SubElement(new_dep, "artifactId").text = artifact_id
            # You might want to specify a version or use a property for versioning
            # ET.SubElement(new_dep, "version").text = "${mule.version}"

    # Transfer properties
    domain_props = domain_root.find("./{*}properties")
    if domain_props is not None:
        project_props = project_root.find("./{*}properties")
        if project_props is None:
            project_props = ET.SubElement(project_root, "properties")

        for prop in domain_props:
            if not project_props.find(f".//{prop.tag}"):
                project_props.append(prop)

    # Remove domain dependency
    for dep in project_root.findall(".//{*}dependency"):
        artifact_id = dep.find("./{*}artifactId")
        classifier = dep.find("./{*}classifier")
        if (artifact_id is not None and artifact_id.text == "my-domain-project" and
            classifier is not None and classifier.text == "mule-domain"):
            project_deps.remove(dep)

    project_tree.write(project_pom_path, encoding="UTF-8", xml_declaration=True)

def migrate_project(domain_path: str, project_path: str):
    print(f"Migrating project: {os.path.basename(project_path)}")

    # Parse domain resources and collect used namespaces
    resources, used_namespaces = parse_domain_resources(domain_path)

    # Update project configuration
    update_project_config(project_path, resources, used_namespaces)

    # Update project pom
    update_project_pom(project_path, domain_path, used_namespaces)

    # Copy any additional resources
    domain_resources_path = os.path.join(domain_path, "src", "main", "resources")
    project_resources_path = os.path.join(project_path, "src", "main", "resources")
    if os.path.exists(domain_resources_path):
        for item in os.listdir(domain_resources_path):
            s = os.path.join(domain_resources_path, item)
            d = os.path.join(project_resources_path, item)
            if os.path.isfile(s):
                shutil.copy2(s, d)

    print(f"Migration completed for project: {os.path.basename(project_path)}")

def main():
    domain_path = "/path/to/domain/project"
    projects = [
        "/path/to/project1",
        "/path/to/project2",
        # Add more project paths as needed
    ]

    for project_path in projects:
        migrate_project(domain_path, project_path)

if __name__ == "__main__":
    main()


module.exports = {
  content: [
    "./src/**/*.{js,jsx,ts,tsx}",
  ],
  theme: {
    extend: {},
  },
  plugins: [],
}

import base64
from email import policy
from email.parser import BytesParser
from io import BytesIO

# Function to handle the conversion of an image to a data URI
def get_image_data_uri(part):
    image_content = part.get_payload(decode=True)
    image_type = part.get_content_subtype()
    base64_image = base64.b64encode(image_content).decode('utf-8')
    return f"data:image/{image_type};base64,{base64_image}"

# Function to convert text/plain to HTML (if needed)
def convert_plain_text_to_html(plain_text):
    html_content = "<html><body><pre style='font-family: monospace;'>{}</pre></body></html>"
    return html_content.format(plain_text)

# Function to convert email to HTML with correctly embedded images
def email_to_html_with_images_or_text(msg):
    html_parts = []
    image_cid_map = {}

    # First pass: Extract images and build a map of cid references to data URIs
    for part in msg.walk():
        content_type = part.get_content_type()
        if "image" in content_type:
            # Get the content ID (cid) and strip surrounding angle brackets if present
            cid = part.get("Content-ID")
            if cid:
                cid = cid.strip('<>')
                image_cid_map[cid] = get_image_data_uri(part)

    # Second pass: Build the HTML and replace cid references with the corresponding data URIs
    for part in msg.walk():
        content_type = part.get_content_type()
        if content_type == "text/html":
            html_content = part.get_payload(decode=True).decode(part.get_content_charset())
            # Replace all cid references with data URIs
            for cid, data_uri in image_cid_map.items():
                html_content = html_content.replace(f'cid:{cid}', data_uri)
            html_parts.append(html_content)

    return ''.join(html_parts)

# Simulate reading and processing an email (this is where you'd provide the actual email)
# email_content = b"Your raw MIME email content here"
# msg = BytesParser(policy=


from io import BytesIO
from email import policy
from email.parser import BytesParser

# Simulating reading an email (In real scenario, load from file or other source)
email_content = b"""Your raw MIME email content here"""

# Parsing the email
msg = BytesParser(policy=policy.default).parsebytes(email_content)

# Function to convert text/plain to HTML
def convert_plain_text_to_html(plain_text):
    html_content = "<html><body><pre style='font-family: monospace;'>{}</pre></body></html>"
    return html_content.format(plain_text)

# Function to convert email to HTML with embedded images (or plain text handling)
def email_to_html_with_images_or_text(msg):
    html_parts = []
    for part in msg.walk():
        content_type = part.get_content_type()
        if content_type == "text/html":
            html_parts.append(part.get_payload(decode=True).decode(part.get_content_charset()))
        elif content_type == "text/plain":
            plain_text = part.get_payload(decode=True).decode(part.get_content_charset())
            html_parts.append(convert_plain_text_to_html(plain_text))
        elif "image" in content_type:
            image_data_uri = get_image_data_uri(part)
            html_parts.append(f'<img src="{image_data_uri}"/>')

    return ''.join(html_parts)

# Convert and store HTML
html_output = email_to_html_with_images_or_text(msg)

# For demonstration, we're using BytesIO as in-memory file
html_file = BytesIO(html_output.encode('utf-8'))
html_file.seek(0)  # Rewind to the beginning of the file
print(html_file.read().decode('utf-8'))  # Print the HTML content for demonstration


Monitor and Restart:
Given the Flask application is not running
When the monitoring system checks the application status
Then the system should restart the application using nohup
Log Management:
Given the Flask application is running and logging output
When a new day starts
Then the system should create a new log file for that day
Given log files older than 7 days exist
When the system performs its daily check
Then it should delete these old log files
Persistence:
Given the server has just booted up
When the operating system has finished loading
Then the monitoring system should start automatically
Usability:
Given a new developer wants to use the monitoring system
When they read the provided instructions
Then they should be able to set up and configure the system without additional assistance
Testing:
Given the Flask application is running and being monitored
When the application is manually terminated to simulate a crash
Then the monitoring system should detect this and restart the application within one minute

Acceptance Criteria in Given-When-Then Format
Given I am subscribed to the mailbox,
When a new email arrives,
Then the system should immediately trigger the callback function.

Given a new email has triggered the callback function,
When the email is processed,
Then it should be processed within seconds of its arrival without manual intervention.

Given the email notification system is operational,
When new emails arrive,
Then all new emails should be detected and processed accurately, with no emails missed or processed more than once.

Given the system is set up for testing,
When rapid successive emails are simulated to arrive,
Then the system should handle them without crashing or missing any emails.

Given typical system usage,
When the system processes incoming emails,
Then it should remain stable and perform within the expected parameters.


Acceptance Criteria in Given-When-Then Format
Detect and Convert Images:

Given an email with attached images,
When the email is processed,
Then the tool should recognize all images and convert each image into a data URI format.
Embed Images in HTML:

Given images have been converted to data URIs,
When the email is converted to HTML,
Then all images must appear as <img> tags at their correct positions within the content, and the HTML should visually match the original email including all text and images.
Content Accuracy:

Given an email is being converted to HTML,
When the conversion process is complete,
Then the HTML should maintain the original email's layout and formatting without losing any parts of the email such as text or images.
Handle Various Emails:

Given emails with multiple images,
When these emails are processed,
Then the tool should successfully convert all images and the text content.
And Given emails without any images,
When these emails are processed,
Then the tool should ensure the text content is still properly converted to HTML.
Testing:

Given the system is set for testing,
When emails are processed through the system,
Then tests should confirm that images are correctly detected and embedded.
And the HTML output should accurately reflect the content and structure of the original email.

Want an automated system to monitor and restart our Flask application
So that the application remains available even after server restarts or unexpected crashes


Our MMBot and JiraBot on a development server. Currently, the application is started manually using nohup python server.py.
However, this approach doesn't ensure the application's continuous availability, especially after server restarts or if the application crashes unexpectedly.
We need a robust solution that automatically monitors the application's status and restarts it when necessary, while also managing log files efficiently


Monitor and Restart:
Automatically check if the Flask app is running every minute
Restart the app using nohup if it's not running


Log Management:
Append all outputs to a log file
Rotate log files daily


Persistence:
The monitoring system starts automatically on server boot
Continues running until manually stopped


Usability:
Provide simple instructions for setup and use
Allow easy configuration of app name and log directory


Testing:
Demonstrate successful app restart after a simulated crash


Title: Enhance Email-to-HTML Conversion to Include Embedded Images

Description
As a user, I want the email-to-HTML conversion process to include images embedded within the email so that the resulting
HTML is a complete and accurate representation of the original email content.

Currently, our tool converts email text to HTML but does not handle images, resulting in missing visual information in the converted files.
The enhancement will modify our existing script to detect and embed images as data URIs directly in the HTML,
ensuring that the output is self-contained and visually consistent with the original email.

Acceptance Criteria

Detect and Convert Images:
The tool should recognize all images attached in the email.
Each image should be converted into a data URI format.

Embed Images in HTML:
Images must appear in the HTML as <img> tags at the correct points in the content.
The HTML should look like the original email, with all text and images in place.

Content Accuracy:
The converted HTML should maintain the original email's layout and formatting.
Ensure that no parts of the email, such as text or images, are lost in conversion.

Handle Various Emails:
The tool should successfully convert emails that have multiple images.
It should also handle emails without any images, ensuring the text content is still properly converted to HTML.

Testing:
Ensure the HTML output accurately reflects the content and structure of the original email.


Title: Implement Subscription-Based Email Notification System

Description
As a user, I want the system to notify me immediately when a new email arrives so that I can process emails in real-time, reducing the response time and system load compared to the current long polling method.

Acceptance Criteria

Subscription Setup:
The system must subscribe to new email notifications.
New emails should trigger a callback function.

Real-Time Processing:
The system should process emails within seconds of their arrival.
Ensure that each new email triggers the designated processing logic without user intervention.

Accuracy:
All new emails must be detected and processed.
No emails should be missed or processed more than once.

Testing:
Testing should confirm that the system handles rapid successive email arrivals.


import base64
from email import policy
from email.parser import BytesParser
from io import BytesIO

# Simulating reading an email with images (In real scenario, load from file or other source)
email_content = b"""Your raw MIME email content here"""

# Parsing the email
msg = BytesParser(policy=policy.default).parsebytes(email_content)

# Function to handle the conversion of an image to a data URI
def get_image_data_uri(part):
    image_content = part.get_payload(decode=True)
    image_type = part.get_content_subtype()
    base64_image = base64.b64encode(image_content).decode('utf-8')
    return f"data:image/{image_type};base64,{base64_image}"

# Function to convert email to HTML with embedded images
def email_to_html_with_images(msg):
    html_parts = []
    for part in msg.walk():
        content_type = part.get_content_type()
        if content_type == "text/html":
            html_parts.append(part.get_payload(decode=True).decode(part.get_content_charset()))
        elif "image" in content_type:
            image_data_uri = get_image_data_uri(part)
            html_parts.append(f'<img src="{image_data_uri}"/>')

    return ''.join(html_parts)

# Convert and store HTML
html_output = email_to_html_with_images(msg)

# For demonstration, we're using BytesIO as in-memory file
html_file = BytesIO(html_output.encode('utf-8'))
html_file.seek(0)  # Rewind to the beginning of the file
print(html_file.read().decode('utf-8'))  # Print the HTML content for demonstration


#!/bin/bash

APP_NAME="app.py"
LOG_DIR="/path/to/your/logs"
LOG_FILE="$LOG_DIR/app_$(date +%Y%m%d).log"

# Create log directory if it doesn't exist
mkdir -p "$LOG_DIR"

# Function to get current log file name
get_log_file() {
    echo "$LOG_DIR/app_$(date +%Y%m%d).log"
}

while true
do
    # Update log file name
    LOG_FILE=$(get_log_file)

    if ! pgrep -f "$APP_NAME" > /dev/null
    then
        echo "Starting $APP_NAME at $(date)" >> "$LOG_FILE"
        nohup python "$APP_NAME" >> "$LOG_FILE" 2>&1 &
    fi

    # Optional: Delete logs older than 7 days
    find "$LOG_DIR" -name "app_*.log" -type f -mtime +7 -delete

    sleep 60
done


1. Code Carbon Inefficiency Scanner

Problem Statement: Develop a tool that scans existing codebases to identify carbon inefficiencies. This scanner should be capable of analyzing code structure, detecting inefficient algorithms, and flagging practices that lead to excessive energy consumption.
Practicality: This is a feasible project, especially with a focus on integrating existing static analysis tools and expanding them to include energy efficiency metrics. The challenge will be in defining the criteria for "carbon inefficiency" and ensuring the scanner provides actionable feedback.
2. IDE Plugin for Carbon Efficiency

Problem Statement: Create an interactive framework that integrates into popular IDEs (like VSCode or IntelliJ) as a plugin. The plugin should highlight carbon inefficiencies in real-time as developers write code, offering suggestions for more efficient alternatives.
Practicality: Building an IDE plugin is achievable, but the effectiveness depends on the robustness of the underlying analysis engine. Leveraging existing tools or APIs like CodeCarbon could help expedite development, but significant customization will be needed to provide real-time feedback.
3. Carbon Efficiency Dashboard

Problem Statement: Design a dashboard that wraps around the CodeCarbon tool, providing visual insights into the carbon impact of code during development, testing, and deployment stages. The dashboard should offer historical data, trends, and actionable insights.
Practicality: This is practical and could be implemented by extending CodeCarbon’s existing capabilities. The key challenge will be ensuring the dashboard is intuitive and provides meaningful, actionable data for developers and managers.
4. Process-Intensive Task Optimization

Problem Statement: Identify process-intensive tasks within the software development lifecycle (e.g., build time, merge time, test execution time) and develop solutions to minimize their carbon footprint. This could involve optimizing build pipelines, reducing redundant testing, or parallelizing tasks more effectively.
Practicality: This is highly practical and aligns well with DevOps practices. The main challenge will be in measuring the carbon footprint of these processes accurately and finding optimization techniques that are universally applicable.
5. Code Optimizer Tool

Problem Statement: Create a tool that automatically converts inefficient code into a more carbon-efficient version. The tool should be capable of refactoring code, optimizing algorithms, and suggesting alternative approaches to reduce energy consumption.
Practicality: This is an ambitious but potentially groundbreaking project. Developing such a tool would require a deep understanding of different programming languages and the ability to analyze and refactor code without introducing bugs. Starting with a more limited scope, such as optimizing specific patterns, could make this more manageable.
6. Open Frameworks for Code Optimization

Problem Statement: Develop or identify frameworks outside the traditional scope that can help optimize code for energy efficiency. This could involve leveraging AI/ML for code analysis, exploring new paradigms for energy-efficient software design, or integrating sustainability metrics into existing frameworks.
Practicality: This is a broad and exploratory problem statement. The practical approach would be to focus on a specific area within this framework and develop a proof of concept. Collaborating with researchers or open-source communities could provide valuable insights and accelerate progress.


Objective:
The Green Computing Hackathon Challenge is designed to spark curiosity and innovation among developers by focusing on energy-efficient programming practices.
Participants will engage in various coding tasks aimed at optimizing software for reduced energy consumption, all within the confines of BAND laptops.
This hackathon encourages hands-on learning and application of green computing concepts without requiring additional infrastructure.

Themes:

Theme 1: Energy Efficiency Comparisons

Comparing Energy Efficiency: REST vs gRPC
Develop a comprehensive benchmarking suite to compare the energy efficiency of gRPC-based Protocol Buffers applications with traditional REST/JSON applications. The goal is to quantify the energy savings and performance improvements that can be achieved by using gRPC over REST/JSON for different types of operations and data loads.

Comparing Energy Efficiency: Native Compiled vs. VM Compiled vs. Interpreted Languages
Develop a comprehensive benchmarking suite to compare the energy efficiency of native compiled languages (e.g., Rust, C++), compiled languages running on a virtual machine (e.g., Java, C#), and interpreted languages (e.g., Python, Ruby, etc.). The goal is to measure and analyze the energy consumption, performance, and resource utilization of various computational tasks implemented in these different types of languages.

Theme 2: Green Coding Practices
Guide for Green Coding Practices:
Develop an interactive guide that educates users on green coding practices. The guide should provide practical tips, real-time feedback, and educational resources to promote energy-efficient coding and software operations.

Theme 3: Code Optimization
Optimizing Legacy Code for Energy Efficiency:
Take a piece of legacy code (available in various languages) and optimize it for energy efficiency. Participants should measure the energy consumption before and after optimization and provide detailed reports on their improvements.


Theme 4: Algorithm and Data Structure Efficiency
Energy-Efficient Algorithms
Implement common algorithms (e.g., sorting, searching, graph traversal) in the most energy-efficient way possible. Compare different implementations to find the most energy-efficient solution.

Sustainable Data Structures
Analyze and implement different data structures to determine which are the most energy-efficient for various operations (e.g., insertions, deletions, searches). Provide a comparative study.

Goal:
By the end of the hackathon, participants will have developed a deeper understanding of green computing practices, learned to optimize code for energy efficiency, and created solutions that contribute to a more sustainable future. The focus is on practical, hands-on experience that can be immediately applied to real-world software development scenarios.

Impact:
This hackathon aims to promote environmentally conscious coding practices, reduce the carbon footprint of software applications, and encourage a culture of sustainability within the developer community.