import redis
import time

# Connect to your Redis node
redis_host = "localhost"  # Change this to your Redis node's IP or hostname
redis_port = 6379  # Adjust if your Redis server uses a different port
r = redis.Redis(host=redis_host, port=redis_port, decode_responses=True)

def load_test_redis(total_operations=10000):
    write_times = []

    for i in range(total_operations):
        start_time = time.time()
        # Performing a simple write operation. Adjust the key-value pair as needed.
        r.set(f"key-{i}", f"value-{i}")
        end_time = time.time()

        # Calculating the time taken for the write operation
        operation_time = end_time - start_time
        write_times.append(operation_time)

        if i % 100 == 0:  # Just to keep track of progress without flooding the console
            print(f"Completed {i} operations")

    # Let's calculate some basic performance metrics
    avg_time = sum(write_times) / len(write_times)
    max_time = max(write_times)
    min_time = min(write_times)

    print(f"Average write time: {avg_time} seconds")
    print(f"Maximum write time: {max_time} seconds")
    print(f"Minimum write time: {min_time} seconds")

# Kick off the test
load_test_redis()


https://www.elastic.co/guide/en/observability/current/ci-cd-observability.html

https://github.com/open-telemetry/oteps/pull/223



items.find({
    "repo": "my-example-repo",
    "created": {
        "$gte": "2024-02-24T00:00:00.000Z",
        "$lte": "2024-02-25T00:00:00.000Z"
    }
})

pip install "apache-airflow==YOUR_AIRFLOW_VERSION" --constraint path/to/your/constraints-file.txt

airflow db init

airflow users create \
    --username admin \
    --firstname YOUR_FIRST_NAME \
    --lastname YOUR_LAST_NAME \
    --role Admin \
    --email YOUR_EMAIL@example.com

airflow webserver --port 8080

airflow scheduler

from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta

def my_first_function():
    print("Hello from the first function!")

def my_second_function():
    print("Hello from the second function!")

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2021, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG('simple_dag',
          default_args=default_args,
          description='A simple DAG',
          schedule_interval=timedelta(days=1),
          )

t1 = PythonOperator(
    task_id='first_function',
    python_callable=my_first_function,
    dag=dag,
)

t2 = PythonOperator(
    task_id='second_function',
    python_callable=my_second_function,
    dag=dag,
)

t1 >> t2






 propose transitioning our PySpark ETL jobs from VMs to an Airflow-based container solution with serverless PySpark. This move offers improved scalability, cost efficiency, and a more streamlined workflow, positioning us well for future data processing challenges."

 flow in this context acts as a powerful orchestrator, managing and scheduling our ETL jobs with greater efficiency and reliability. It provides a user-friendly interface for monitoring workflows, ensuring smoother, more transparent operations.