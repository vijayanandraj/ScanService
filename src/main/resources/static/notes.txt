
Commiting changes from ChatGPT text. This can be a game changer for work.

import java.util.HashSet;
import java.util.Set;
import java.util.List;
import java.util.ArrayList;
import java.util.Map;
import java.util.stream.Collectors;

@Service
public class EtlService {

    @Autowired
    private NamedParameterJdbcTemplate namedParameterJdbcTemplate;

    // This set should be populated with the ait_numbers to exclude.
    private Set<Double> exclusionSet = new HashSet<>();

    public void loadExclusionSet(List<Double> aitNumbersToExclude) {
        exclusionSet.addAll(aitNumbersToExclude);
    }

    public void performEtl() {
        int offset = 0;
        List<Map<String, Object>> batch;

        do {
            batch = extractBatch(offset, BATCH_SIZE);

            List<Map<String, Object>> filteredBatch = batch.stream()
                    .filter(row -> !exclusionSet.contains(((Number) row.get("ait_number")).doubleValue()))
                    .collect(Collectors.toList());

            // Continue with transform and load operations
            List<Map<String, Object>> transformedBatch = transformBatch(filteredBatch);
            loadBatch(transformedBatch);

            offset += BATCH_SIZE;
        } while (!batch.isEmpty());
    }

    // Extract, transform, and load methods would be defined here as before
}


    private Integer convertToBit(Object value) {
        if ("yes".equalsIgnoreCase((String) value)) {
            return 1;
        } else if ("no".equalsIgnoreCase((String) value)) {
            return 0;
        }
        // Add additional checks and conversions as necessary
        return null; // or a default value
    }

  private static class LowerCaseMap<V> extends HashMap<String, V> {
    @Override
    public V put(String key, V value) {
      return super.put(key.toLowerCase(), value);
    }

    @Override
    public V get(Object key) {
      if (key instanceof String) {
        return super.get(((String) key).toLowerCase());
      }
      return super.get(key);
    }

    @Override
    public boolean containsKey(Object key) {
      if (key instanceof String) {
        return super.containsKey(((String) key).toLowerCase());
      }
      return super.containsKey(key);
    }
    // ... other overrides as necessary
  }

import csv

# Function to add double quotes to each item in a row
def add_quotes(row):
    return ['"' + str(item) + '"' for item in row]

def process_csv(input_file_path, output_file_path):
    with open(input_file_path, 'r', newline='') as csvfile_in:
        reader = csv.reader(csvfile_in)

        with open(output_file_path, 'w', newline='') as csvfile_out:
            writer = csv.writer(csvfile_out, quoting=csv.QUOTE_NONE, escapechar='\\')

            for row in reader:
                writer.writerow(add_quotes(row))

# Replace 'input.csv' and 'output.csv' with your file paths
process_csv('input.csv', 'output.csv')


import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.jdbc.core.namedparam.NamedParameterJdbcTemplate;
import org.springframework.jdbc.core.namedparam.SqlParameterSource;
import org.springframework.jdbc.core.namedparam.SqlParameterSourceUtils;
import org.springframework.stereotype.Service;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

@Service
public class EtlService {

    private static final int BATCH_SIZE = 1000; // Example batch size
    private static final Map<String, String> columnMapping = getColumnMapping(); // Mock column mapping

    @Autowired
    private NamedParameterJdbcTemplate namedParameterJdbcTemplate;

    public void performEtl() {
        int offset = 0;
        List<Map<String, Object>> batch;

        do {
            batch = extractBatch(offset, BATCH_SIZE);

            List<Map<String, Object>> transformedBatch = transformBatch(batch);

            loadBatch(transformedBatch);

            offset += batch.size();
        } while (batch.size() == BATCH_SIZE);
    }

    private List<Map<String, Object>> extractBatch(int offset, int batchSize) {
        String sql = "SELECT * FROM source_table LIMIT :limit OFFSET :offset";
        return namedParameterJdbcTemplate.queryForList(sql, Map.of("limit", batchSize, "offset", offset));
    }

    private List<Map<String, Object>> transformBatch(List<Map<String, Object>> batch) {
        List<Map<String, Object>> transformedBatch = new ArrayList<>();
        for (Map<String, Object> row : batch) {
            Map<String, Object> transformedRow = renameKeys(row, columnMapping);
            transformedBatch.add(transformedRow);
        }
        return transformedBatch;
    }

    private void loadBatch(List<Map<String, Object>> batch) {
        String sql = "INSERT INTO target_table (col1, col2, col3, ...) VALUES (:col1, :col2, :col3, ...)";
        SqlParameterSource[] batchParams = SqlParameterSourceUtils.createBatch(batch.toArray());
        namedParameterJdbcTemplate.batchUpdate(sql, batchParams);
    }

    private static Map<String, String> getColumnMapping() {
        Map<String, String> columnMapping = new HashMap<>();
        columnMapping.put("srcName1", "tgtName1");
        columnMapping.put("srcName2", "tgtName2");
        // ... add all mappings here
        return columnMapping;
    }

    private Map<String, Object> renameKeys(Map<String, Object> row, Map<String, String> columnMapping) {
        Map<String, Object> transformedRow = new HashMap<>();
        for (Map.Entry<String, Object> entry : row.entrySet()) {
            String sourceColumnName = entry.getKey();
            Object value = entry.getValue();
            String targetColumnName = columnMapping.getOrDefault(sourceColumnName, sourceColumnName);
            transformedRow.put(targetColumnName, value);
        }
        return transformedRow;
    }
}


@Service
public class AitService {

    @Autowired
    private JdbcTemplate jdbcTemplate;

    public void insertSoftwareDetails(int ait_id) {
        String selectQuery = "select distinct(osname), osversion from ait_infra where ait_id = ?";

        List<String> osDetails = jdbcTemplate.query(selectQuery, new Object[]{ait_id}, new RowMapper<String>() {
            @Override
            public String mapRow(ResultSet rs, int rowNum) throws SQLException {
                return rs.getString("osname") + "_" + rs.getString("osversion");
            }
        });

        for (String osDetail : osDetails) {
            String insertStatement = "insert into ait_sw_details(ait_id, sw_type, sw_name) values (?, 'OS', ?)";
            jdbcTemplate.update(insertStatement, ait_id, osDetail);
        }
    }
}


import numpy as np

# Generate some synthetic CPU utilization data as an example
np.random.seed(42)
data_points = np.random.randint(0, 100, 43200)

# Sort the data
sorted_data = np.sort(data_points)

# Calculate the 95th percentile index
index_95th = int((95 / 100) * len(sorted_data))

# Get the 95th percentile value
value_95th = sorted_data[index_95th]




# Importing pandas and recalculating the target number of cores

import pandas as pd

data = {
    'Server name': ['Server1', 'Server2', 'Server3', 'Server4', 'Server5', 'Server6', 'Server7', 'Server8'],
    'Total Cores': [12, 12, 12, 12, 16, 16, 16, 16],
    'March Utilization': [3.98, 2.98, 1.98, 10.98, 3.71, 4.98, 3.71, 16.07],
    'April Utilization': [4.03, 5.03, 8.03, 45.50, 3.06, 7.50, 3.06, 24.21],
    'May Utilization': [5.67, 8.67, 4.67, 3.67, 2.73, 3.67, 2.73, 23.67],
    'June Utilization': [3.98, 12.98, 11.98, 4.98, 11.53, 4.98, 11.53, 4.98],
    'July Utilization': [10.91, 12.91, 10.91, 2.91, 18.77, 2.91, 18.77, 2.91],
    'August Utilization': [69.1, 0.74, 1.74, 3.28, 27.10, 3.28, 27.10, 33.28]
}

#df = pd.read_excel("/path/to/your/excel/file.xlsx", sheet_name="Sheet1")

df = pd.DataFrame(data)



# Calculate monthly peak core utilization for each server
for month in ['March Utilization', 'April Utilization', 'May Utilization', 'June Utilization', 'July Utilization', 'August Utilization']:
    df[f'{month} Cores'] = df[month] * df['Total Cores']

# Aggregate monthly peaks
monthly_peaks = df[[f'{month} Cores' for month in ['March Utilization', 'April Utilization', 'May Utilization', 'June Utilization', 'July Utilization', 'August Utilization']]].sum()

# Determine overall peak utilization
overall_peak = monthly_peaks.max()

# Consider 20% headroom
headroom = 0.20 * overall_peak

# Calculate total target cores
target_cores = overall_peak + headroom

target_cores

# Compute monthly peaks for each server
monthly_peaks_df = df[[f'{month} Cores' for month in ['March Utilization', 'April Utilization', 'May Utilization', 'June Utilization', 'July Utilization', 'August Utilization']]]

# Compute the median of peaks for each month
median_peaks = monthly_peaks_df.median()

# Anomaly detection using Interquartile Range (IQR)
Q1 = median_peaks.quantile(0.25)
Q3 = median_peaks.quantile(0.75)
IQR = Q3 - Q1

# Define bounds for outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Filter out outliers
adjusted_peaks = median_peaks[(median_peaks >= lower_bound) & (median_peaks <= upper_bound)]

# Compute the adjusted peak and add 20% headroom
adjusted_peak = adjusted_peaks.max()
headroom = 0.20 * adjusted_peak
target_cores_adjusted = adjusted_peak + headroom

target_cores_adjusted


@Service
public class EtlService {

    @Autowired
    private JdbcTemplate jdbcTemplate;

    public void startEtlProcess() {
        List<String> allAitIds = getAllAitIds(); // Assume this gets all unique AIT_IDs using native SQL

        List<CompletableFuture<Void>> futures = new ArrayList<>();
        for (String aitId : allAitIds) {
            CompletableFuture<Void> future = processAitId(aitId);
            futures.add(future);
        }

        CompletableFuture.allOf(futures.toArray(new CompletableFuture[0])).join();
    }

    @Async("taskExecutor")
    public CompletableFuture<Void> processAitId(String aitId) {
        // Fetch records for a specific AIT_ID
        String fetchQuery = "SELECT * FROM source_table WHERE AIT_ID = ?";
        List<Map<String, Object>> fetchedRecords = jdbcTemplate.queryForList(fetchQuery, new Object[]{aitId});

        // Transformation logic
        List<Map<String, Object>> transformedRecords = transform(fetchedRecords); // Implement your own transform method

        // Batch insert into the target table
        String insertQuery = "INSERT INTO target_table (column1, column2, ...) VALUES (?, ?, ...)";
        List<Object[]> batchArgsList = new ArrayList<>();
        for (Map<String, Object> record : transformedRecords) {
            Object[] args = {record.get("column1"), record.get("column2"), /*...*/};
            batchArgsList.add(args);
        }

        jdbcTemplate.batchUpdate(insertQuery, batchArgsList);
        return CompletableFuture.completedFuture(null);
    }

    private List<Map<String, Object>> transform(List<Map<String, Object>> records) {
        // Your transformation logic here
        return records; // Return the transformed records
    }
}
